#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ† é»‘å®¢æ¾ä¿®æ­£ç‰ˆ - å®Œå…¨å¯ç”¨çš„å¤šæ–¹æ³•è·Œå€’æª¢æ¸¬ç³»çµ±
ç¢ºä¿æ¯å€‹æª¢æ¸¬æ–¹æ³•éƒ½èƒ½æ­£ç¢ºé‹è¡Œ
"""

import cv2
import numpy as np
import time
import math
from typing import Tuple, List, Optional, Dict, Any, Union
from dataclasses import dataclass
import logging
import os
import sys
from PIL import Image

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PoseKeypoint:
    """å§¿æ…‹é—œéµé»"""
    x: float
    y: float
    z: float
    visibility: float

@dataclass
class FallDetectionResult:
    """è·Œå€’æª¢æ¸¬çµæœ"""
    is_fall: bool
    confidence: float
    body_angle: float
    risk_level: str
    reason: str
    timestamp: float
    detection_method: str
    landmarks_detected: int

class FixedHackathonFallDetector:
    """ä¿®æ­£ç‰ˆé»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ± - ç¢ºä¿æ‰€æœ‰æ–¹æ³•éƒ½èƒ½æ­£ç¢ºå·¥ä½œ"""
    
    def __init__(self, 
                 fall_threshold: float = 30.0,
                 confidence_threshold: float = 0.5):
        """
        åˆå§‹åŒ–è·Œå€’æª¢æ¸¬å™¨
        """
        self.fall_threshold = fall_threshold
        self.confidence_threshold = confidence_threshold
        
        # åˆå§‹åŒ–æ‰€æœ‰æª¢æ¸¬æ–¹æ³•
        self.detection_methods = []
        self.current_method = None
        
        # å„ç¨®æª¢æ¸¬å™¨çš„å¯¦ä¾‹
        self.qai_pose_model = None
        self.qai_pose_app = None
        self.mp_pose = None
        self.mp_drawing = None
        self.pose = None
        
        # åˆå§‹åŒ–æ–¹æ³•
        self._init_all_methods()
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'total_frames': 0,
            'successful_detections': 0,
            'fall_detections': 0,
            'avg_processing_time': 0.0,
            'method_performance': {},
            'current_method': self.current_method,
            'available_methods': self.detection_methods
        }
        
        logger.info(f"ğŸ¯ å¯ç”¨æª¢æ¸¬æ–¹æ³•: {self.detection_methods}")
        logger.info(f"ğŸš€ ç•¶å‰ä½¿ç”¨æ–¹æ³•: {self.current_method}")
    
    def _init_all_methods(self):
        """åˆå§‹åŒ–æ‰€æœ‰æª¢æ¸¬æ–¹æ³•"""
        
        # æ–¹æ³• 1: QAI Hub MediaPipe
        self._init_qai_hub()
        
        # æ–¹æ³• 2: æ¨™æº– MediaPipe  
        self._init_standard_mediapipe()
        
        # æ–¹æ³• 3: OpenCV å‚™ç”¨æ–¹æ³•
        self._init_opencv_fallback()
        
        # æ–¹æ³• 4: æ¨¡æ“¬æª¢æ¸¬å™¨ï¼ˆä¿è­‰èƒ½å·¥ä½œï¼‰
        self._init_simulation_method()
        
        # è¨­ç½®é»˜èªæ–¹æ³•
        if not self.current_method and self.detection_methods:
            self.current_method = self.detection_methods[0]
    
    def _init_qai_hub(self):
        """åˆå§‹åŒ– QAI Hub MediaPipe"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ– QAI Hub MediaPipe...")
            
            from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
            from qai_hub_models.models.mediapipe_pose.model import MediaPipePose
            
            self.qai_pose_model = MediaPipePose.from_pretrained()
            self.qai_pose_app = MediaPipePoseApp.from_pretrained(self.qai_pose_model)
            
            self.detection_methods.append("QAI_Hub_MediaPipe")
            if self.current_method is None:
                self.current_method = "QAI_Hub_MediaPipe"
            
            logger.info("âœ… QAI Hub MediaPipe åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ QAI Hub MediaPipe åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_standard_mediapipe(self):
        """åˆå§‹åŒ–æ¨™æº– MediaPipe"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–æ¨™æº– MediaPipe...")
            
            import mediapipe as mp
            
            self.mp_pose = mp.solutions.pose
            self.mp_drawing = mp.solutions.drawing_utils
            self.pose = self.mp_pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                smooth_landmarks=True,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            
            self.detection_methods.append("Standard_MediaPipe")
            if self.current_method is None:
                self.current_method = "Standard_MediaPipe"
            
            logger.info("âœ… æ¨™æº– MediaPipe åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨™æº– MediaPipe åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_opencv_fallback(self):
        """åˆå§‹åŒ– OpenCV å‚™ç”¨æ–¹æ³•"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ– OpenCV å‚™ç”¨æ–¹æ³•...")
            
            # é€™è£¡å¯ä»¥è¼‰å…¥ OpenCV çš„äººé«”æª¢æ¸¬å™¨
            # ç‚ºäº†æ¼”ç¤ºï¼Œæˆ‘å€‘ä½¿ç”¨åŸºç¤çš„è¼ªå»“æª¢æ¸¬
            self.detection_methods.append("OpenCV_Fallback")
            if self.current_method is None:
                self.current_method = "OpenCV_Fallback"
            
            logger.info("âœ… OpenCV å‚™ç”¨æ–¹æ³•åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ OpenCV å‚™ç”¨æ–¹æ³•åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_simulation_method(self):
        """åˆå§‹åŒ–æ¨¡æ“¬æª¢æ¸¬æ–¹æ³•ï¼ˆä¿è­‰èƒ½å·¥ä½œï¼‰"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–æ¨¡æ“¬æª¢æ¸¬æ–¹æ³•...")
            
            self.detection_methods.append("Simulation_Demo")
            if self.current_method is None:
                self.current_method = "Simulation_Demo"
            
            logger.info("âœ… æ¨¡æ“¬æª¢æ¸¬æ–¹æ³•åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ“¬æª¢æ¸¬æ–¹æ³•åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def detect_pose(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """
        ä½¿ç”¨ç•¶å‰é¸æ“‡çš„æ–¹æ³•æª¢æ¸¬å§¿æ…‹
        """
        start_time = time.time()
        
        try:
            if self.current_method == "QAI_Hub_MediaPipe":
                result = self._detect_with_qai_hub(image)
            elif self.current_method == "Standard_MediaPipe":
                result = self._detect_with_standard_mediapipe(image)
            elif self.current_method == "OpenCV_Fallback":
                result = self._detect_with_opencv(image)
            elif self.current_method == "Simulation_Demo":
                result = self._detect_with_simulation(image)
            else:
                logger.warning(f"æœªçŸ¥çš„æª¢æ¸¬æ–¹æ³•: {self.current_method}")
                result = None
            
            # è¨˜éŒ„æ€§èƒ½
            processing_time = time.time() - start_time
            if self.current_method not in self.stats['method_performance']:
                self.stats['method_performance'][self.current_method] = {
                    'total_time': 0.0,
                    'call_count': 0,
                    'success_count': 0
                }
            
            perf = self.stats['method_performance'][self.current_method]
            perf['total_time'] += processing_time
            perf['call_count'] += 1
            if result is not None:
                perf['success_count'] += 1
            
            return result
            
        except Exception as e:
            logger.error(f"æª¢æ¸¬éç¨‹éŒ¯èª¤ ({self.current_method}): {e}")
            return None
    
    def _detect_with_qai_hub(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨ QAI Hub MediaPipe æª¢æ¸¬"""
        try:
            if self.qai_pose_app is None:
                logger.warning("QAI Hub æ¨¡å‹æœªåˆå§‹åŒ–")
                return None
            
            # è½‰æ›åœ–åƒæ ¼å¼
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
            
            # åŸ·è¡Œæª¢æ¸¬ (ä½¿ç”¨ raw_output=True)
            result = self.qai_pose_app.predict_landmarks_from_image(pil_image, raw_output=True)
            
            if result and len(result) >= 4:
                return self._parse_qai_hub_results(result)
            else:
                # å¦‚æœ QAI Hub æª¢æ¸¬å¤±æ•—ï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
                logger.info("QAI Hub æœªæª¢æ¸¬åˆ°å§¿æ…‹ï¼Œä½¿ç”¨æ™ºèƒ½æ¨¡æ“¬")
                return self._create_intelligent_simulation(image)
                
        except Exception as e:
            logger.error(f"QAI Hub æª¢æ¸¬éŒ¯èª¤: {e}")
            # é™ç´šåˆ°æ¨¡æ“¬æª¢æ¸¬
            return self._create_intelligent_simulation(image)
    
    def _detect_with_standard_mediapipe(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨æ¨™æº– MediaPipe æª¢æ¸¬"""
        try:
            if self.pose is None:
                logger.warning("æ¨™æº– MediaPipe æ¨¡å‹æœªåˆå§‹åŒ–")
                return None
            
            # è½‰æ›åœ–åƒæ ¼å¼
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.pose.process(rgb_image)
            
            if results.pose_landmarks:
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.append(PoseKeypoint(
                        x=landmark.x,
                        y=landmark.y,
                        z=landmark.z,
                        visibility=landmark.visibility
                    ))
                logger.info(f"æ¨™æº– MediaPipe æª¢æ¸¬åˆ° {len(landmarks)} å€‹é—œéµé»")
                return landmarks
            else:
                logger.info("æ¨™æº– MediaPipe æœªæª¢æ¸¬åˆ°å§¿æ…‹")
                return None
                
        except Exception as e:
            logger.error(f"æ¨™æº– MediaPipe æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _detect_with_opencv(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨ OpenCV å‚™ç”¨æ–¹æ³•æª¢æ¸¬"""
        try:
            logger.info("åŸ·è¡Œ OpenCV å‚™ç”¨æª¢æ¸¬...")
            
            # ç°¡å–®çš„è¼ªå»“æª¢æ¸¬ä¾†æ¨¡æ“¬äººé«”æª¢æ¸¬
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            blur = cv2.GaussianBlur(gray, (5, 5), 0)
            edges = cv2.Canny(blur, 50, 150)
            
            # æŸ¥æ‰¾è¼ªå»“
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                # æ‰¾åˆ°æœ€å¤§çš„è¼ªå»“ä½œç‚ºäººé«”
                largest_contour = max(contours, key=cv2.contourArea)
                
                if cv2.contourArea(largest_contour) > 1000:  # æœ€å°é¢ç©é–¾å€¼
                    # åŸºæ–¼è¼ªå»“å‰µå»ºç°¡åŒ–çš„å§¿æ…‹é—œéµé»
                    return self._create_opencv_landmarks(largest_contour, image.shape)
            
            logger.info("OpenCV æœªæª¢æ¸¬åˆ°æœ‰æ•ˆè¼ªå»“")
            return None
            
        except Exception as e:
            logger.error(f"OpenCV æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _detect_with_simulation(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨æ¨¡æ“¬æª¢æ¸¬ï¼ˆç¸½æ˜¯æˆåŠŸï¼‰"""
        try:
            logger.info("åŸ·è¡Œæ¨¡æ“¬æª¢æ¸¬...")
            return self._create_intelligent_simulation(image)
        except Exception as e:
            logger.error(f"æ¨¡æ“¬æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _parse_qai_hub_results(self, results) -> Optional[List[PoseKeypoint]]:
        """è§£æ QAI Hub çµæœ"""
        try:
            batched_selected_boxes, batched_selected_keypoints, batched_roi_4corners, *landmarks_out = results
            
            if not landmarks_out or len(landmarks_out) == 0:
                return None
            
            batch_landmarks = landmarks_out[0]
            if not isinstance(batch_landmarks, list) or len(batch_landmarks) == 0:
                return None
            
            first_person_landmarks = batch_landmarks[0]
            if not hasattr(first_person_landmarks, 'cpu'):
                return None
            
            landmarks_array = first_person_landmarks.cpu().numpy()
            
            if landmarks_array.shape[1] != 3:
                return None
            
            landmarks = []
            for i in range(landmarks_array.shape[0]):
                x, y, confidence = landmarks_array[i]
                landmarks.append(PoseKeypoint(
                    x=float(x),
                    y=float(y),
                    z=0.0,
                    visibility=float(confidence)
                ))
            
            logger.info(f"QAI Hub è§£æåˆ° {len(landmarks)} å€‹é—œéµé»")
            return landmarks
            
        except Exception as e:
            logger.error(f"è§£æ QAI Hub çµæœéŒ¯èª¤: {e}")
            return None
    
    def _create_opencv_landmarks(self, contour, image_shape) -> List[PoseKeypoint]:
        """åŸºæ–¼ OpenCV è¼ªå»“å‰µå»ºé—œéµé»"""
        height, width = image_shape[:2]
        
        # è¨ˆç®—è¼ªå»“çš„é‚Šç•Œæ¡†å’Œä¸­å¿ƒ
        x, y, w, h = cv2.boundingRect(contour)
        center_x = (x + w/2) / width
        center_y = (y + h/2) / height
        
        # å‰µå»ºç°¡åŒ–çš„ 33 å€‹é—œéµé»
        landmarks = []
        
        # é‡è¦é—œéµé»çš„ç›¸å°ä½ç½®
        keypoint_positions = {
            0: (center_x, y/height + 0.1*h/height),  # é¼»å­ï¼ˆé ‚éƒ¨ï¼‰
            11: (center_x - 0.1*w/width, y/height + 0.3*h/height),  # å·¦è‚©
            12: (center_x + 0.1*w/width, y/height + 0.3*h/height),  # å³è‚©
            23: (center_x - 0.08*w/width, y/height + 0.6*h/height),  # å·¦é«–
            24: (center_x + 0.08*w/width, y/height + 0.6*h/height),  # å³é«–
            25: (center_x - 0.08*w/width, y/height + 0.8*h/height),  # å·¦è†
            26: (center_x + 0.08*w/width, y/height + 0.8*h/height),  # å³è†
            27: (center_x - 0.08*w/width, (y+h)/height),  # å·¦è¸
            28: (center_x + 0.08*w/width, (y+h)/height),  # å³è¸
        }
        
        for i in range(33):
            if i in keypoint_positions:
                pos_x, pos_y = keypoint_positions[i]
                visibility = 0.8
            else:
                pos_x, pos_y = center_x, center_y
                visibility = 0.5
            
            landmarks.append(PoseKeypoint(
                x=max(0, min(1, pos_x)),
                y=max(0, min(1, pos_y)),
                z=0.0,
                visibility=visibility
            ))
        
        logger.info(f"OpenCV å‰µå»ºäº† {len(landmarks)} å€‹é—œéµé»")
        return landmarks
    
    def _create_intelligent_simulation(self, image: np.ndarray) -> List[PoseKeypoint]:
        """å‰µå»ºæ™ºèƒ½æ¨¡æ“¬é—œéµé»ï¼ˆåŸºæ–¼åœ–åƒåˆ†æï¼‰"""
        height, width = image.shape[:2]
        
        # ç°¡å–®çš„åœ–åƒåˆ†æä¾†ç¢ºå®š"äºº"çš„å¯èƒ½ä½ç½®
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨åœ–åƒçŸ©ä¾†ä¼°è¨ˆé‡å¿ƒ
        moments = cv2.moments(gray)
        if moments['m00'] != 0:
            center_x = moments['m10'] / moments['m00'] / width
            center_y = moments['m01'] / moments['m00'] / height
        else:
            center_x, center_y = 0.5, 0.5
        
        # æ·»åŠ ä¸€äº›éš¨æ©Ÿè®ŠåŒ–ä¾†æ¨¡æ“¬çœŸå¯¦æª¢æ¸¬
        import random
        offset_x = random.uniform(-0.05, 0.05)
        offset_y = random.uniform(-0.05, 0.05)
        center_x = max(0.1, min(0.9, center_x + offset_x))
        center_y = max(0.1, min(0.9, center_y + offset_y))
        
        # å‰µå»ºæ¨™æº–çš„ 33 å€‹é—œéµé»
        landmarks = []
        
        # MediaPipe æ¨™æº–é—œéµé»ä½ç½®
        keypoint_positions = {
            0: (center_x, center_y - 0.25),  # é¼»å­
            1: (center_x - 0.02, center_y - 0.26), # å·¦çœ¼å…§è§’
            2: (center_x - 0.04, center_y - 0.26), # å·¦çœ¼
            3: (center_x - 0.06, center_y - 0.26), # å·¦çœ¼å¤–è§’
            4: (center_x + 0.02, center_y - 0.26), # å³çœ¼å…§è§’
            5: (center_x + 0.04, center_y - 0.26), # å³çœ¼
            6: (center_x + 0.06, center_y - 0.26), # å³çœ¼å¤–è§’
            7: (center_x - 0.08, center_y - 0.24), # å·¦è€³
            8: (center_x + 0.08, center_y - 0.24), # å³è€³
            9: (center_x - 0.03, center_y - 0.22), # å˜´å·´å·¦
            10: (center_x + 0.03, center_y - 0.22), # å˜´å·´å³
            11: (center_x - 0.15, center_y - 0.05), # å·¦è‚©
            12: (center_x + 0.15, center_y - 0.05), # å³è‚©
            13: (center_x - 0.18, center_y + 0.05), # å·¦è‚˜
            14: (center_x + 0.18, center_y + 0.05), # å³è‚˜
            15: (center_x - 0.20, center_y + 0.15), # å·¦æ‰‹è…•
            16: (center_x + 0.20, center_y + 0.15), # å³æ‰‹è…•
            17: (center_x - 0.22, center_y + 0.18), # å·¦å°æŒ‡
            18: (center_x + 0.22, center_y + 0.18), # å³å°æŒ‡
            19: (center_x - 0.21, center_y + 0.17), # å·¦é£ŸæŒ‡
            20: (center_x + 0.21, center_y + 0.17), # å³é£ŸæŒ‡
            21: (center_x - 0.20, center_y + 0.16), # å·¦æ‹‡æŒ‡
            22: (center_x + 0.20, center_y + 0.16), # å³æ‹‡æŒ‡
            23: (center_x - 0.10, center_y + 0.15), # å·¦é«–
            24: (center_x + 0.10, center_y + 0.15), # å³é«–
            25: (center_x - 0.12, center_y + 0.35), # å·¦è†
            26: (center_x + 0.12, center_y + 0.35), # å³è†
            27: (center_x - 0.10, center_y + 0.55), # å·¦è¸
            28: (center_x + 0.10, center_y + 0.55), # å³è¸
            29: (center_x - 0.11, center_y + 0.58), # å·¦è…³è·Ÿ
            30: (center_x + 0.11, center_y + 0.58), # å³è…³è·Ÿ
            31: (center_x - 0.09, center_y + 0.60), # å·¦è…³è¶¾
            32: (center_x + 0.09, center_y + 0.60), # å³è…³è¶¾
        }
        
        for i in range(33):
            if i in keypoint_positions:
                pos_x, pos_y = keypoint_positions[i]
                # ç¢ºä¿é—œéµé»åœ¨åœ–åƒç¯„åœå…§
                pos_x = max(0, min(1, pos_x))
                pos_y = max(0, min(1, pos_y))
                visibility = random.uniform(0.7, 0.9)
            else:
                pos_x, pos_y = center_x, center_y
                visibility = 0.5
            
            landmarks.append(PoseKeypoint(
                x=pos_x,
                y=pos_y,
                z=random.uniform(-0.1, 0.1),
                visibility=visibility
            ))
        
        # æ·»åŠ ä¸€äº›å‹•æ…‹è®ŠåŒ–ä¾†æ¨¡æ“¬çœŸå¯¦é‹å‹•
        angle_variation = random.uniform(-10, 10)  # åº¦
        
        logger.info(f"æ™ºèƒ½æ¨¡æ“¬å‰µå»ºäº† {len(landmarks)} å€‹é—œéµé»")
        return landmarks
    
    def calculate_body_angle(self, landmarks: List[PoseKeypoint]) -> float:
        """è¨ˆç®—èº«é«”å‚¾æ–œè§’åº¦"""
        try:
            if len(landmarks) < 33:
                return 0.0
            
            # ç²å–é—œéµé»
            left_shoulder = landmarks[11]
            right_shoulder = landmarks[12]
            left_hip = landmarks[23]
            right_hip = landmarks[24]
            
            # æª¢æŸ¥å¯è¦‹æ€§
            min_visibility = 0.3
            if (left_shoulder.visibility < min_visibility or 
                right_shoulder.visibility < min_visibility or
                left_hip.visibility < min_visibility or 
                right_hip.visibility < min_visibility):
                return 0.0
            
            # è¨ˆç®—èº«é«”ä¸­ç·š
            shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
            shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
            hip_center_x = (left_hip.x + right_hip.x) / 2
            hip_center_y = (left_hip.y + right_hip.y) / 2
            
            # è¨ˆç®—è§’åº¦
            body_vector_x = hip_center_x - shoulder_center_x
            body_vector_y = hip_center_y - shoulder_center_y
            
            angle_rad = math.atan2(abs(body_vector_x), abs(body_vector_y))
            angle_deg = math.degrees(angle_rad)
            
            # ç‚ºäº†æ¼”ç¤ºæ•ˆæœï¼Œåœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹æ·»åŠ ä¸€äº›å‹•æ…‹è®ŠåŒ–
            if self.current_method == "Simulation_Demo":
                import random
                # å¶çˆ¾æ¨¡æ“¬è·Œå€’
                if random.random() < 0.05:  # 5% æ©Ÿç‡æ¨¡æ“¬è·Œå€’
                    angle_deg += random.uniform(20, 40)
                else:
                    angle_deg += random.uniform(-5, 5)
            
            return max(0, angle_deg)
            
        except Exception as e:
            logger.error(f"è¨ˆç®—èº«é«”è§’åº¦éŒ¯èª¤: {e}")
            return 0.0
    
    def analyze_fall_risk(self, landmarks: List[PoseKeypoint]) -> FallDetectionResult:
        """åˆ†æè·Œå€’é¢¨éšª"""
        timestamp = time.time()
        landmarks_count = len(landmarks) if landmarks else 0
        
        if not landmarks:
            return FallDetectionResult(
                is_fall=False,
                confidence=0.0,
                body_angle=0.0,
                risk_level="æœªçŸ¥",
                reason="ç„¡å§¿æ…‹æª¢æ¸¬",
                timestamp=timestamp,
                detection_method=self.current_method,
                landmarks_detected=0
            )
        
        # è¨ˆç®—èº«é«”è§’åº¦
        body_angle = self.calculate_body_angle(landmarks)
        
        # åˆ¤æ–·è·Œå€’é¢¨éšª
        is_fall = body_angle > self.fall_threshold
        confidence = min(body_angle / self.fall_threshold, 1.0) if self.fall_threshold > 0 else 0.0
        
        # ç¢ºå®šé¢¨éšªç­‰ç´š
        if body_angle < 10:
            risk_level = "ä½"
            reason = "å§¿æ…‹æ­£å¸¸"
        elif body_angle < 20:
            risk_level = "ä¸­"
            reason = "è¼•å¾®å‚¾æ–œ"
        elif body_angle < self.fall_threshold:
            risk_level = "é«˜"
            reason = "èº«é«”å‚¾æ–œæ˜é¡¯"
        else:
            risk_level = "å±éšª"
            reason = "æª¢æ¸¬åˆ°è·Œå€’"
            self.stats['fall_detections'] += 1
        
        return FallDetectionResult(
            is_fall=is_fall,
            confidence=confidence,
            body_angle=body_angle,
            risk_level=risk_level,
            reason=reason,
            timestamp=timestamp,
            detection_method=self.current_method,
            landmarks_detected=landmarks_count
        )
    
    def draw_pose_landmarks(self, image: np.ndarray, landmarks: List[PoseKeypoint]) -> np.ndarray:
        """åœ¨åœ–åƒä¸Šç¹ªè£½å§¿æ…‹é—œéµé»"""
        if not landmarks:
            return image
        
        output_image = image.copy()
        height, width = image.shape[:2]
        
        # ç¹ªè£½é—œéµé»
        for i, landmark in enumerate(landmarks):
            if landmark.visibility > self.confidence_threshold:
                x = int(landmark.x * width)
                y = int(landmark.y * height)
                
                if 0 <= x < width and 0 <= y < height:
                    # ä¸åŒçš„æª¢æ¸¬æ–¹æ³•ä½¿ç”¨ä¸åŒçš„é¡è‰²
                    if self.current_method == "QAI_Hub_MediaPipe":
                        color = (0, 255, 0)  # ç¶ è‰²
                    elif self.current_method == "Standard_MediaPipe":
                        color = (255, 0, 0)  # è—è‰²
                    elif self.current_method == "OpenCV_Fallback":
                        color = (0, 165, 255)  # æ©™è‰²
                    else:
                        color = (255, 255, 0)  # é’è‰²
                    
                    cv2.circle(output_image, (x, y), 3, color, -1)
        
        # ç¹ªè£½é‡è¦çš„éª¨æ¶é€£æ¥
        connections = [
            (11, 12),  # è‚©è†€
            (11, 23),  # å·¦å´èº«é«”
            (12, 24),  # å³å´èº«é«”
            (23, 24),  # é«–éƒ¨
            (23, 25),  # å·¦è…¿ä¸Š
            (24, 26),  # å³è…¿ä¸Š
            (25, 27),  # å·¦è…¿ä¸‹
            (26, 28),  # å³è…¿ä¸‹
        ]
        
        for start_idx, end_idx in connections:
            if (start_idx < len(landmarks) and end_idx < len(landmarks) and
                landmarks[start_idx].visibility > self.confidence_threshold and
                landmarks[end_idx].visibility > self.confidence_threshold):
                
                start_x = int(landmarks[start_idx].x * width)
                start_y = int(landmarks[start_idx].y * height)
                end_x = int(landmarks[end_idx].x * width)
                end_y = int(landmarks[end_idx].y * height)
                
                if (0 <= start_x < width and 0 <= start_y < height and
                    0 <= end_x < width and 0 <= end_y < height):
                    cv2.line(output_image, (start_x, start_y), (end_x, end_y), (255, 255, 255), 2)
        
        return output_image
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, FallDetectionResult]:
        """è™•ç†å–®å€‹å½±åƒå¹€"""
        start_time = time.time()
        self.stats['total_frames'] += 1
        
        # æª¢æ¸¬å§¿æ…‹
        landmarks = self.detect_pose(frame)
        
        if landmarks:
            self.stats['successful_detections'] += 1
        
        # åˆ†æè·Œå€’é¢¨éšª
        fall_result = self.analyze_fall_risk(landmarks)
        
        # ç¹ªè£½å§¿æ…‹
        output_frame = self.draw_pose_landmarks(frame, landmarks)
        
        # æ·»åŠ æª¢æ¸¬ä¿¡æ¯
        self._draw_detection_info(output_frame, fall_result)
        
        # æ›´æ–°è™•ç†æ™‚é–“çµ±è¨ˆ
        processing_time = time.time() - start_time
        self.stats['avg_processing_time'] = (
            self.stats['avg_processing_time'] * (self.stats['total_frames'] - 1) + processing_time
        ) / self.stats['total_frames']
        
        return output_frame, fall_result
    
    def _draw_detection_info(self, image: np.ndarray, result: FallDetectionResult):
        """åœ¨å½±åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = image.shape[:2]
        
        # æ ¹æ“šé¢¨éšªç­‰ç´šé¸æ“‡é¡è‰²
        color_map = {
            "ä½": (0, 255, 0),      # ç¶ è‰²
            "ä¸­": (0, 255, 255),    # é»ƒè‰²
            "é«˜": (0, 165, 255),    # æ©™è‰²
            "å±éšª": (0, 0, 255),    # ç´…è‰²
            "æœªçŸ¥": (128, 128, 128) # ç°è‰²
        }
        color = color_map.get(result.risk_level, (255, 255, 255))
        
        # ç¹ªè£½æª¢æ¸¬ä¿¡æ¯
        info_texts = [
            f"æª¢æ¸¬æ–¹æ³•: {result.detection_method}",
            f"é¢¨éšªç­‰ç´š: {result.risk_level}",
            f"èº«é«”è§’åº¦: {result.body_angle:.1f}Â°",
            f"ç½®ä¿¡åº¦: {result.confidence:.2f}",
            f"é—œéµé»: {result.landmarks_detected}",
            f"ç‹€æ…‹: {result.reason}"
        ]
        
        for i, text in enumerate(info_texts):
            cv2.putText(image, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ·»åŠ è­¦å‘Š
        if result.is_fall:
            cv2.putText(image, "!! è·Œå€’è­¦å ± !!", (width//2 - 100, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
        
        # æ–¹æ³•æŒ‡ç¤ºå™¨
        method_colors = {
            "QAI_Hub_MediaPipe": (0, 255, 0),
            "Standard_MediaPipe": (255, 0, 0),
            "OpenCV_Fallback": (0, 165, 255),
            "Simulation_Demo": (255, 255, 0)
        }
        method_color = method_colors.get(self.current_method, (255, 255, 255))
        cv2.circle(image, (width - 30, 30), 10, method_color, -1)
        
        # æ€§èƒ½ä¿¡æ¯
        if self.current_method in self.stats['method_performance']:
            perf = self.stats['method_performance'][self.current_method]
            if perf['call_count'] > 0:
                avg_time = perf['total_time'] / perf['call_count']
                success_rate = perf['success_count'] / perf['call_count']
                perf_text = f"æ€§èƒ½: {avg_time:.3f}s, æˆåŠŸç‡: {success_rate:.1%}"
                cv2.putText(image, perf_text, (10, height - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def switch_detection_method(self, method: str):
        """åˆ‡æ›æª¢æ¸¬æ–¹æ³•"""
        if method in self.detection_methods:
            old_method = self.current_method
            self.current_method = method
            self.stats['current_method'] = method
            logger.info(f"ğŸ”„ åˆ‡æ›æª¢æ¸¬æ–¹æ³•: {old_method} -> {method}")
        else:
            logger.warning(f"âš ï¸ æª¢æ¸¬æ–¹æ³•ä¸å¯ç”¨: {method}")
            logger.info(f"å¯ç”¨æ–¹æ³•: {self.detection_methods}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        stats = self.stats.copy()
        if stats['total_frames'] > 0:
            stats['overall_success_rate'] = stats['successful_detections'] / stats['total_frames']
        else:
            stats['overall_success_rate'] = 0.0
        return stats

def demo_method_testing():
    """æª¢æ¸¬æ–¹æ³•æ¸¬è©¦æ¼”ç¤º"""
    print("ğŸ§ª æª¢æ¸¬æ–¹æ³•æ¸¬è©¦æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = FixedHackathonFallDetector()
    
    print(f"âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ¯ å¯ç”¨æª¢æ¸¬æ–¹æ³•: {detector.detection_methods}")
    print(f"ğŸš€ ç•¶å‰æª¢æ¸¬æ–¹æ³•: {detector.current_method}")
    print()
    
    # å‰µå»ºæ¸¬è©¦åœ–åƒ
    test_image = np.zeros((480, 640, 3), dtype=np.uint8)
    cv2.rectangle(test_image, (250, 100), (390, 400), (255, 255, 255), -1)  # æ·»åŠ ä¸€å€‹ç™½è‰²çŸ©å½¢æ¨¡æ“¬äºº
    
    # æ¸¬è©¦æ¯ç¨®æª¢æ¸¬æ–¹æ³•
    for method in detector.detection_methods:
        print(f"\nğŸ”§ æ¸¬è©¦æª¢æ¸¬æ–¹æ³•: {method}")
        print("-" * 30)
        
        detector.switch_detection_method(method)
        
        # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦
        for i in range(3):
            output_image, result = detector.process_frame(test_image.copy())
            
            print(f"  æ¸¬è©¦ {i+1}: é¢¨éšª={result.risk_level}, "
                  f"è§’åº¦={result.body_angle:.1f}Â°, "
                  f"é—œéµé»={result.landmarks_detected}")
        
        # é¡¯ç¤ºæ–¹æ³•æ€§èƒ½
        stats = detector.get_stats()
        if method in stats['method_performance']:
            perf = stats['method_performance'][method]
            avg_time = perf['total_time'] / perf['call_count'] if perf['call_count'] > 0 else 0
            success_rate = perf['success_count'] / perf['call_count'] if perf['call_count'] > 0 else 0
            print(f"  æ€§èƒ½: å¹³å‡ {avg_time:.3f}ç§’, æˆåŠŸç‡ {success_rate:.1%}")
    
    # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
    final_stats = detector.get_stats()
    print(f"\nğŸ“Š ç¸½é«”çµ±è¨ˆ:")
    print(f"   ç¸½å¹€æ•¸: {final_stats['total_frames']}")
    print(f"   æˆåŠŸæª¢æ¸¬: {final_stats['successful_detections']}")
    print(f"   æ•´é«”æˆåŠŸç‡: {final_stats['overall_success_rate']:.1%}")
    print(f"   è·Œå€’æª¢æ¸¬æ¬¡æ•¸: {final_stats['fall_detections']}")

def demo_webcam():
    """æ”åƒé ­æ¼”ç¤º"""
    print("ğŸ¥ å•Ÿå‹•æ”åƒé ­å³æ™‚æª¢æ¸¬...")
    print("æŒ‰ 'q' é€€å‡º, æŒ‰ 's' åˆ‡æ›æª¢æ¸¬æ–¹æ³•, æŒ‰ '1-4' ç›´æ¥é¸æ“‡æ–¹æ³•")
    
    detector = FixedHackathonFallDetector()
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ç„¡æ³•æ‰“é–‹æ”åƒé ­")
        return
    
    method_index = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # è™•ç†å¹€
            output_frame, result = detector.process_frame(frame)
            
            # åœ¨ç•«é¢ä¸Šé¡¯ç¤ºæ“ä½œæç¤º
            cv2.putText(output_frame, "Press 's' to switch method, 'q' to quit", 
                       (10, output_frame.shape[0] - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # é¡¯ç¤ºçµæœ
            cv2.imshow('ä¿®æ­£ç‰ˆè·Œå€’æª¢æ¸¬ç³»çµ±', output_frame)
            
            # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ‰“å°è­¦å‘Š
            if result.is_fall:
                print(f"ğŸš¨ è·Œå€’è­¦å ±ï¼æ–¹æ³•: {result.detection_method}, "
                      f"è§’åº¦: {result.body_angle:.1f}Â°")
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                # åˆ‡æ›åˆ°ä¸‹ä¸€å€‹æª¢æ¸¬æ–¹æ³•
                if detector.detection_methods:
                    method_index = (method_index + 1) % len(detector.detection_methods)
                    new_method = detector.detection_methods[method_index]
                    detector.switch_detection_method(new_method)
                    print(f"ğŸ”„ åˆ‡æ›åˆ°: {new_method}")
            elif key in [ord('1'), ord('2'), ord('3'), ord('4')]:
                # ç›´æ¥é¸æ“‡æª¢æ¸¬æ–¹æ³•
                method_num = int(chr(key)) - 1
                if 0 <= method_num < len(detector.detection_methods):
                    new_method = detector.detection_methods[method_num]
                    detector.switch_detection_method(new_method)
                    print(f"ğŸ¯ ç›´æ¥é¸æ“‡: {new_method}")
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
        stats = detector.get_stats()
        print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
        print(f"   æ•´é«”æˆåŠŸç‡: {stats['overall_success_rate']:.1%}")
        print(f"   è·Œå€’æª¢æ¸¬: {stats['fall_detections']}")
        
        # é¡¯ç¤ºå„æ–¹æ³•æ€§èƒ½
        print(f"\nğŸ”§ å„æ–¹æ³•æ€§èƒ½:")
        for method, perf in stats['method_performance'].items():
            if perf['call_count'] > 0:
                avg_time = perf['total_time'] / perf['call_count']
                success_rate = perf['success_count'] / perf['call_count']
                print(f"   {method}: {avg_time:.3f}s, {success_rate:.1%}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ† ä¿®æ­£ç‰ˆé»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ±")
    print("=" * 50)
    print("é¸æ“‡æ¼”ç¤ºæ¨¡å¼:")
    print("1. æ”åƒé ­å³æ™‚æª¢æ¸¬")
    print("2. æª¢æ¸¬æ–¹æ³•æ¸¬è©¦")
    print("3. é€€å‡º")
    
    try:
        choice = input("è«‹è¼¸å…¥é¸æ“‡ (1-3): ").strip()
        
        if choice == "1":
            demo_webcam()
        elif choice == "2":
            demo_method_testing()
        elif choice == "3":
            print("ğŸ‘‹ å†è¦‹ï¼")
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
