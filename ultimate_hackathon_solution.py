#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ† é»‘å®¢æ¾çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ
çµåˆ QAI Hub æ¶æ§‹å±•ç¤º + å¯¦éš› MediaPipe æª¢æ¸¬
åŒæ™‚è§£æ±º protobuf è¡çªå’ŒæŠ€è¡“å±•ç¤ºéœ€æ±‚
"""

import cv2
import numpy as np
import time
import math
from typing import Tuple, List, Optional, Dict, Any
from dataclasses import dataclass
import logging
import os
import sys

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PoseKeypoint:
    """å§¿æ…‹é—œéµé»"""
    x: float
    y: float
    z: float
    visibility: float

@dataclass
class FallDetectionResult:
    """è·Œå€’æª¢æ¸¬çµæœ"""
    is_fall: bool
    confidence: float
    body_angle: float
    risk_level: str
    reason: str
    timestamp: float
    detection_method: str

class UltimateHackathonFallDetector:
    """çµ‚æ¥µé»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ± - æ”¯æ´å¤šç¨®æª¢æ¸¬æ–¹æ³•"""
    
    def __init__(self, 
                 fall_threshold: float = 30.0,
                 confidence_threshold: float = 0.5):
        """
        åˆå§‹åŒ–è·Œå€’æª¢æ¸¬å™¨
        
        Args:
            fall_threshold: è·Œå€’è§’åº¦é–¾å€¼ï¼ˆåº¦ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼
        """
        self.fall_threshold = fall_threshold
        self.confidence_threshold = confidence_threshold
        
        # å˜—è©¦åˆå§‹åŒ–ä¸åŒçš„æª¢æ¸¬æ–¹æ³•
        self.detection_methods = []
        self.current_method = None
        
        # æ–¹æ³• 1: å˜—è©¦ QAI Hub MediaPipe
        self._try_init_qai_hub()
        
        # æ–¹æ³• 2: å˜—è©¦æ¨™æº– MediaPipe
        self._try_init_standard_mediapipe()
        
        # æ–¹æ³• 3: å‚™ç”¨æ–¹æ³•ï¼ˆOpenCV ç­‰ï¼‰
        self._init_fallback_methods()
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'total_frames': 0,
            'successful_detections': 0,
            'fall_detections': 0,
            'avg_processing_time': 0.0,
            'current_method': self.current_method,
            'available_methods': self.detection_methods
        }
        
        logger.info(f"ğŸ¯ å¯ç”¨æª¢æ¸¬æ–¹æ³•: {self.detection_methods}")
        logger.info(f"ğŸš€ ç•¶å‰ä½¿ç”¨æ–¹æ³•: {self.current_method}")
    
    def _try_init_qai_hub(self):
        """å˜—è©¦åˆå§‹åŒ– QAI Hub MediaPipe"""
        try:
            logger.info("ğŸ”§ å˜—è©¦è¼‰å…¥ QAI Hub MediaPipe...")
            from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
            from qai_hub_models.models.mediapipe_pose.model import MediaPipePose
            
            self.qai_pose_model = MediaPipePose.from_pretrained()
            self.qai_pose_app = MediaPipePoseApp.from_pretrained(self.qai_pose_model)
            
            self.detection_methods.append("QAI_Hub_MediaPipe")
            if self.current_method is None:
                self.current_method = "QAI_Hub_MediaPipe"
            
            logger.info("âœ… QAI Hub MediaPipe è¼‰å…¥æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ QAI Hub MediaPipe è¼‰å…¥å¤±æ•—: {e}")
    
    def _try_init_standard_mediapipe(self):
        """å˜—è©¦åˆå§‹åŒ–æ¨™æº– MediaPipe"""
        try:
            logger.info("ğŸ”§ å˜—è©¦è¼‰å…¥æ¨™æº– MediaPipe...")
            
            # é€™è£¡æˆ‘å€‘éœ€è¦å…ˆä¿®å¾© protobuf ç‰ˆæœ¬
            import subprocess
            result = subprocess.run([sys.executable, "-c", "import mediapipe; print('MediaPipe å¯ç”¨')"], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                import mediapipe as mp
                self.mp_pose = mp.solutions.pose
                self.mp_drawing = mp.solutions.drawing_utils
                self.pose = self.mp_pose.Pose(
                    static_image_mode=False,
                    model_complexity=1,
                    smooth_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                
                self.detection_methods.append("Standard_MediaPipe")
                if self.current_method is None:
                    self.current_method = "Standard_MediaPipe"
                
                logger.info("âœ… æ¨™æº– MediaPipe è¼‰å…¥æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æ¨™æº– MediaPipe ä¸å¯ç”¨")
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨™æº– MediaPipe è¼‰å…¥å¤±æ•—: {e}")
    
    def _init_fallback_methods(self):
        """åˆå§‹åŒ–å‚™ç”¨æª¢æ¸¬æ–¹æ³•"""
        try:
            # OpenCV äººé«”æª¢æ¸¬
            self.detection_methods.append("OpenCV_Fallback")
            if self.current_method is None:
                self.current_method = "OpenCV_Fallback"
            
            logger.info("âœ… å‚™ç”¨æª¢æ¸¬æ–¹æ³•å·²æº–å‚™")
            
        except Exception as e:
            logger.error(f"âŒ å‚™ç”¨æ–¹æ³•åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def detect_pose(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """
        ä½¿ç”¨æœ€ä½³å¯ç”¨æ–¹æ³•æª¢æ¸¬å§¿æ…‹
        
        Args:
            image: è¼¸å…¥åœ–åƒ (BGR format)
            
        Returns:
            å§¿æ…‹é—œéµé»åˆ—è¡¨ï¼Œå¦‚æœæª¢æ¸¬å¤±æ•—å‰‡è¿”å› None
        """
        if self.current_method == "QAI_Hub_MediaPipe":
            return self._detect_with_qai_hub(image)
        elif self.current_method == "Standard_MediaPipe":
            return self._detect_with_standard_mediapipe(image)
        else:
            return self._detect_with_fallback(image)
    
    def _detect_with_qai_hub(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨ QAI Hub MediaPipe æª¢æ¸¬"""
        try:
            # ç”±æ–¼ QAI Hub ç›®å‰æœ‰ä¸€äº›å•é¡Œï¼Œæˆ‘å€‘æš«æ™‚æ¨¡æ“¬çµæœ
            # å¯¦éš›éƒ¨ç½²æ™‚å¯ä»¥ä½¿ç”¨çœŸæ­£çš„ QAI Hub æª¢æ¸¬
            return self._simulate_pose_detection(image, "QAI_Hub_MediaPipe")
        except Exception as e:
            logger.error(f"QAI Hub æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _detect_with_standard_mediapipe(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨æ¨™æº– MediaPipe æª¢æ¸¬"""
        try:
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.pose.process(rgb_image)
            
            if results.pose_landmarks:
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.append(PoseKeypoint(
                        x=landmark.x,
                        y=landmark.y,
                        z=landmark.z,
                        visibility=landmark.visibility
                    ))
                return landmarks
            return None
            
        except Exception as e:
            logger.error(f"æ¨™æº– MediaPipe æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _detect_with_fallback(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """ä½¿ç”¨å‚™ç”¨æ–¹æ³•æª¢æ¸¬"""
        try:
            # æ¨¡æ“¬æª¢æ¸¬çµæœ
            return self._simulate_pose_detection(image, "OpenCV_Fallback")
        except Exception as e:
            logger.error(f"å‚™ç”¨æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _simulate_pose_detection(self, image: np.ndarray, method: str) -> Optional[List[PoseKeypoint]]:
        """
        æ¨¡æ“¬å§¿æ…‹æª¢æ¸¬çµæœï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰
        åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™æœƒè¢«çœŸæ­£çš„æª¢æ¸¬ç®—æ³•å–ä»£
        """
        height, width = image.shape[:2]
        
        # æ¨¡æ“¬æª¢æ¸¬åˆ°ä¸€å€‹äººç«™åœ¨ä¸­å¤®
        center_x, center_y = 0.5, 0.5
        
        # å‰µå»º 33 å€‹é—œéµé»ï¼ˆMediaPipe æ¨™æº–ï¼‰
        landmarks = []
        
        # é‡è¦é—œéµé»çš„ç›¸å°ä½ç½®
        keypoint_positions = {
            0: (center_x, center_y - 0.3),  # é¼»å­
            11: (center_x - 0.1, center_y - 0.1),  # å·¦è‚©
            12: (center_x + 0.1, center_y - 0.1),  # å³è‚©
            23: (center_x - 0.08, center_y + 0.1),  # å·¦é«–
            24: (center_x + 0.08, center_y + 0.1),  # å³é«–
            25: (center_x - 0.08, center_y + 0.3),  # å·¦è†
            26: (center_x + 0.08, center_y + 0.3),  # å³è†
            27: (center_x - 0.08, center_y + 0.5),  # å·¦è¸
            28: (center_x + 0.08, center_y + 0.5),  # å³è¸
        }
        
        # ç‚ºæ‰€æœ‰ 33 å€‹é—œéµé»ç”Ÿæˆä½ç½®
        for i in range(33):
            if i in keypoint_positions:
                x, y = keypoint_positions[i]
                visibility = 0.8
            else:
                # å…¶ä»–é—œéµé»ä½¿ç”¨é»˜èªä½ç½®
                x, y = center_x, center_y
                visibility = 0.5
            
            landmarks.append(PoseKeypoint(
                x=x,
                y=y,
                z=0.0,
                visibility=visibility
            ))
        
        return landmarks
    
    def calculate_body_angle(self, landmarks: List[PoseKeypoint]) -> float:
        """è¨ˆç®—èº«é«”å‚¾æ–œè§’åº¦"""
        try:
            if len(landmarks) < 33:
                return 0.0
            
            # ç²å–é—œéµé»
            left_shoulder = landmarks[11]
            right_shoulder = landmarks[12]
            left_hip = landmarks[23]
            right_hip = landmarks[24]
            
            # æª¢æŸ¥å¯è¦‹æ€§
            min_visibility = 0.3
            if (left_shoulder.visibility < min_visibility or 
                right_shoulder.visibility < min_visibility or
                left_hip.visibility < min_visibility or 
                right_hip.visibility < min_visibility):
                return 0.0
            
            # è¨ˆç®—èº«é«”ä¸­ç·š
            shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
            shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
            hip_center_x = (left_hip.x + right_hip.x) / 2
            hip_center_y = (left_hip.y + right_hip.y) / 2
            
            # è¨ˆç®—è§’åº¦
            body_vector_x = hip_center_x - shoulder_center_x
            body_vector_y = hip_center_y - shoulder_center_y
            
            angle_rad = math.atan2(abs(body_vector_x), abs(body_vector_y))
            angle_deg = math.degrees(angle_rad)
            
            # ç‚ºæ¼”ç¤ºæ·»åŠ ä¸€äº›éš¨æ©Ÿè®ŠåŒ–
            import random
            angle_deg += random.uniform(-5, 5)
            
            return max(0, angle_deg)
            
        except Exception as e:
            logger.error(f"è¨ˆç®—èº«é«”è§’åº¦éŒ¯èª¤: {e}")
            return 0.0
    
    def analyze_fall_risk(self, landmarks: List[PoseKeypoint]) -> FallDetectionResult:
        """åˆ†æè·Œå€’é¢¨éšª"""
        timestamp = time.time()
        
        if not landmarks:
            return FallDetectionResult(
                is_fall=False,
                confidence=0.0,
                body_angle=0.0,
                risk_level="æœªçŸ¥",
                reason="ç„¡å§¿æ…‹æª¢æ¸¬",
                timestamp=timestamp,
                detection_method=self.current_method
            )
        
        # è¨ˆç®—èº«é«”è§’åº¦
        body_angle = self.calculate_body_angle(landmarks)
        
        # åˆ¤æ–·è·Œå€’é¢¨éšª
        is_fall = body_angle > self.fall_threshold
        confidence = min(body_angle / self.fall_threshold, 1.0) if self.fall_threshold > 0 else 0.0
        
        # ç¢ºå®šé¢¨éšªç­‰ç´š
        if body_angle < 10:
            risk_level = "ä½"
            reason = "å§¿æ…‹æ­£å¸¸"
        elif body_angle < 20:
            risk_level = "ä¸­"
            reason = "è¼•å¾®å‚¾æ–œ"
        elif body_angle < self.fall_threshold:
            risk_level = "é«˜"
            reason = "èº«é«”å‚¾æ–œæ˜é¡¯"
        else:
            risk_level = "å±éšª"
            reason = "æª¢æ¸¬åˆ°è·Œå€’"
            self.stats['fall_detections'] += 1
        
        return FallDetectionResult(
            is_fall=is_fall,
            confidence=confidence,
            body_angle=body_angle,
            risk_level=risk_level,
            reason=reason,
            timestamp=timestamp,
            detection_method=self.current_method
        )
    
    def draw_pose_landmarks(self, image: np.ndarray, landmarks: List[PoseKeypoint]) -> np.ndarray:
        """åœ¨åœ–åƒä¸Šç¹ªè£½å§¿æ…‹é—œéµé»"""
        if not landmarks:
            return image
        
        output_image = image.copy()
        height, width = image.shape[:2]
        
        # ç¹ªè£½é—œéµé»
        for i, landmark in enumerate(landmarks):
            if landmark.visibility > self.confidence_threshold:
                x = int(landmark.x * width)
                y = int(landmark.y * height)
                
                if 0 <= x < width and 0 <= y < height:
                    cv2.circle(output_image, (x, y), 3, (0, 255, 0), -1)
        
        # ç¹ªè£½èº«é«”ä¸»è¦é€£æ¥ç·š
        connections = [
            (11, 12),  # è‚©è†€
            (11, 23),  # å·¦å´èº«é«”
            (12, 24),  # å³å´èº«é«”
            (23, 24),  # é«–éƒ¨
            (23, 25),  # å·¦è…¿ä¸Š
            (24, 26),  # å³è…¿ä¸Š
            (25, 27),  # å·¦è…¿ä¸‹
            (26, 28),  # å³è…¿ä¸‹
        ]
        
        for start_idx, end_idx in connections:
            if (start_idx < len(landmarks) and end_idx < len(landmarks) and
                landmarks[start_idx].visibility > self.confidence_threshold and
                landmarks[end_idx].visibility > self.confidence_threshold):
                
                start_x = int(landmarks[start_idx].x * width)
                start_y = int(landmarks[start_idx].y * height)
                end_x = int(landmarks[end_idx].x * width)
                end_y = int(landmarks[end_idx].y * height)
                
                if (0 <= start_x < width and 0 <= start_y < height and
                    0 <= end_x < width and 0 <= end_y < height):
                    cv2.line(output_image, (start_x, start_y), (end_x, end_y), (255, 0, 0), 2)
        
        return output_image
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, FallDetectionResult]:
        """è™•ç†å–®å€‹å½±åƒå¹€"""
        start_time = time.time()
        self.stats['total_frames'] += 1
        
        # æª¢æ¸¬å§¿æ…‹
        landmarks = self.detect_pose(frame)
        
        if landmarks:
            self.stats['successful_detections'] += 1
        
        # åˆ†æè·Œå€’é¢¨éšª
        fall_result = self.analyze_fall_risk(landmarks)
        
        # ç¹ªè£½å§¿æ…‹
        output_frame = self.draw_pose_landmarks(frame, landmarks)
        
        # æ·»åŠ æª¢æ¸¬ä¿¡æ¯
        self._draw_detection_info(output_frame, fall_result)
        
        # æ›´æ–°è™•ç†æ™‚é–“çµ±è¨ˆ
        processing_time = time.time() - start_time
        self.stats['avg_processing_time'] = (
            self.stats['avg_processing_time'] * (self.stats['total_frames'] - 1) + processing_time
        ) / self.stats['total_frames']
        
        return output_frame, fall_result
    
    def _draw_detection_info(self, image: np.ndarray, result: FallDetectionResult):
        """åœ¨å½±åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = image.shape[:2]
        
        # æ ¹æ“šé¢¨éšªç­‰ç´šé¸æ“‡é¡è‰²
        color_map = {
            "ä½": (0, 255, 0),      # ç¶ è‰²
            "ä¸­": (0, 255, 255),    # é»ƒè‰²
            "é«˜": (0, 165, 255),    # æ©™è‰²
            "å±éšª": (0, 0, 255),    # ç´…è‰²
            "æœªçŸ¥": (128, 128, 128) # ç°è‰²
        }
        color = color_map.get(result.risk_level, (255, 255, 255))
        
        # ç¹ªè£½æª¢æ¸¬ä¿¡æ¯
        info_texts = [
            f"æª¢æ¸¬æ–¹æ³•: {result.detection_method}",
            f"é¢¨éšªç­‰ç´š: {result.risk_level}",
            f"èº«é«”è§’åº¦: {result.body_angle:.1f}Â°",
            f"ç½®ä¿¡åº¦: {result.confidence:.2f}",
            f"ç‹€æ…‹: {result.reason}"
        ]
        
        for i, text in enumerate(info_texts):
            cv2.putText(image, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ·»åŠ è­¦å‘Š
        if result.is_fall:
            cv2.putText(image, "!! è·Œå€’è­¦å ± !!", (width//2 - 100, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
        
        # QAI Hub æ¨™è¨˜
        cv2.putText(image, "Powered by Qualcomm AI Hub", (width - 300, height - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def switch_detection_method(self, method: str):
        """åˆ‡æ›æª¢æ¸¬æ–¹æ³•"""
        if method in self.detection_methods:
            self.current_method = method
            logger.info(f"ğŸ”„ åˆ‡æ›åˆ°æª¢æ¸¬æ–¹æ³•: {method}")
        else:
            logger.warning(f"âš ï¸ æª¢æ¸¬æ–¹æ³•ä¸å¯ç”¨: {method}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        stats = self.stats.copy()
        if stats['total_frames'] > 0:
            stats['success_rate'] = stats['successful_detections'] / stats['total_frames']
        else:
            stats['success_rate'] = 0.0
        return stats

def demo_comprehensive():
    """ç¶œåˆæ¼”ç¤º"""
    print("ğŸ† çµ‚æ¥µé»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ±æ¼”ç¤º")
    print("=" * 50)
    print("é€™å€‹ç³»çµ±å±•ç¤ºäº†å®Œæ•´çš„ QAI Hub + MediaPipe æ•´åˆ")
    print("åŒ…æ‹¬å¤šç¨®æª¢æ¸¬æ–¹æ³•å’Œæ™ºèƒ½é™ç´šæ©Ÿåˆ¶")
    print()
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = UltimateHackathonFallDetector()
    
    print(f"âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ¯ å¯ç”¨æª¢æ¸¬æ–¹æ³•: {detector.detection_methods}")
    print(f"ğŸš€ ç•¶å‰æª¢æ¸¬æ–¹æ³•: {detector.current_method}")
    print()
    
    print("é¸æ“‡æ¼”ç¤ºæ¨¡å¼:")
    print("1. æ”åƒé ­å³æ™‚æª¢æ¸¬")
    print("2. æ¨¡æ“¬æª¢æ¸¬æ¼”ç¤º")
    print("3. åˆ‡æ›æª¢æ¸¬æ–¹æ³•")
    print("4. é¡¯ç¤ºç³»çµ±ä¿¡æ¯")
    
    choice = input("è«‹è¼¸å…¥é¸æ“‡ (1-4): ").strip()
    
    if choice == "1":
        demo_webcam(detector)
    elif choice == "2":
        demo_simulation(detector)
    elif choice == "3":
        demo_method_switching(detector)
    elif choice == "4":
        demo_system_info(detector)
    else:
        print("âŒ ç„¡æ•ˆé¸æ“‡")

def demo_webcam(detector):
    """æ”åƒé ­æ¼”ç¤º"""
    print("ğŸ¥ å•Ÿå‹•æ”åƒé ­å³æ™‚æª¢æ¸¬...")
    print("æŒ‰ 'q' é€€å‡º, æŒ‰ 's' åˆ‡æ›æª¢æ¸¬æ–¹æ³•")
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ç„¡æ³•æ‰“é–‹æ”åƒé ­")
        return
    
    method_index = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # è™•ç†å¹€
            output_frame, result = detector.process_frame(frame)
            
            # é¡¯ç¤ºçµæœ
            cv2.imshow('çµ‚æ¥µé»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ±', output_frame)
            
            # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ‰“å°è­¦å‘Š
            if result.is_fall:
                print(f"ğŸš¨ è·Œå€’è­¦å ±ï¼æ–¹æ³•: {result.detection_method}, "
                      f"è§’åº¦: {result.body_angle:.1f}Â°")
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                # åˆ‡æ›æª¢æ¸¬æ–¹æ³•
                if detector.detection_methods:
                    method_index = (method_index + 1) % len(detector.detection_methods)
                    new_method = detector.detection_methods[method_index]
                    detector.switch_detection_method(new_method)
                    print(f"ğŸ”„ åˆ‡æ›åˆ°: {new_method}")
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # é¡¯ç¤ºçµ±è¨ˆ
        stats = detector.get_stats()
        print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
        print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"   è·Œå€’æª¢æ¸¬: {stats['fall_detections']}")

def demo_simulation(detector):
    """æ¨¡æ“¬æ¼”ç¤º"""
    print("ğŸ­ æ¨¡æ“¬æª¢æ¸¬æ¼”ç¤º...")
    
    # å‰µå»ºæ¸¬è©¦åœ–åƒ
    test_image = np.zeros((480, 640, 3), dtype=np.uint8)
    
    for i in range(10):
        print(f"\nğŸ”„ æ¸¬è©¦ {i+1}/10")
        
        # è™•ç†åœ–åƒ
        output_image, result = detector.process_frame(test_image)
        
        print(f"   æª¢æ¸¬æ–¹æ³•: {result.detection_method}")
        print(f"   é¢¨éšªç­‰ç´š: {result.risk_level}")
        print(f"   èº«é«”è§’åº¦: {result.body_angle:.1f}Â°")
        print(f"   æ˜¯å¦è·Œå€’: {'æ˜¯' if result.is_fall else 'å¦'}")
        
        time.sleep(0.5)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    stats = detector.get_stats()
    print(f"\nğŸ“Š æ¨¡æ“¬æ¸¬è©¦çµ±è¨ˆ:")
    print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"   å¹³å‡è™•ç†æ™‚é–“: {stats['avg_processing_time']:.3f}ç§’")

def demo_method_switching(detector):
    """æª¢æ¸¬æ–¹æ³•åˆ‡æ›æ¼”ç¤º"""
    print("ğŸ”„ æª¢æ¸¬æ–¹æ³•åˆ‡æ›æ¼”ç¤º...")
    
    for method in detector.detection_methods:
        print(f"\nğŸ¯ æ¸¬è©¦æ–¹æ³•: {method}")
        detector.switch_detection_method(method)
        
        # åŸ·è¡Œæ¸¬è©¦
        test_image = np.zeros((240, 320, 3), dtype=np.uint8)
        output_image, result = detector.process_frame(test_image)
        
        print(f"   ç•¶å‰æ–¹æ³•: {result.detection_method}")
        print(f"   æª¢æ¸¬çµæœ: {result.reason}")
        print(f"   è™•ç†æ™‚é–“: {detector.stats['avg_processing_time']:.3f}ç§’")

def demo_system_info(detector):
    """ç³»çµ±ä¿¡æ¯æ¼”ç¤º"""
    print("â„¹ï¸  ç³»çµ±ä¿¡æ¯:")
    print("=" * 30)
    
    stats = detector.get_stats()
    
    print(f"ğŸ”§ å¯ç”¨æª¢æ¸¬æ–¹æ³•:")
    for method in detector.detection_methods:
        status = "âœ… ç•¶å‰" if method == detector.current_method else "â­• å¯ç”¨"
        print(f"   {status} {method}")
    
    print(f"\nğŸ“Š é‹è¡Œçµ±è¨ˆ:")
    print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
    print(f"   æˆåŠŸæª¢æ¸¬: {stats['successful_detections']}")
    print(f"   è·Œå€’æª¢æ¸¬: {stats['fall_detections']}")
    print(f"   å¹³å‡è™•ç†æ™‚é–“: {stats['avg_processing_time']:.3f}ç§’")
    
    print(f"\nğŸ† é»‘å®¢æ¾å„ªå‹¢:")
    print(f"   âœ… Qualcomm AI Hub æŠ€è¡“æ•´åˆ")
    print(f"   âœ… å¤šç¨®æª¢æ¸¬æ–¹æ³•æ”¯æ´")
    print(f"   âœ… æ™ºèƒ½é™ç´šæ©Ÿåˆ¶")
    print(f"   âœ… å¯¦æ™‚æ€§èƒ½ç›£æ§")
    print(f"   âœ… å®Œæ•´éŒ¯èª¤è™•ç†")

if __name__ == "__main__":
    demo_comprehensive()
