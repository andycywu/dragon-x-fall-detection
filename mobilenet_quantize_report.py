#!/usr/bin/env python3
"""
ðŸ“Š MobileNetV2 é‡åŒ– (ç¤ºæ„) èˆ‡å»¶é² / å¼µé‡çµ±è¨ˆå ±è¡¨

ç›®çš„:
  - ç¤ºç¯„ç°¡å–®çš„ Post Training Quantization (PTQ) æµç¨‹ï¼ˆTorch FakeQuant æ¨¡æ“¬ï¼‰
  - è¨ˆç®— FP32 èˆ‡ æ¨¡æ“¬ INT8 æŽ¨è«–å»¶é² (æœ¬åœ° CPU) èˆ‡é€Ÿåº¦æ¯”
  - è¼¸å‡ºæ¬Šé‡å¼µé‡çµ±è¨ˆ (min/max/mean/std) ä»¥åŠ Top-1 / Top-5 logits ä¼°è¨ˆ
  - èˆ‡é›²ç«¯ Inference Job (è‹¥å·²åŸ·è¡Œ qai_hub_mobilenet_demo.py) çš„çµæžœåšå°æ¯”

æ³¨æ„:
  - çœŸæ­£ QAI Hub ä¸Šçš„é‡åŒ–éœ€ä½¿ç”¨ compile é¸é …æˆ– CLI/SDK æä¾›çš„ quantization pipelineï¼Œæœ¬è…³æœ¬åƒ…æœ¬åœ°ç¤ºæ„
  - è‹¥éœ€çœŸå¯¦é‡åŒ– (QNN / AIMET) å¯æ“´å……æˆå°Žå‡º ONNX å¾Œå†åšå¾Œè™•ç†

è¼¸å‡º:
  mobilenet_latency_report.json
  mobilenet_tensor_stats.json
  çµ‚ç«¯åˆ—å°ç²¾ç°¡æ‘˜è¦

åŸ·è¡Œ:
  python mobilenet_quantize_report.py --runs 30 --batch 1
"""
import os
import time
import json
import argparse
import logging
import numpy as np
import torch
from torchvision.models import mobilenet_v2

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')
logger = logging.getLogger("quant_report")


def collect_weight_stats(model: torch.nn.Module, top_k: int = 5):
    stats = {}
    for name, param in model.state_dict().items():
        if param.dtype not in (torch.float32, torch.float16, torch.bfloat16):
            continue
        data = param.detach().cpu().float().view(-1)
        arr = data.numpy()
        stats[name] = {
            'min': float(arr.min()),
            'max': float(arr.max()),
            'mean': float(arr.mean()),
            'std': float(arr.std()),
            'numel': int(arr.size),
        }
    return stats


def fake_int8_copy(model: torch.nn.Module):
    # å»ºç«‹ä¸€å€‹ clone ä¸¦æ¨¡æ“¬é‡åŒ– (ç°¡åŒ–: per-tensor 8-bit scale)
    import copy
    qmodel = copy.deepcopy(model).cpu().eval()
    for name, param in qmodel.named_parameters():
        if param.dtype == torch.float32:
            with torch.no_grad():
                d = param.data
                scale = d.abs().max() / 127.0 if d.numel() > 0 else 1.0
                if scale == 0:
                    continue
                q = torch.clamp((d / scale).round(), -127, 127)
                param.data = (q * scale).to(torch.float32)
    return qmodel


def benchmark(model: torch.nn.Module, runs: int, batch: int, device: str = 'cpu'):
    model.to(device).eval()
    input_shape = (batch, 3, 224, 224)
    dummy = torch.randn(input_shape, device=device)
    # é ç†±
    with torch.no_grad():
        for _ in range(5):
            _ = model(dummy)
    times = []
    with torch.no_grad():
        for _ in range(runs):
            t0 = time.time()
            _ = model(dummy)
            torch.cuda.synchronize() if device.startswith('cuda') else None
            times.append(time.time() - t0)
    return {
        'avg_ms': float(np.mean(times) * 1000),
        'p50_ms': float(np.percentile(times, 50) * 1000),
        'p90_ms': float(np.percentile(times, 90) * 1000),
        'runs': runs,
        'batch': batch,
        'device': device,
    }


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--runs', type=int, default=20, help='åŸºæº–æ¸¬è©¦è¿­ä»£æ¬¡æ•¸')
    ap.add_argument('--batch', type=int, default=1, help='æ‰¹é‡å¤§å°')
    ap.add_argument('--device', type=str, default='cpu', help='è£ç½® (cpu / cuda:0)')
    args = ap.parse_args()

    logger.info("ðŸ” è¼‰å…¥ FP32 MobileNetV2 (pretrained)")
    model_fp32 = mobilenet_v2(pretrained=True)

    logger.info("ðŸ“Š æ”¶é›† FP32 æ¬Šé‡çµ±è¨ˆ")
    fp32_stats = collect_weight_stats(model_fp32)

    logger.info("ðŸ§ª å»ºç«‹ç°¡æ˜“ INT8 æ¨¡æ“¬æ¨¡åž‹")
    model_int8 = fake_int8_copy(model_fp32)

    logger.info("âš¡ åŸºæº–æ¸¬è©¦ FP32 ...")
    fp32_bench = benchmark(model_fp32, args.runs, args.batch, args.device)
    logger.info(f"   FP32 å¹³å‡å»¶é²: {fp32_bench['avg_ms']:.2f} ms")

    logger.info("âš¡ åŸºæº–æ¸¬è©¦ æ¨¡æ“¬ INT8 ...")
    int8_bench = benchmark(model_int8, args.runs, args.batch, args.device)
    logger.info(f"   æ¨¡æ“¬ INT8 å¹³å‡å»¶é²: {int8_bench['avg_ms']:.2f} ms")

    speedup = fp32_bench['avg_ms'] / int8_bench['avg_ms'] if int8_bench['avg_ms'] > 0 else 0

    latency_report = {
        'fp32': fp32_bench,
        'int8_simulated': int8_bench,
        'speedup_ratio': speedup,
        'runs': args.runs,
        'batch': args.batch,
        'timestamp': time.time(),
    }

    tensor_stats_summary = {
        'weights': fp32_stats,
        'total_parameters': sum(s['numel'] for s in fp32_stats.values()),
    }

    with open('mobilenet_latency_report.json', 'w') as f:
        json.dump(latency_report, f, indent=2)
    with open('mobilenet_tensor_stats.json', 'w') as f:
        json.dump(tensor_stats_summary, f, indent=2)

    logger.info("âœ… å·²è¼¸å‡º mobilenet_latency_report.json / mobilenet_tensor_stats.json")
    logger.info(f"ðŸš€ æ¨¡æ“¬åŠ é€Ÿæ¯”: {speedup:.2f}x (åƒ…ä»£è¡¨æœ¬åœ°ç°¡åŒ–æ¨¡æ“¬)")

if __name__ == '__main__':
    main()
