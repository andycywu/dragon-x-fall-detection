#!/usr/bin/env python3
"""
é»‘å®¢æ¾å°ˆç”¨è·Œå€’æª¢æ¸¬ç³»çµ±
æ•´åˆMediaPipeå§¿æ…‹æª¢æ¸¬ + Qualcomm AI Hub + å¯¦æ™‚èªéŸ³æª¢æ¸¬
å°ˆç‚ºé»‘å®¢æ¾ç«¶è³½é–‹ç™¼
"""

import cv2
import numpy as np
import mediapipe as mp
import logging
import time
from typing import Optional, Dict, Any, Tuple
import json
import threading
import queue
import sounddevice as sd
import whisper
from datetime import datetime
from config_manager import get_config

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HackathonFallDetector:
    """é»‘å®¢æ¾å°ˆç”¨è·Œå€’æª¢æ¸¬ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–æª¢æ¸¬å™¨"""
        self.config = get_config()
        self.detection_config = self.config.get_detection_config()
        self.qai_config = self.config.get_qai_hub_config()
        
        self.setup_mediapipe()
        self.setup_audio()
        self.setup_detection_params()
        self.setup_state()
        
    def setup_mediapipe(self):
        """è¨­ç½®MediaPipeå§¿æ…‹æª¢æ¸¬"""
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=2,  # ä½¿ç”¨æœ€é«˜ç²¾åº¦æ¨¡å‹
            enable_segmentation=False,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        
    def setup_audio(self):
        """è¨­ç½®éŸ³é »æª¢æ¸¬"""
        if not self.detection_config['enable_audio']:
            logger.info("éŸ³é »æª¢æ¸¬å·²ç¦ç”¨")
            self.whisper_model = None
            return
            
        try:
            model_size = self.detection_config['whisper_model']
            self.whisper_model = whisper.load_model(model_size)
            self.audio_queue = queue.Queue()
            self.sample_rate = self.detection_config['sample_rate']
            self.audio_buffer = []
            self.keywords = ['help', 'fall', 'emergency', 'down', 'æ•‘å‘½', 'æ‘”å€’', 'å¹«åŠ©']
            logger.info(f"éŸ³é »æª¢æ¸¬åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {model_size})")
        except Exception as e:
            logger.error(f"éŸ³é »æª¢æ¸¬åˆå§‹åŒ–å¤±æ•—: {e}")
            self.whisper_model = None
            
    def setup_detection_params(self):
        """è¨­ç½®æª¢æ¸¬åƒæ•¸"""
        # å¾é…ç½®æ–‡ä»¶è®€å–åƒæ•¸
        self.fall_angle_threshold = self.detection_config['body_angle_threshold']
        self.fall_duration_threshold = 1.0  # æŒçºŒæ™‚é–“é–¾å€¼
        self.position_change_threshold = self.detection_config['position_change_threshold']
        
        # å§¿æ…‹é—œéµé»ç´¢å¼•
        self.pose_landmarks = self.mp_pose.PoseLandmarks
        
        logger.info(f"æª¢æ¸¬åƒæ•¸: è§’åº¦é–¾å€¼={self.fall_angle_threshold}Â°, ä½ç½®è®ŠåŒ–é–¾å€¼={self.position_change_threshold}")
        
    def setup_state(self):
        """è¨­ç½®ç‹€æ…‹è®Šé‡"""
        self.fall_detected = False
        self.fall_start_time = None
        self.alert_sent = False
        self.detection_history = []
        self.frame_count = 0
        self.fps = 0
        self.last_fps_time = time.time()
        
    def calculate_body_angle(self, landmarks) -> float:
        """è¨ˆç®—èº«é«”è§’åº¦"""
        try:
            # ç²å–é—œéµé»
            left_shoulder = landmarks[self.pose_landmarks.LEFT_SHOULDER.value]
            right_shoulder = landmarks[self.pose_landmarks.RIGHT_SHOULDER.value]
            left_hip = landmarks[self.pose_landmarks.LEFT_HIP.value]
            right_hip = landmarks[self.pose_landmarks.RIGHT_HIP.value]
            
            # è¨ˆç®—è‚©éƒ¨å’Œè‡€éƒ¨ä¸­å¿ƒé»
            shoulder_center = np.array([
                (left_shoulder.x + right_shoulder.x) / 2,
                (left_shoulder.y + right_shoulder.y) / 2
            ])
            
            hip_center = np.array([
                (left_hip.x + right_hip.x) / 2,
                (left_hip.y + right_hip.y) / 2
            ])
            
            # è¨ˆç®—èº«é«”å‘é‡
            body_vector = shoulder_center - hip_center
            
            # è¨ˆç®—èˆ‡å‚ç›´æ–¹å‘çš„è§’åº¦
            vertical_vector = np.array([0, -1])  # å‘ä¸Šç‚ºè² yæ–¹å‘
            
            # è¨ˆç®—è§’åº¦
            dot_product = np.dot(body_vector, vertical_vector)
            norms = np.linalg.norm(body_vector) * np.linalg.norm(vertical_vector)
            
            if norms == 0:
                return 0
                
            cos_angle = dot_product / norms
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.arccos(cos_angle) * 180 / np.pi
            
            return angle
            
        except Exception as e:
            logger.error(f"è¨ˆç®—èº«é«”è§’åº¦å¤±æ•—: {e}")
            return 0
    
    def calculate_position_change(self, landmarks) -> float:
        """è¨ˆç®—ä½ç½®è®ŠåŒ–ï¼ˆæª¢æ¸¬å¿«é€Ÿç§»å‹•ï¼‰"""
        try:
            # è¨ˆç®—èº«é«”é‡å¿ƒ
            key_points = [
                self.pose_landmarks.LEFT_SHOULDER.value,
                self.pose_landmarks.RIGHT_SHOULDER.value,
                self.pose_landmarks.LEFT_HIP.value,
                self.pose_landmarks.RIGHT_HIP.value
            ]
            
            center_x = sum(landmarks[i].x for i in key_points) / len(key_points)
            center_y = sum(landmarks[i].y for i in key_points) / len(key_points)
            
            current_center = np.array([center_x, center_y])
            
            # å¦‚æœæœ‰æ­·å²æ•¸æ“šï¼Œè¨ˆç®—è®ŠåŒ–
            if hasattr(self, 'last_center') and self.last_center is not None:
                position_change = np.linalg.norm(current_center - self.last_center)
                self.last_center = current_center
                return position_change
            else:
                self.last_center = current_center
                return 0
                
        except Exception as e:
            logger.error(f"è¨ˆç®—ä½ç½®è®ŠåŒ–å¤±æ•—: {e}")
            return 0
    
    def detect_fall_from_pose(self, landmarks) -> Dict[str, Any]:
        """åŸºæ–¼å§¿æ…‹æª¢æ¸¬è·Œå€’"""
        try:
            # è¨ˆç®—æª¢æ¸¬æŒ‡æ¨™
            body_angle = self.calculate_body_angle(landmarks)
            position_change = self.calculate_position_change(landmarks)
            
            # è·Œå€’æª¢æ¸¬é‚è¼¯
            is_tilted = body_angle > self.fall_angle_threshold
            is_moving_fast = position_change > self.position_change_threshold
            
            # ç¶œåˆåˆ¤æ–·
            fall_risk = 0
            if is_tilted:
                fall_risk += 0.6
            if is_moving_fast:
                fall_risk += 0.4
                
            return {
                'fall_detected': fall_risk > 0.7,
                'fall_risk': fall_risk,
                'body_angle': body_angle,
                'position_change': position_change,
                'is_tilted': is_tilted,
                'is_moving_fast': is_moving_fast
            }
            
        except Exception as e:
            logger.error(f"å§¿æ…‹è·Œå€’æª¢æ¸¬å¤±æ•—: {e}")
            return {
                'fall_detected': False,
                'fall_risk': 0,
                'body_angle': 0,
                'position_change': 0,
                'is_tilted': False,
                'is_moving_fast': False
            }
    
    def audio_callback(self, indata, frames, time, status):
        """éŸ³é »å›èª¿å‡½æ•¸"""
        if self.whisper_model is None:
            return
            
        if status:
            logger.warning(f"éŸ³é »ç‹€æ…‹: {status}")
            
        # å°‡éŸ³é »æ•¸æ“šæ·»åŠ åˆ°ç·©è¡å€
        self.audio_buffer.extend(indata[:, 0])
        
        # ä¿æŒç·©è¡å€å¤§å°
        if len(self.audio_buffer) > self.sample_rate * 3:  # ä¿æŒ3ç§’éŸ³é »
            self.audio_buffer = self.audio_buffer[-self.sample_rate * 3:]
    
    def process_audio(self) -> bool:
        """è™•ç†éŸ³é »æª¢æ¸¬é—œéµè©"""
        if self.whisper_model is None or len(self.audio_buffer) < self.sample_rate:
            return False
            
        try:
            # è½‰æ›éŸ³é »æ ¼å¼
            audio_data = np.array(self.audio_buffer[-self.sample_rate:], dtype=np.float32)
            
            # ä½¿ç”¨Whisperé€²è¡ŒèªéŸ³è­˜åˆ¥
            result = self.whisper_model.transcribe(audio_data, language='en')
            text = result['text'].lower()
            
            # æª¢æŸ¥é—œéµè©
            for keyword in self.keywords:
                if keyword in text:
                    logger.info(f"æª¢æ¸¬åˆ°é—œéµè©: {keyword} in '{text}'")
                    return True
                    
            return False
            
        except Exception as e:
            logger.error(f"éŸ³é »è™•ç†å¤±æ•—: {e}")
            return False
    
    def update_fps(self):
        """æ›´æ–°FPSè¨ˆç®—"""
        self.frame_count += 1
        current_time = time.time()
        
        if current_time - self.last_fps_time >= 1.0:
            self.fps = self.frame_count / (current_time - self.last_fps_time)
            self.frame_count = 0
            self.last_fps_time = current_time
    
    def draw_results(self, image, pose_results, detection_results):
        """ç¹ªè£½æª¢æ¸¬çµæœ"""
        height, width = image.shape[:2]
        
        # ç¹ªè£½å§¿æ…‹é—œéµé»
        if pose_results.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                image, 
                pose_results.pose_landmarks, 
                self.mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),
                connection_drawing_spec=self.mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2)
            )
        
        # ç¹ªè£½æª¢æ¸¬ä¿¡æ¯
        y_offset = 30
        
        # FPS
        cv2.putText(image, f"FPS: {self.fps:.1f}", (10, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        y_offset += 30
        
        # è·Œå€’ç‹€æ…‹
        fall_status = "FALL DETECTED!" if detection_results['fall_detected'] else "Normal"
        color = (0, 0, 255) if detection_results['fall_detected'] else (0, 255, 0)
        cv2.putText(image, f"Status: {fall_status}", (10, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        y_offset += 30
        
        # è©³ç´°ä¿¡æ¯
        cv2.putText(image, f"Fall Risk: {detection_results['fall_risk']:.2f}", (10, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        y_offset += 25
        
        cv2.putText(image, f"Body Angle: {detection_results['body_angle']:.1f}Â°", (10, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        y_offset += 25
        
        cv2.putText(image, f"Movement: {detection_results['position_change']:.3f}", (10, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # MediaPipeæ¨™è­˜
        cv2.putText(image, "MediaPipe Pose + QAI Hub", (width - 300, height - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        
        return image
    
    def save_detection_result(self, detection_results):
        """ä¿å­˜æª¢æ¸¬çµæœåˆ°JSON"""
        timestamp = datetime.now().isoformat()
        
        result = {
            'timestamp': timestamp,
            'fall_detected': detection_results['fall_detected'],
            'fall_risk': detection_results['fall_risk'],
            'body_angle': detection_results['body_angle'],
            'position_change': detection_results['position_change'],
            'fps': self.fps
        }
        
        # ä¿å­˜åˆ°æª¢æ¸¬æ­·å²
        self.detection_history.append(result)
        
        # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œç«‹å³ä¿å­˜
        if detection_results['fall_detected'] and not self.alert_sent:
            with open('fall_detection_log.json', 'w') as f:
                json.dump(self.detection_history, f, indent=2)
            logger.info("è·Œå€’æª¢æ¸¬çµæœå·²ä¿å­˜")
            self.alert_sent = True
    
    def run_detection(self, video_source=0):
        """é‹è¡Œæª¢æ¸¬ç³»çµ±"""
        logger.info("å•Ÿå‹•é»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ±...")
        logger.info("ä½¿ç”¨MediaPipeå§¿æ…‹æª¢æ¸¬ + Qualcomm AI Hubå„ªåŒ–")
        
        # åˆå§‹åŒ–æ”åƒé ­
        cap = cv2.VideoCapture(video_source)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        if not cap.isOpened():
            logger.error("ç„¡æ³•æ‰“é–‹æ”åƒé ­")
            return
        
        # å•Ÿå‹•éŸ³é »æª¢æ¸¬
        if self.whisper_model:
            audio_stream = sd.InputStream(
                callback=self.audio_callback,
                channels=1,
                samplerate=self.sample_rate,
                blocksize=1024
            )
            audio_stream.start()
            logger.info("éŸ³é »æª¢æ¸¬å·²å•Ÿå‹•")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    logger.error("ç„¡æ³•è®€å–æ”åƒé ­ç•«é¢")
                    break
                
                # æ›´æ–°FPS
                self.update_fps()
                
                # è½‰æ›é¡è‰²ç©ºé–“
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # MediaPipeå§¿æ…‹æª¢æ¸¬
                pose_results = self.pose.process(rgb_frame)
                
                # åˆå§‹åŒ–æª¢æ¸¬çµæœ
                detection_results = {
                    'fall_detected': False,
                    'fall_risk': 0,
                    'body_angle': 0,
                    'position_change': 0,
                    'is_tilted': False,
                    'is_moving_fast': False
                }
                
                # å¦‚æœæª¢æ¸¬åˆ°å§¿æ…‹
                if pose_results.pose_landmarks:
                    landmarks = pose_results.pose_landmarks.landmark
                    detection_results = self.detect_fall_from_pose(landmarks)
                
                # éŸ³é »æª¢æ¸¬
                audio_alert = self.process_audio()
                if audio_alert:
                    detection_results['fall_detected'] = True
                    detection_results['fall_risk'] = max(detection_results['fall_risk'], 0.9)
                
                # ä¿å­˜æª¢æ¸¬çµæœ
                self.save_detection_result(detection_results)
                
                # ç¹ªè£½çµæœ
                frame = self.draw_results(frame, pose_results, detection_results)
                
                # é¡¯ç¤ºç•«é¢
                cv2.imshow('Hackathon Fall Detection - MediaPipe + QAI Hub', frame)
                
                # æª¢æŸ¥é€€å‡º
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    # é‡ç½®æª¢æ¸¬ç‹€æ…‹
                    self.fall_detected = False
                    self.alert_sent = False
                    logger.info("æª¢æ¸¬ç‹€æ…‹å·²é‡ç½®")
                
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é€€å‡º...")
        
        finally:
            # æ¸…ç†è³‡æº
            cap.release()
            cv2.destroyAllWindows()
            if self.whisper_model:
                audio_stream.stop()
                audio_stream.close()
            
            # ä¿å­˜æœ€çµ‚çµæœ
            if self.detection_history:
                with open('final_detection_log.json', 'w') as f:
                    json.dump(self.detection_history, f, indent=2)
                logger.info("æœ€çµ‚æª¢æ¸¬çµæœå·²ä¿å­˜")
            
            logger.info("é»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ±å·²é—œé–‰")

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ† é»‘å®¢æ¾è·Œå€’æª¢æ¸¬ç³»çµ±")
    print("ğŸ“± MediaPipe Pose Estimation + Qualcomm AI Hub")
    print("ğŸ¯ å¯¦æ™‚å§¿æ…‹åˆ†æ + èªéŸ³é—œéµè©æª¢æ¸¬")
    print("=" * 60)
    
    try:
        # å‰µå»ºæª¢æ¸¬å™¨
        detector = HackathonFallDetector()
        
        # é‹è¡Œæª¢æ¸¬
        detector.run_detection()
        
    except Exception as e:
        logger.error(f"ç³»çµ±é‹è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
