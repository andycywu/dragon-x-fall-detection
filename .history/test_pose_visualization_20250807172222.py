#!/usr/bin/env python3
"""
æ¸¬è©¦å§¿æ…‹å¯è¦–åŒ–åŠŸèƒ½
é©—è­‰æ‰€æœ‰å››ç¨®æª¢æ¸¬æ–¹æ³•çš„éª¨æ¶é¡¯ç¤º
"""

import cv2
import numpy as np
from pathlib import Path
import sys

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from completely_fixed_detector import CompletelyFixedHackathonDetector

def test_pose_visualization():
    """æ¸¬è©¦å§¿æ…‹å¯è¦–åŒ–åŠŸèƒ½"""
    
    print("=" * 80)
    print("ğŸ§ª æ¸¬è©¦å§¿æ…‹å¯è¦–åŒ–åŠŸèƒ½")
    print("=" * 80)
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = CompletelyFixedHackathonDetector()
    
    # è¼‰å…¥æ¸¬è©¦åœ–åƒ
    test_image_path = "assets/test_images/person_standing.jpg"
    if not Path(test_image_path).exists():
        print(f"âš ï¸ æ¸¬è©¦åœ–åƒä¸å­˜åœ¨: {test_image_path}")
        print("ä½¿ç”¨æ”åƒé ­é€²è¡Œæ¸¬è©¦...")
        
        # ä½¿ç”¨æ”åƒé ­
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âŒ ç„¡æ³•æ‰“é–‹æ”åƒé ­")
            return
        
        print("ğŸ“¹ ä½¿ç”¨æ”åƒé ­æ¸¬è©¦å§¿æ…‹å¯è¦–åŒ–...")
        print("æ§åˆ¶èªªæ˜:")
        print("  - æŒ‰ '1' æ¸¬è©¦ QAI Hub MediaPipe")
        print("  - æŒ‰ '2' æ¸¬è©¦ Standard MediaPipe") 
        print("  - æŒ‰ '3' æ¸¬è©¦ OpenCV Fallback")
        print("  - æŒ‰ '4' æ¸¬è©¦ Simulation Demo")
        print("  - æŒ‰ 'q' é€€å‡º")
        
        current_method = "QAI_Hub_MediaPipe"
        method_names = {
            "1": "QAI_Hub_MediaPipe",
            "2": "Standard_MediaPipe",
            "3": "OpenCV_Fallback", 
            "4": "Simulation_Demo"
        }
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æª¢æ¸¬å§¿æ…‹
            result = detector.detect_pose(frame, method=current_method)
            
            if result['success'] and result['landmarks']:
                # ç¹ªè£½å§¿æ…‹éª¨æ¶
                frame = draw_pose_skeleton(frame, result['landmarks'], current_method)
                
                # é¡¯ç¤ºæª¢æ¸¬ä¿¡æ¯
                info_text = f"Method: {current_method} | Keypoints: {len(result['landmarks'])}"
                cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                status_text = f"Status: {result.get('fall_risk', 'Unknown')}"
                cv2.putText(frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
            else:
                # é¡¯ç¤ºæª¢æ¸¬å¤±æ•—
                cv2.putText(frame, f"Detection Failed: {current_method}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            # é¡¯ç¤ºå¹€
            cv2.imshow("Pose Visualization Test", frame)
            
            # è™•ç†æŒ‰éµ
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif chr(key) in method_names:
                current_method = method_names[chr(key)]
                print(f"ğŸ”„ åˆ‡æ›åˆ°æª¢æ¸¬æ–¹æ³•: {current_method}")
        
        cap.release()
        cv2.destroyAllWindows()
    
    else:
        # ä½¿ç”¨éœæ…‹åœ–åƒæ¸¬è©¦
        image = cv2.imread(test_image_path)
        if image is None:
            print(f"âŒ ç„¡æ³•è¼‰å…¥åœ–åƒ: {test_image_path}")
            return
        
        print(f"ğŸ“¸ ä½¿ç”¨éœæ…‹åœ–åƒæ¸¬è©¦: {test_image_path}")
        
        # æ¸¬è©¦æ‰€æœ‰æª¢æ¸¬æ–¹æ³•
        methods = ["QAI_Hub_MediaPipe", "Standard_MediaPipe", "OpenCV_Fallback", "Simulation_Demo"]
        
        for i, method in enumerate(methods):
            print(f"\nğŸ§ª æ¸¬è©¦æ–¹æ³• {i+1}/4: {method}")
            
            result = detector.detect_pose(image.copy(), method=method)
            
            if result['success'] and result['landmarks']:
                print(f"  âœ… æª¢æ¸¬æˆåŠŸï¼Œé—œéµé»æ•¸é‡: {len(result['landmarks'])}")
                
                # ç¹ªè£½å§¿æ…‹éª¨æ¶
                vis_image = draw_pose_skeleton(image.copy(), result['landmarks'], method)
                
                # æ·»åŠ æ¨™é¡Œ
                title = f"{method} - {len(result['landmarks'])} keypoints"
                cv2.putText(vis_image, title, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                
                # é¡¯ç¤ºçµæœ
                cv2.imshow(f"Pose Test - {method}", vis_image)
                print(f"  ğŸ’¡ æŒ‰ä»»æ„éµç¹¼çºŒä¸‹ä¸€å€‹æ¸¬è©¦...")
                cv2.waitKey(0)
                cv2.destroyAllWindows()
            else:
                print(f"  âŒ æª¢æ¸¬å¤±æ•—: {result.get('error', 'Unknown error')}")
    
    print("\nğŸ‰ å§¿æ…‹å¯è¦–åŒ–æ¸¬è©¦å®Œæˆï¼")

def draw_pose_skeleton(frame: np.ndarray, landmarks: list, method: str) -> np.ndarray:
    """
    ç¹ªè£½å§¿æ…‹éª¨æ¶
    æ ¹æ“šä¸åŒçš„æª¢æ¸¬æ–¹æ³•ä½¿ç”¨å°æ‡‰çš„é€£æ¥æ–¹å¼
    """
    if not landmarks:
        return frame
    
    # MediaPipe å§¿æ…‹é€£æ¥ (33å€‹é—œéµé»)
    MEDIAPIPE_POSE_CONNECTIONS = [
        # è‡‰éƒ¨è¼ªå»“
        (0, 1), (1, 2), (2, 3), (3, 7),
        (0, 4), (4, 5), (5, 6), (6, 8),
        (9, 10),
        # è»€å¹¹
        (11, 12), (11, 13), (12, 14), (13, 15), (14, 16),
        (11, 23), (12, 24), (23, 24),
        # æ‰‹è‡‚
        (15, 17), (15, 19), (15, 21), (16, 18), (16, 20), (16, 22),
        (17, 19), (18, 20), (19, 21), (20, 22),
        # è…¿éƒ¨
        (23, 25), (24, 26), (25, 27), (26, 28),
        (27, 29), (28, 30), (29, 31), (30, 32),
        (27, 31), (28, 32)
    ]
    
    # OpenCV/COCO å§¿æ…‹é€£æ¥ (25å€‹é—œéµé»)
    OPENCV_POSE_CONNECTIONS = [
        # è»€å¹¹
        (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7),
        (1, 8), (8, 9), (9, 10), (1, 11), (11, 12), (12, 13),
        # æ‰‹è‡‚
        (1, 0), (0, 14), (14, 16), (0, 15), (15, 17),
        # è…¿éƒ¨
        (8, 9), (9, 10), (11, 12), (12, 13),
        (10, 20), (10, 21), (13, 22), (13, 23)
    ]
    
    # é¸æ“‡é€£æ¥æ–¹å¼
    if method in ["QAI_Hub_MediaPipe", "Standard_MediaPipe"]:
        connections = MEDIAPIPE_POSE_CONNECTIONS
        point_color = (0, 255, 0)  # ç¶ è‰²
        line_color = (255, 255, 0)  # é’è‰²
    elif method == "OpenCV_Fallback":
        connections = OPENCV_POSE_CONNECTIONS
        point_color = (255, 0, 0)  # è—è‰²
        line_color = (0, 255, 255)  # é»ƒè‰²
    else:  # Simulation_Demo
        connections = MEDIAPIPE_POSE_CONNECTIONS
        point_color = (255, 0, 255)  # ç´«è‰²
        line_color = (0, 255, 255)  # é»ƒè‰²
    
    h, w = frame.shape[:2]
    
    # ç¹ªè£½é€£æ¥ç·š
    for connection in connections:
        if connection[0] < len(landmarks) and connection[1] < len(landmarks):
            pt1 = landmarks[connection[0]]
            pt2 = landmarks[connection[1]]
            
            # è½‰æ›åæ¨™
            if isinstance(pt1, tuple) and len(pt1) >= 2:
                x1, y1 = int(pt1[0] * w), int(pt1[1] * h)
                x2, y2 = int(pt2[0] * w), int(pt2[1] * h)
                
                # æª¢æŸ¥åæ¨™æœ‰æ•ˆæ€§
                if 0 <= x1 < w and 0 <= y1 < h and 0 <= x2 < w and 0 <= y2 < h:
                    cv2.line(frame, (x1, y1), (x2, y2), line_color, 2)
    
    # ç¹ªè£½é—œéµé»
    for i, landmark in enumerate(landmarks):
        if isinstance(landmark, tuple) and len(landmark) >= 2:
            x, y = int(landmark[0] * w), int(landmark[1] * h)
            
            if 0 <= x < w and 0 <= y < h:
                cv2.circle(frame, (x, y), 4, point_color, -1)
                cv2.circle(frame, (x, y), 4, (255, 255, 255), 1)
                
                # å¯é¸ï¼šé¡¯ç¤ºé—œéµé»ç·¨è™Ÿ
                cv2.putText(frame, str(i), (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
    
    return frame

if __name__ == "__main__":
    test_pose_visualization()
