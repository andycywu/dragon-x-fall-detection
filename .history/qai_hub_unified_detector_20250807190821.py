#!/usr/bin/env python3
"""
ðŸš€ QAI Hub çµ±ä¸€æª¢æ¸¬ç³»çµ±
ä½¿ç”¨Qualcomm AI Hubå„ªåŒ–çš„æ¨¡åž‹é€²è¡Œäººè‡‰æª¢æ¸¬ã€å§¿æ…‹æª¢æ¸¬ç­‰
"""

import cv2
import numpy as np
import time
import os
from typing import Dict, List, Tuple, Any, Optional
import logging
from PIL import Image

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QAIHubUnifiedDetector:
    """QAI Hubçµ±ä¸€æª¢æ¸¬ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–QAI Hubæª¢æ¸¬å™¨"""
        self.face_detector = None
        self.pose_detector = None
        self.selfie_detector = None
        
        logger.info("ðŸš€ åˆå§‹åŒ– QAI Hub çµ±ä¸€æª¢æ¸¬ç³»çµ±...")
        self._init_face_detection()
        self._init_pose_detection()
        self._init_selfie_detection()
        logger.info("âœ… QAI Hub çµ±ä¸€æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _init_face_detection(self):
        """åˆå§‹åŒ–QAI Hubäººè‡‰æª¢æ¸¬"""
        try:
            from qai_hub_models.models.mediapipe_face.app import MediaPipeFaceApp
            from qai_hub_models.models.mediapipe_face.model import MediaPipeFace
            
            face_model = MediaPipeFace.from_pretrained()
            self.face_detector = MediaPipeFaceApp.from_pretrained(face_model)
            logger.info("âœ… QAI Hub MediaPipe Face æª¢æ¸¬å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ QAI Hub äººè‡‰æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_pose_detection(self):
        """åˆå§‹åŒ–QAI Hubå§¿æ…‹æª¢æ¸¬"""
        try:
            from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
            from qai_hub_models.models.mediapipe_pose.model import MediaPipePose
            
            pose_model = MediaPipePose.from_pretrained()
            self.pose_detector = MediaPipePoseApp.from_pretrained(pose_model)
            logger.info("âœ… QAI Hub MediaPipe Pose æª¢æ¸¬å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ QAI Hub å§¿æ…‹æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_selfie_detection(self):
        """åˆå§‹åŒ–QAI Hub Selfieæª¢æ¸¬ï¼ˆäººåƒåˆ†å‰²ï¼‰"""
        try:
            from qai_hub_models.models.mediapipe_selfie.app import MediaPipeSelfieApp
            from qai_hub_models.models.mediapipe_selfie.model import MediaPipeSelfie
            
            selfie_model = MediaPipeSelfie.from_pretrained()
            self.selfie_detector = MediaPipeSelfieApp.from_pretrained(selfie_model)
            logger.info("âœ… QAI Hub MediaPipe Selfie æª¢æ¸¬å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ QAI Hub Selfie æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def detect_faces(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ä½¿ç”¨QAI Hubæª¢æ¸¬äººè‡‰"""
        if self.face_detector is None:
            return []
        
        try:
            # è½‰æ›ç‚ºRGB PILåœ–åƒ
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
            
            # ä½¿ç”¨QAI Hubæª¢æ¸¬äººè‡‰
            result = self.face_detector.predict_landmarks_from_image(pil_image)
            
            faces = []
            if result and len(result) > 0:
                # è§£æžäººè‡‰æª¢æ¸¬çµæžœ
                face_landmarks = result[0] if isinstance(result, list) else result
                
                if hasattr(face_landmarks, 'landmark') and face_landmarks.landmark:
                    # è¨ˆç®—äººè‡‰é‚Šç•Œæ¡†
                    landmarks = face_landmarks.landmark
                    h, w = image.shape[:2]
                    
                    x_coords = [int(lm.x * w) for lm in landmarks]
                    y_coords = [int(lm.y * h) for lm in landmarks]
                    
                    if x_coords and y_coords:
                        x_min, x_max = min(x_coords), max(x_coords)
                        y_min, y_max = min(y_coords), max(y_coords)
                        
                        # æ·»åŠ é‚Šè·
                        margin = 20
                        x_min = max(0, x_min - margin)
                        y_min = max(0, y_min - margin)
                        x_max = min(w, x_max + margin)
                        y_max = min(h, y_max + margin)
                        
                        faces.append({
                            'bbox': [x_min, y_min, x_max - x_min, y_max - y_min],
                            'landmarks': [(int(lm.x * w), int(lm.y * h)) for lm in landmarks],
                            'confidence': 0.95  # QAI Hubé€šå¸¸æœ‰å¾ˆé«˜çš„ç½®ä¿¡åº¦
                        })
            
            return faces
            
        except Exception as e:
            logger.error(f"âŒ QAI Hub äººè‡‰æª¢æ¸¬å¤±æ•—: {e}")
            return []
    
    def detect_pose(self, image: np.ndarray) -> Dict[str, Any]:
        """ä½¿ç”¨QAI Hubæª¢æ¸¬å§¿æ…‹"""
        if self.pose_detector is None:
            return {'success': False, 'landmarks': [], 'info': 'QAI Hubå§¿æ…‹æª¢æ¸¬å™¨æœªåˆå§‹åŒ–'}
        
        try:
            # è½‰æ›ç‚ºRGB PILåœ–åƒ
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
            
            # ä½¿ç”¨QAI Hubæª¢æ¸¬å§¿æ…‹
            result = self.pose_detector.predict_landmarks_from_image(pil_image, raw_output=True)
            
            if result and len(result) >= 4:
                # è§£æžå§¿æ…‹æª¢æ¸¬çµæžœ
                pose_landmarks = result[0]
                
                landmarks = []
                h, w = image.shape[:2]
                
                if hasattr(pose_landmarks, 'landmark') and pose_landmarks.landmark:
                    for lm in pose_landmarks.landmark:
                        landmarks.append((float(lm.x * w), float(lm.y * h)))
                
                return {
                    'success': True,
                    'landmarks': landmarks,
                    'info': f'QAI Hubæª¢æ¸¬åˆ° {len(landmarks)} å€‹å§¿æ…‹é—œéµé»ž'
                }
            else:
                return {'success': False, 'landmarks': [], 'info': 'QAI Hubæœªæª¢æ¸¬åˆ°å§¿æ…‹'}
            
        except Exception as e:
            logger.error(f"âŒ QAI Hub å§¿æ…‹æª¢æ¸¬å¤±æ•—: {e}")
            return {'success': False, 'landmarks': [], 'info': str(e)}
    
    def detect_person_segmentation(self, image: np.ndarray) -> Dict[str, Any]:
        """ä½¿ç”¨QAI Hubé€²è¡Œäººåƒåˆ†å‰²"""
        if self.selfie_detector is None:
            return {'success': False, 'mask': None, 'info': 'QAI Hub Selfieæª¢æ¸¬å™¨æœªåˆå§‹åŒ–'}
        
        try:
            # è½‰æ›ç‚ºRGB PILåœ–åƒ
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
            
            # ä½¿ç”¨QAI Hubé€²è¡Œäººåƒåˆ†å‰²
            result = self.selfie_detector.predict_mask_from_image(pil_image)
            
            if result is not None:
                # è½‰æ›åˆ†å‰²æŽ©ç¢¼
                if hasattr(result, 'numpy'):
                    mask = result.numpy()
                else:
                    mask = np.array(result)
                
                return {
                    'success': True,
                    'mask': mask,
                    'info': f'QAI Hubäººåƒåˆ†å‰²æˆåŠŸï¼ŒæŽ©ç¢¼å°ºå¯¸: {mask.shape}'
                }
            else:
                return {'success': False, 'mask': None, 'info': 'QAI Hubäººåƒåˆ†å‰²å¤±æ•—'}
            
        except Exception as e:
            logger.error(f"âŒ QAI Hub äººåƒåˆ†å‰²å¤±æ•—: {e}")
            return {'success': False, 'mask': None, 'info': str(e)}
    
    def unified_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """çµ±ä¸€æª¢æ¸¬ï¼šäººè‡‰ã€å§¿æ…‹ã€äººåƒåˆ†å‰²"""
        results = {
            'timestamp': time.time(),
            'faces': [],
            'pose': {'success': False, 'landmarks': []},
            'segmentation': {'success': False, 'mask': None},
            'summary': {}
        }
        
        # 1. äººè‡‰æª¢æ¸¬
        start_time = time.time()
        faces = self.detect_faces(image)
        face_time = time.time() - start_time
        results['faces'] = faces
        
        # 2. å§¿æ…‹æª¢æ¸¬
        start_time = time.time()
        pose_result = self.detect_pose(image)
        pose_time = time.time() - start_time
        results['pose'] = pose_result
        
        # 3. äººåƒåˆ†å‰²
        start_time = time.time()
        seg_result = self.detect_person_segmentation(image)
        seg_time = time.time() - start_time
        results['segmentation'] = seg_result
        
        # ç”Ÿæˆæ‘˜è¦
        results['summary'] = {
            'faces_detected': len(faces),
            'pose_detected': pose_result['success'],
            'person_segmented': seg_result['success'],
            'processing_time': {
                'face_detection': face_time,
                'pose_detection': pose_time,
                'segmentation': seg_time,
                'total': face_time + pose_time + seg_time
            }
        }
        
        return results
    
    def extract_face_encoding(self, image: np.ndarray, face_bbox: List[int]) -> Optional[np.ndarray]:
        """å¾žæª¢æ¸¬åˆ°çš„äººè‡‰æå–ç‰¹å¾µç·¨ç¢¼ï¼ˆç”¨æ–¼äººè‡‰è­˜åˆ¥ï¼‰"""
        try:
            # æå–äººè‡‰å€åŸŸ
            x, y, w, h = face_bbox
            face_image = image[y:y+h, x:x+w]
            
            if face_image.size == 0:
                return None
            
            # ä½¿ç”¨QAI Hubäººè‡‰æª¢æ¸¬ç²å–æ›´è©³ç´°çš„ç‰¹å¾µ
            faces = self.detect_faces(face_image)
            
            if faces and len(faces) > 0:
                # å°‡äººè‡‰landmarksè½‰æ›ç‚ºç‰¹å¾µå‘é‡
                landmarks = faces[0].get('landmarks', [])
                if len(landmarks) > 0:
                    # ç°¡åŒ–çš„ç‰¹å¾µç·¨ç¢¼ï¼ˆåŸºæ–¼é—œéµé»žè·é›¢å’Œè§’åº¦ï¼‰
                    encoding = self._landmarks_to_encoding(landmarks)
                    return encoding
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ äººè‡‰ç·¨ç¢¼æå–å¤±æ•—: {e}")
            return None
    
    def _landmarks_to_encoding(self, landmarks: List[Tuple[int, int]]) -> np.ndarray:
        """å°‡äººè‡‰é—œéµé»žè½‰æ›ç‚ºç‰¹å¾µç·¨ç¢¼"""
        try:
            # è¨ˆç®—é—œéµé»žä¹‹é–“çš„è·é›¢å’Œè§’åº¦ä½œç‚ºç‰¹å¾µ
            features = []
            
            if len(landmarks) >= 5:  # è‡³å°‘éœ€è¦5å€‹é—œéµé»ž
                # è¨ˆç®—çœ¼ç›ã€é¼»å­ã€å˜´å·´ä¹‹é–“çš„è·é›¢
                for i in range(len(landmarks)):
                    for j in range(i+1, min(i+10, len(landmarks))):  # åªè¨ˆç®—é™„è¿‘çš„é»ž
                        p1, p2 = landmarks[i], landmarks[j]
                        dist = np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                        features.append(dist)
                
                # æ­£è¦åŒ–ç‰¹å¾µ
                features = np.array(features)
                if len(features) > 0:
                    features = features / np.linalg.norm(features)
                
                # å¡«å……æˆ–æˆªæ–·åˆ°å›ºå®šé•·åº¦ï¼ˆ128ç¶­ï¼Œé¡žä¼¼face_recognitionï¼‰
                target_size = 128
                if len(features) < target_size:
                    # å¡«å……0
                    encoding = np.zeros(target_size)
                    encoding[:len(features)] = features
                else:
                    # æˆªæ–·
                    encoding = features[:target_size]
                
                return encoding
            
            # å¦‚æžœé—œéµé»žä¸è¶³ï¼Œè¿”å›žé›¶å‘é‡
            return np.zeros(128)
            
        except Exception as e:
            logger.error(f"âŒ é—œéµé»žç‰¹å¾µç·¨ç¢¼å¤±æ•—: {e}")
            return np.zeros(128)

def demo_qai_hub_detection():
    """æ¼”ç¤ºQAI Hubçµ±ä¸€æª¢æ¸¬"""
    print("ðŸš€ QAI Hub çµ±ä¸€æª¢æ¸¬æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = QAIHubUnifiedDetector()
    
    # æ¸¬è©¦åœ–åƒæª¢æ¸¬
    test_images = ["andy.jpg", "official_test_image.jpg"]
    
    for img_path in test_images:
        if os.path.exists(img_path):
            print(f"\nðŸ“· æ¸¬è©¦åœ–åƒ: {img_path}")
            
            # è¼‰å…¥åœ–åƒ
            image = cv2.imread(img_path)
            if image is None:
                print(f"âŒ ç„¡æ³•è¼‰å…¥åœ–åƒ: {img_path}")
                continue
            
            # çµ±ä¸€æª¢æ¸¬
            results = detector.unified_detection(image)
            
            # é¡¯ç¤ºçµæžœ
            print(f"  ðŸ‘¤ æª¢æ¸¬åˆ°äººè‡‰: {results['summary']['faces_detected']} å€‹")
            print(f"  ðŸ¤¸â€â™€ï¸ å§¿æ…‹æª¢æ¸¬: {'âœ…' if results['summary']['pose_detected'] else 'âŒ'}")
            print(f"  ðŸŽ­ äººåƒåˆ†å‰²: {'âœ…' if results['summary']['person_segmented'] else 'âŒ'}")
            print(f"  â±ï¸ ç¸½è™•ç†æ™‚é–“: {results['summary']['processing_time']['total']:.3f}ç§’")
            
            # å¯è¦–åŒ–çµæžœ
            display_image = image.copy()
            
            # ç¹ªè£½äººè‡‰æ¡†
            for face in results['faces']:
                x, y, w, h = face['bbox']
                cv2.rectangle(display_image, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(display_image, f"Face {face['confidence']:.2f}", 
                           (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            
            # ç¹ªè£½å§¿æ…‹é—œéµé»ž
            if results['pose']['success']:
                landmarks = results['pose']['landmarks']
                for i, (x, y) in enumerate(landmarks):
                    cv2.circle(display_image, (int(x), int(y)), 3, (255, 0, 0), -1)
                    if i < 10:  # åªæ¨™è¨»å‰10å€‹é»žé¿å…å¤ªäº‚
                        cv2.putText(display_image, str(i), (int(x)+5, int(y)), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)
            
            # ä¿å­˜çµæžœ
            output_path = f"qai_hub_result_{os.path.basename(img_path)}"
            cv2.imwrite(output_path, display_image)
            print(f"  ðŸ’¾ çµæžœå·²ä¿å­˜: {output_path}")

def test_live_detection():
    """æ¸¬è©¦å³æ™‚æª¢æ¸¬"""
    print("\nðŸŽ¥ QAI Hub å³æ™‚æª¢æ¸¬æ¸¬è©¦")
    
    detector = QAIHubUnifiedDetector()
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ç„¡æ³•é–‹å•Ÿæ”åƒé ­")
        return
    
    print("æŒ‰ 'q' é€€å‡ºå³æ™‚æª¢æ¸¬")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ç¸®å°åœ–åƒä»¥æé«˜è™•ç†é€Ÿåº¦
            small_frame = cv2.resize(frame, (640, 480))
            
            # QAI Hubçµ±ä¸€æª¢æ¸¬
            results = detector.unified_detection(small_frame)
            
            # ç¹ªè£½æª¢æ¸¬çµæžœ
            display_frame = small_frame.copy()
            
            # äººè‡‰æ¡†
            for face in results['faces']:
                x, y, w, h = face['bbox']
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(display_frame, "QAI Face", (x, y-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # å§¿æ…‹é—œéµé»ž
            if results['pose']['success']:
                landmarks = results['pose']['landmarks']
                for x, y in landmarks[:11]:  # åªé¡¯ç¤ºä¸»è¦é—œéµé»ž
                    cv2.circle(display_frame, (int(x), int(y)), 4, (255, 0, 0), -1)
            
            # é¡¯ç¤ºè™•ç†æ™‚é–“
            total_time = results['summary']['processing_time']['total']
            fps = 1.0 / max(total_time, 0.001)
            cv2.putText(display_frame, f"QAI Hub FPS: {fps:.1f}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.imshow('QAI Hub Unified Detection', display_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    
    finally:
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    # é‹è¡Œæ¼”ç¤º
    demo_qai_hub_detection()
    
    # è©¢å•æ˜¯å¦é‹è¡Œå³æ™‚æª¢æ¸¬
    response = input("\nðŸŽ¥ æ˜¯å¦æ¸¬è©¦å³æ™‚æª¢æ¸¬ï¼Ÿ(y/n): ")
    if response.lower() == 'y':
        test_live_detection()
    
    print("\nðŸŽ‰ QAI Hub çµ±ä¸€æª¢æ¸¬æ¼”ç¤ºå®Œæˆï¼")
