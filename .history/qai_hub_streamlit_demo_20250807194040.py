#!/usr/bin/env python3
"""
ğŸ¯ QAI Hub Live Demo - Streamlit Webæ‡‰ç”¨
å¯¦æ™‚è€äººè¡Œç‚ºæª¢æ¸¬èˆ‡é¢¨éšªè©•ä¼°ç³»çµ±
"""

import streamlit as st
import cv2
import numpy as np
import tempfile
import os
import time
from datetime import datetime
import logging
from PIL import Image
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="QAI Hub è€äººè¡Œç‚ºæª¢æ¸¬ç³»çµ±",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSSæ¨£å¼
st.markdown("""
<style>
.main-header {
    font-size: 3rem;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}
.sub-header {
    font-size: 1.5rem;
    color: #2c3e50;
    margin: 1rem 0;
}
.metric-card {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #1f77b4;
    margin: 0.5rem 0;
}
.alert-high {
    background-color: #fee;
    color: #c53030;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #c53030;
}
.alert-medium {
    background-color: #fffbf0;
    color: #d69e2e;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #d69e2e;
}
.alert-low {
    background-color: #f0fff4;
    color: #38a169;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #38a169;
}
</style>
""", unsafe_allow_html=True)

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@st.cache_resource
def load_detection_systems():
    """è¼‰å…¥æª¢æ¸¬ç³»çµ±ï¼ˆä½¿ç”¨Streamlitå¿«å–ï¼‰"""
    try:
        from official_qai_hub_detector import OfficialQAIHubDetector
        from elderly_behavior_predictor import ElderlyBehaviorPredictor
        
        detector = OfficialQAIHubDetector()
        predictor = ElderlyBehaviorPredictor()
        
        return detector, predictor, True
    except Exception as e:
        st.error(f"ç„¡æ³•è¼‰å…¥æª¢æ¸¬ç³»çµ±: {e}")
        return None, None, False

def process_uploaded_image(uploaded_file, detector, predictor):
    """è™•ç†ä¸Šå‚³çš„åœ–åƒ"""
    try:
        # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶åˆ°è‡¨æ™‚ç›®éŒ„
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            temp_path = tmp_file.name
        
        # è¼‰å…¥åœ–åƒ
        image = cv2.imread(temp_path)
        if image is None:
            st.error("ç„¡æ³•è¼‰å…¥åœ–åƒ")
            return None
        
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # åŸ·è¡Œæª¢æ¸¬
        with st.spinner("ğŸ” åŸ·è¡ŒQAI Hubæª¢æ¸¬..."):
            unified_result = detector.unified_detection(rgb_image)
            
            # è¡Œç‚ºåˆ†æ
            user_id = f"demo_user_{int(time.time())}"
            interaction_result = predictor.process_user_interaction(user_id, image)
        
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        os.unlink(temp_path)
        
        return {
            'image': rgb_image,
            'unified_result': unified_result,
            'interaction_result': interaction_result
        }
        
    except Exception as e:
        st.error(f"è™•ç†åœ–åƒæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def process_webcam_frame(frame, detector, predictor):
    """è™•ç†ç¶²è·¯æ”å½±æ©Ÿå¹€"""
    try:
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # åŸ·è¡Œæª¢æ¸¬
        unified_result = detector.unified_detection(rgb_frame)
        
        # è¡Œç‚ºåˆ†æ
        user_id = f"webcam_user_{int(time.time())}"
        interaction_result = predictor.process_user_interaction(user_id, frame)
        
        return {
            'image': rgb_frame,
            'unified_result': unified_result,
            'interaction_result': interaction_result
        }
        
    except Exception as e:
        st.error(f"è™•ç†ç¶²è·¯æ”å½±æ©Ÿå¹€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def display_detection_results(results):
    """é¡¯ç¤ºæª¢æ¸¬çµæœ"""
    if not results:
        return
    
    unified_result = results['unified_result']
    interaction_result = results['interaction_result']
    
    # æª¢æ¸¬çµ±è¨ˆ
    col1, col2, col3 = st.columns(3)
    
    with col1:
        face_count = unified_result.get('total_detections', {}).get('faces', 0)
        st.metric("ğŸ‘¤ äººè‡‰æª¢æ¸¬", f"{face_count}å€‹")
    
    with col2:
        pose_count = unified_result.get('total_detections', {}).get('poses', 0)
        st.metric("ğŸš¶ å§¿æ…‹æª¢æ¸¬", f"{pose_count}å€‹")
    
    with col3:
        hand_count = unified_result.get('total_detections', {}).get('hands', 0)
        st.metric("âœ‹ æ‰‹éƒ¨æª¢æ¸¬", f"{hand_count}å€‹")
    
    # è¡Œç‚ºåˆ†æçµæœ
    if interaction_result:
        st.markdown("### ğŸ§  è¡Œç‚ºåˆ†æ")
        
        pose_analysis = interaction_result.get('pose_analysis', {})
        risk_assessment = interaction_result.get('risk_assessment', {})
        
        if pose_analysis and 'error' not in pose_analysis:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                balance_score = pose_analysis.get('balance_score', 0)
                st.metric("âš–ï¸ å¹³è¡¡è©•åˆ†", f"{balance_score:.2f}")
            
            with col2:
                stability_score = pose_analysis.get('stability_score', 0)
                st.metric("ğŸ¯ ç©©å®šæ€§è©•åˆ†", f"{stability_score:.2f}")
            
            with col3:
                posture_deviation = pose_analysis.get('posture_deviation', 0)
                st.metric("ğŸ“ å§¿æ…‹åå·®", f"{posture_deviation:.2f}")
        
        # é¢¨éšªè©•ä¼°
        if risk_assessment:
            risk_score = risk_assessment.get('score', 0)
            risk_level = risk_assessment.get('level', 'unknown')
            
            st.markdown("### ğŸš¨ é¢¨éšªè©•ä¼°")
            
            # æ ¹æ“šé¢¨éšªç­‰ç´šé¸æ“‡é¡è‰²å’Œåœ–æ¨™
            if risk_level == 'high':
                risk_color = "ğŸ”´"
                alert_class = "alert-high"
            elif risk_level == 'medium':
                risk_color = "ğŸŸ¡"
                alert_class = "alert-medium"
            else:
                risk_color = "ğŸŸ¢"
                alert_class = "alert-low"
            
            st.markdown(f"""
            <div class="{alert_class}">
                <h4>{risk_color} é¢¨éšªç­‰ç´š: {risk_level.upper()}</h4>
                <p>é¢¨éšªè©•åˆ†: {risk_score:.2f}</p>
            </div>
            """, unsafe_allow_html=True)
            
            # é¢¨éšªè©•åˆ†åœ–è¡¨
            fig = go.Figure(go.Indicator(
                mode = "gauge+number+delta",
                value = risk_score,
                domain = {'x': [0, 1], 'y': [0, 1]},
                title = {'text': "é¢¨éšªè©•åˆ†"},
                delta = {'reference': 0.5},
                gauge = {
                    'axis': {'range': [None, 1]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, 0.3], 'color': "lightgreen"},
                        {'range': [0.3, 0.7], 'color': "yellow"},
                        {'range': [0.7, 1], 'color': "red"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 0.9
                    }
                }
            ))
            
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)

def display_annotated_images(results, detector):
    """é¡¯ç¤ºæ¨™è¨»åœ–åƒ"""
    if not results:
        return
    
    rgb_image = results['image']
    
    st.markdown("### ğŸ–¼ï¸ æª¢æ¸¬çµæœè¦–è¦ºåŒ–")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("#### ğŸ‘¤ äººè‡‰æª¢æ¸¬")
        face_result = detector.detect_faces(rgb_image, raw_output=False)
        if face_result.get('success') and 'annotated_image' in face_result:
            st.image(face_result['annotated_image'], use_container_width=True)
        else:
            st.image(rgb_image, caption="ç„¡æª¢æ¸¬çµæœ", use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸš¶ å§¿æ…‹æª¢æ¸¬")
        pose_result = detector.detect_pose(rgb_image, raw_output=False)
        if pose_result.get('success') and 'annotated_image' in pose_result:
            st.image(pose_result['annotated_image'], use_container_width=True)
        else:
            st.image(rgb_image, caption="ç„¡æª¢æ¸¬çµæœ", use_container_width=True)
    
    with col3:
        st.markdown("#### âœ‹ æ‰‹éƒ¨æª¢æ¸¬")
        hand_result = detector.detect_hands(rgb_image, raw_output=False)
        if hand_result.get('success') and 'annotated_image' in hand_result:
            st.image(hand_result['annotated_image'], use_container_width=True)
        else:
            st.image(rgb_image, caption="ç„¡æª¢æ¸¬çµæœ", use_container_width=True)

def main():
    """ä¸»å‡½æ•¸"""
    # æ¨™é¡Œ
    st.markdown('<h1 class="main-header">ğŸ§  QAI Hub è€äººè¡Œç‚ºæª¢æ¸¬ç³»çµ±</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #7f8c8d;">åŸºæ–¼Qualcomm AI Hubçš„å¯¦æ™‚è€äººè¡Œç‚ºç›£æ¸¬èˆ‡é¢¨éšªè©•ä¼°</p>', unsafe_allow_html=True)
    
    # è¼‰å…¥æª¢æ¸¬ç³»çµ±
    detector, predictor, system_loaded = load_detection_systems()
    
    if not system_loaded:
        st.error("ğŸš« æª¢æ¸¬ç³»çµ±è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç³»çµ±é…ç½®")
        return
    
    # å´é‚Šæ¬„é…ç½®
    st.sidebar.markdown("## âš™ï¸ ç³»çµ±é…ç½®")
    
    demo_mode = st.sidebar.selectbox(
        "é¸æ“‡æ¼”ç¤ºæ¨¡å¼",
        ["ğŸ“· åœ–åƒä¸Šå‚³", "ğŸ“¹ ç¶²è·¯æ”å½±æ©Ÿ", "ğŸ–¼ï¸ ç¤ºä¾‹åœ–åƒ"]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š ç³»çµ±ç‹€æ…‹")
    st.sidebar.success("âœ… QAI Hub æª¢æ¸¬ç³»çµ±å·²è¼‰å…¥")
    st.sidebar.success("âœ… è¡Œç‚ºé æ¸¬ç³»çµ±å·²è¼‰å…¥")
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    if demo_mode == "ğŸ“· åœ–åƒä¸Šå‚³":
        st.markdown("## ğŸ“· åœ–åƒä¸Šå‚³æª¢æ¸¬")
        
        uploaded_file = st.file_uploader(
            "é¸æ“‡åœ–åƒæ–‡ä»¶",
            type=['jpg', 'jpeg', 'png'],
            help="æ”¯æ´JPGã€JPEGã€PNGæ ¼å¼"
        )
        
        if uploaded_file is not None:
            # é¡¯ç¤ºåŸå§‹åœ–åƒ
            image = Image.open(uploaded_file)
            st.image(image, caption="ä¸Šå‚³çš„åœ–åƒ", use_container_width=True)
            
            # è™•ç†åœ–åƒ
            results = process_uploaded_image(uploaded_file, detector, predictor)
            
            if results:
                # é¡¯ç¤ºæª¢æ¸¬çµæœ
                display_detection_results(results)
                
                # é¡¯ç¤ºæ¨™è¨»åœ–åƒ
                display_annotated_images(results, detector)
    
    elif demo_mode == "ğŸ“¹ ç¶²è·¯æ”å½±æ©Ÿ":
        st.markdown("## ğŸ“¹ ç¶²è·¯æ”å½±æ©Ÿå¯¦æ™‚æª¢æ¸¬")
        
        # æ”å½±æ©Ÿæ§åˆ¶
        col1, col2 = st.columns([1, 1])
        
        with col1:
            start_camera = st.button("ğŸ¥ å•Ÿå‹•æ”å½±æ©Ÿ", type="primary")
        
        with col2:
            stop_camera = st.button("â¹ï¸ åœæ­¢æ”å½±æ©Ÿ")
        
        # æ”å½±æ©Ÿå¹€é¡¯ç¤ºå€åŸŸ
        camera_placeholder = st.empty()
        results_placeholder = st.empty()
        
        if start_camera:
            try:
                cap = cv2.VideoCapture(0)
                
                if not cap.isOpened():
                    st.error("ç„¡æ³•æ‰“é–‹æ”å½±æ©Ÿ")
                else:
                    st.success("æ”å½±æ©Ÿå·²å•Ÿå‹•ï¼Œæ­£åœ¨é€²è¡Œå¯¦æ™‚æª¢æ¸¬...")
                    
                    # å¯¦æ™‚è™•ç†å¾ªç’°
                    frame_count = 0
                    while True:
                        ret, frame = cap.read()
                        if not ret:
                            break
                        
                        # æ¯éš”å¹¾å¹€è™•ç†ä¸€æ¬¡ï¼ˆæé«˜æ€§èƒ½ï¼‰
                        if frame_count % 10 == 0:
                            results = process_webcam_frame(frame, detector, predictor)
                            
                            if results:
                                with camera_placeholder.container():
                                    st.image(results['image'], caption="å¯¦æ™‚æª¢æ¸¬", use_container_width=True)
                                
                                with results_placeholder.container():
                                    display_detection_results(results)
                        
                        frame_count += 1
                        
                        # æª¢æŸ¥åœæ­¢æ¢ä»¶
                        if stop_camera:
                            break
                    
                    cap.release()
                    
            except Exception as e:
                st.error(f"æ”å½±æ©ŸéŒ¯èª¤: {e}")
    
    elif demo_mode == "ğŸ–¼ï¸ ç¤ºä¾‹åœ–åƒ":
        st.markdown("## ğŸ–¼ï¸ ç¤ºä¾‹åœ–åƒæª¢æ¸¬")
        
        # åˆ—å‡ºå¯ç”¨çš„ç¤ºä¾‹åœ–åƒ
        sample_images = []
        image_files = ['andy.jpg', 'official_test_image.jpg', 'enhanced_test_image.jpg']
        
        for img_file in image_files:
            if os.path.exists(img_file):
                sample_images.append(img_file)
        
        if sample_images:
            selected_image = st.selectbox("é¸æ“‡ç¤ºä¾‹åœ–åƒ", sample_images)
            
            if selected_image:
                # è¼‰å…¥ä¸¦é¡¯ç¤ºåœ–åƒ
                image = cv2.imread(selected_image)
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                st.image(rgb_image, caption=f"ç¤ºä¾‹åœ–åƒ: {selected_image}", use_container_width=True)
                
                # åŸ·è¡Œæª¢æ¸¬
                with st.spinner("ğŸ” åŸ·è¡Œæª¢æ¸¬..."):
                    unified_result = detector.unified_detection(rgb_image)
                    
                    user_id = f"demo_{selected_image}"
                    interaction_result = predictor.process_user_interaction(user_id, image)
                
                results = {
                    'image': rgb_image,
                    'unified_result': unified_result,
                    'interaction_result': interaction_result
                }
                
                # é¡¯ç¤ºçµæœ
                display_detection_results(results)
                display_annotated_images(results, detector)
        else:
            st.warning("æœªæ‰¾åˆ°ç¤ºä¾‹åœ–åƒ")
    
    # é è…³ä¿¡æ¯
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #7f8c8d; margin-top: 2rem;">
        <p>ğŸ§  QAI Hub è€äººè¡Œç‚ºæª¢æ¸¬ç³»çµ± | åŸºæ–¼ Qualcomm AI Hub MediaPipe æ¨¡å‹</p>
        <p>âš¡ å¯¦æ™‚æª¢æ¸¬ | ğŸ¯ æ™ºèƒ½åˆ†æ | ğŸš¨ é¢¨éšªè©•ä¼°</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
