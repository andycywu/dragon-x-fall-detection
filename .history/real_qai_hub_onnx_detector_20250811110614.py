#!/usr/bin/env python3
"""
ğŸŒ çœŸæ­£çš„QAI Hub + ONNX Runtime æª¢æ¸¬ç³»çµ±
ä½¿ç”¨çœŸå¯¦API Tokené€£æ¥QAI Hubä¸¦éƒ¨ç½²åˆ°ONNX Runtime
"""

import os
import qai_hub as hub
import numpy as np
import cv2
import onnxruntime as ort
import tempfile
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Tuple, Optional
import time
import json

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealQAIHubONNXDetector:
    """çœŸæ­£çš„QAI Hub + ONNX Runtimeæª¢æ¸¬ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–æª¢æ¸¬ç³»çµ±"""
        self.api_token = os.getenv('QAI_HUB_API_TOKEN')
        self.device_group = os.getenv('QAI_HUB_DEVICE_GROUP', 'default')
        self.enable_acceleration = os.getenv('ENABLE_QAI_ACCELERATION', 'true').lower() == 'true'
        
        # ONNX Runtimeé…ç½®
        self.onnx_providers = self._get_onnx_providers()
        self.onnx_sessions = {}
        
        # QAI Hubæ¨¡å‹ç·©å­˜
        self.qai_hub_models = {}
        self.compiled_models = {}
        
        logger.info("ğŸš€ åˆå§‹åŒ–çœŸæ­£çš„QAI Hub + ONNX Runtimeæª¢æ¸¬ç³»çµ±...")
        self._verify_qai_hub_connection()
        self._initialize_models()
    
    def _verify_qai_hub_connection(self):
        """é©—è­‰QAI Hubé€£æ¥"""
        if not self.api_token:
            raise ValueError("âŒ è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®QAI_HUB_API_TOKEN")
        
        try:
            # æ¸¬è©¦é€£æ¥
            devices = hub.get_devices()
            logger.info(f"âœ… QAI Hubé€£æ¥æˆåŠŸï¼Œå¯ç”¨è¨­å‚™æ•¸é‡: {len(devices)}")
            
            # é¸æ“‡ç›®æ¨™è¨­å‚™
            target_devices = [d for d in devices if 'Samsung Galaxy S23' in d.name or 'Snapdragon' in d.name]
            if target_devices:
                self.target_device = target_devices[0]
                logger.info(f"ğŸ¯ é¸æ“‡ç›®æ¨™è¨­å‚™: {self.target_device.name}")
            else:
                # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨è¨­å‚™
                self.target_device = devices[0] if devices else None
                logger.info(f"ğŸ¯ ä½¿ç”¨è¨­å‚™: {self.target_device.name if self.target_device else 'None'}")
                
        except Exception as e:
            logger.error(f"âŒ QAI Hubé€£æ¥å¤±æ•—: {e}")
            raise
    
    def _get_onnx_providers(self):
        """ç²å–ONNX Runtimeæä¾›å•†"""
        providers = []
        
        # æª¢æŸ¥å¯ç”¨çš„åŸ·è¡Œæä¾›å•†
        available_providers = ort.get_available_providers()
        logger.info(f"ğŸ“‹ å¯ç”¨ONNXæä¾›å•†: {available_providers}")
        
        # å„ªå…ˆé †åºï¼šCUDA > DirectML > CPU
        if 'CUDAExecutionProvider' in available_providers:
            providers.append('CUDAExecutionProvider')
            logger.info("âœ… å•Ÿç”¨CUDAåŠ é€Ÿ")
        elif 'DmlExecutionProvider' in available_providers:
            providers.append('DmlExecutionProvider')
            logger.info("âœ… å•Ÿç”¨DirectMLåŠ é€Ÿ")
        
        providers.append('CPUExecutionProvider')
        logger.info("âœ… æ·»åŠ CPUå¾Œå‚™æ”¯æ´")
        
        return providers
    
    def _initialize_models(self):
        """åˆå§‹åŒ–QAI Hubæ¨¡å‹"""
        logger.info("ğŸ“¥ è¼‰å…¥QAI Hubæ¨¡å‹...")
        
        try:
            # MediaPipe Face Detection
            self._load_face_detection_model()
            
            # MediaPipe Pose Estimation  
            self._load_pose_estimation_model()
            
            # MediaPipe Hand Detection
            self._load_hand_detection_model()
            
            logger.info("âœ… æ‰€æœ‰QAI Hubæ¨¡å‹è¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    def _load_face_detection_model(self):
        """è¼‰å…¥äººè‡‰æª¢æ¸¬æ¨¡å‹"""
        try:
            from qai_hub_models.models.mediapipe_face import Model as FaceModel
            
            logger.info("ğŸ“± è¼‰å…¥MediaPipe Face Detection...")
            face_model = FaceModel.from_pretrained()
            self.qai_hub_models['face'] = face_model
            
            # æäº¤ç·¨è­¯Jobåˆ°QAI Hub
            if self.enable_acceleration and self.target_device:
                logger.info("ğŸ”„ æäº¤äººè‡‰æª¢æ¸¬æ¨¡å‹ç·¨è­¯Job...")
                
                compile_job = hub.submit_compile_job(
                    model=face_model,
                    input_specs={"image": ((1, 3, 192, 192), "float32")},
                    device=self.target_device,
                )
                
                logger.info(f"âœ… äººè‡‰æª¢æ¸¬ç·¨è­¯Jobæäº¤: {compile_job.job_id}")
                logger.info(f"ğŸ”— Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                
                # ç­‰å¾…ç·¨è­¯å®Œæˆï¼ˆç•°æ­¥ï¼‰
                self.compiled_models['face_job'] = compile_job
            
            logger.info("âœ… MediaPipe Face Detectionè¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ äººè‡‰æª¢æ¸¬æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    
    def _load_pose_estimation_model(self):
        """è¼‰å…¥å§¿æ…‹ä¼°è¨ˆæ¨¡å‹"""
        try:
            from qai_hub_models.models.mediapipe_pose_estimation import Model as PoseModel
            
            logger.info("ğŸš¶ è¼‰å…¥MediaPipe Pose Estimation...")
            pose_model = PoseModel.from_pretrained()
            self.qai_hub_models['pose'] = pose_model
            
            # æäº¤ç·¨è­¯Jobåˆ°QAI Hub
            if self.enable_acceleration and self.target_device:
                logger.info("ğŸ”„ æäº¤å§¿æ…‹ä¼°è¨ˆæ¨¡å‹ç·¨è­¯Job...")
                
                compile_job = hub.submit_compile_job(
                    model=pose_model,
                    input_specs={"image": ((1, 3, 256, 256), "float32")},
                    device=self.target_device,
                )
                
                logger.info(f"âœ… å§¿æ…‹ä¼°è¨ˆç·¨è­¯Jobæäº¤: {compile_job.job_id}")
                logger.info(f"ğŸ”— Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                
                self.compiled_models['pose_job'] = compile_job
            
            logger.info("âœ… MediaPipe Pose Estimationè¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å§¿æ…‹ä¼°è¨ˆæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    
    def _load_hand_detection_model(self):
        """è¼‰å…¥æ‰‹éƒ¨æª¢æ¸¬æ¨¡å‹"""
        try:
            from qai_hub_models.models.mediapipe_hand import Model as HandModel
            
            logger.info("âœ‹ è¼‰å…¥MediaPipe Hand Detection...")
            hand_model = HandModel.from_pretrained()
            self.qai_hub_models['hand'] = hand_model
            
            # æäº¤ç·¨è­¯Jobåˆ°QAI Hub
            if self.enable_acceleration and self.target_device:
                logger.info("ğŸ”„ æäº¤æ‰‹éƒ¨æª¢æ¸¬æ¨¡å‹ç·¨è­¯Job...")
                
                compile_job = hub.submit_compile_job(
                    model=hand_model,
                    input_specs={"image": ((1, 3, 224, 224), "float32")},
                    device=self.target_device,
                )
                
                logger.info(f"âœ… æ‰‹éƒ¨æª¢æ¸¬ç·¨è­¯Jobæäº¤: {compile_job.job_id}")
                logger.info(f"ğŸ”— Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                
                self.compiled_models['hand_job'] = compile_job
            
            logger.info("âœ… MediaPipe Hand Detectionè¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ‰‹éƒ¨æª¢æ¸¬æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    
    def export_to_onnx(self, model_name: str) -> Optional[str]:
        """å°‡QAI Hubæ¨¡å‹å°å‡ºç‚ºONNXæ ¼å¼"""
        if model_name not in self.qai_hub_models:
            logger.error(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return None
        
        try:
            logger.info(f"ğŸ“¤ å°å‡º {model_name} æ¨¡å‹ç‚ºONNX...")
            
            model = self.qai_hub_models[model_name]
            
            # å‰µå»ºè‡¨æ™‚æ–‡ä»¶ä¿å­˜ONNXæ¨¡å‹
            onnx_path = f"qai_hub_{model_name}_model.onnx"
            
            # æ ¹æ“šæ¨¡å‹é¡å‹è¨­ç½®è¼¸å…¥è¦æ ¼
            if model_name == 'face':
                sample_input = {"image": np.random.randn(1, 3, 192, 192).astype(np.float32)}
            elif model_name == 'pose':
                sample_input = {"image": np.random.randn(1, 3, 256, 256).astype(np.float32)}
            elif model_name == 'hand':
                sample_input = {"image": np.random.randn(1, 3, 224, 224).astype(np.float32)}
            else:
                sample_input = {"image": np.random.randn(1, 3, 224, 224).astype(np.float32)}
            
            # ä½¿ç”¨QAI Hubå°å‡ºONNX
            onnx_model = hub.get_onnx_model(model, sample_input)
            
            # ä¿å­˜ONNXæ–‡ä»¶
            with open(onnx_path, 'wb') as f:
                f.write(onnx_model.model)
            
            logger.info(f"âœ… ONNXæ¨¡å‹å·²ä¿å­˜: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            logger.error(f"âŒ ONNXå°å‡ºå¤±æ•—: {e}")
            return None
    
    def load_onnx_session(self, model_name: str, onnx_path: str):
        """è¼‰å…¥ONNX Runtimeæœƒè©±"""
        try:
            logger.info(f"ğŸ”„ è¼‰å…¥ONNX Runtimeæœƒè©±: {model_name}")
            
            # é…ç½®ONNX Runtimeæœƒè©±é¸é …
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            sess_options.enable_cpu_mem_arena = True
            sess_options.enable_mem_pattern = True
            
            # å‰µå»ºONNX Runtimeæœƒè©±
            session = ort.InferenceSession(
                onnx_path,
                sess_options=sess_options,
                providers=self.onnx_providers
            )
            
            self.onnx_sessions[model_name] = session
            
            # è¨˜éŒ„æ¨¡å‹ä¿¡æ¯
            input_info = [(inp.name, inp.shape, inp.type) for inp in session.get_inputs()]
            output_info = [(out.name, out.shape, out.type) for out in session.get_outputs()]
            
            logger.info(f"âœ… ONNXæœƒè©±è¼‰å…¥æˆåŠŸ: {model_name}")
            logger.info(f"   è¼¸å…¥: {input_info}")
            logger.info(f"   è¼¸å‡º: {output_info}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNXæœƒè©±è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def detect_with_onnx(self, image: np.ndarray, model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨ONNX Runtimeé€²è¡Œæª¢æ¸¬"""
        if model_name not in self.onnx_sessions:
            return {"error": f"ONNXæœƒè©± {model_name} æœªè¼‰å…¥"}
        
        try:
            session = self.onnx_sessions[model_name]
            
            # é è™•ç†åœ–åƒ
            processed_image = self._preprocess_image(image, model_name)
            
            # æº–å‚™è¼¸å…¥
            input_name = session.get_inputs()[0].name
            input_data = {input_name: processed_image}
            
            # åŸ·è¡Œæ¨ç†
            start_time = time.time()
            outputs = session.run(None, input_data)
            inference_time = (time.time() - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
            
            # å¾Œè™•ç†çµæœ
            results = self._postprocess_results(outputs, model_name)
            results['inference_time_ms'] = inference_time
            results['model_name'] = f"QAI_Hub_{model_name}_ONNX"
            
            logger.info(f"âš¡ {model_name} ONNXæ¨ç†å®Œæˆ: {inference_time:.2f}ms")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨ç†å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _preprocess_image(self, image: np.ndarray, model_name: str) -> np.ndarray:
        """é è™•ç†åœ–åƒ"""
        # æ ¹æ“šæ¨¡å‹é¡å‹èª¿æ•´è¼¸å…¥å°ºå¯¸
        if model_name == 'face':
            target_size = (192, 192)
        elif model_name == 'pose':
            target_size = (256, 256)
        elif model_name == 'hand':
            target_size = (224, 224)
        else:
            target_size = (224, 224)
        
        # èª¿æ•´åœ–åƒå¤§å°
        resized = cv2.resize(image, target_size)
        
        # è½‰æ›ç‚ºRGBï¼ˆå¦‚æœéœ€è¦ï¼‰
        if len(resized.shape) == 3 and resized.shape[2] == 3:
            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # æ­£è¦åŒ–åˆ°[0,1]
        normalized = resized.astype(np.float32) / 255.0
        
        # è½‰æ›ç‚ºCHWæ ¼å¼ä¸¦æ·»åŠ batchç¶­åº¦
        preprocessed = np.transpose(normalized, (2, 0, 1))
        preprocessed = np.expand_dims(preprocessed, axis=0)
        
        return preprocessed
    
    def _postprocess_results(self, outputs: list, model_name: str) -> Dict[str, Any]:
        """å¾Œè™•ç†æª¢æ¸¬çµæœ"""
        results = {
            "success": True,
            "detections": [],
            "raw_outputs": [out.tolist() for out in outputs]
        }
        
        try:
            if model_name == 'face':
                # è™•ç†äººè‡‰æª¢æ¸¬çµæœ
                if len(outputs) >= 2:
                    boxes = outputs[0]  # é‚Šç•Œæ¡†
                    scores = outputs[1]  # ç½®ä¿¡åº¦åˆ†æ•¸
                    
                    # éæ¿¾é«˜ç½®ä¿¡åº¦æª¢æ¸¬
                    threshold = 0.5
                    valid_detections = []
                    
                    for i in range(len(scores[0])):
                        if scores[0][i] > threshold:
                            valid_detections.append({
                                "box": boxes[0][i].tolist(),
                                "confidence": float(scores[0][i]),
                                "type": "face"
                            })
                    
                    results["detections"] = valid_detections
                    results["total_faces"] = len(valid_detections)
            
            elif model_name == 'pose':
                # è™•ç†å§¿æ…‹æª¢æ¸¬çµæœ
                if len(outputs) >= 1:
                    keypoints = outputs[0]  # é—œéµé»
                    
                    # è§£æé—œéµé»
                    if keypoints.shape[-1] >= 51:  # 17å€‹é—œéµé» * 3 (x,y,confidence)
                        pose_keypoints = []
                        for i in range(0, 51, 3):
                            pose_keypoints.append({
                                "x": float(keypoints[0][i]),
                                "y": float(keypoints[0][i+1]),
                                "confidence": float(keypoints[0][i+2])
                            })
                        
                        results["detections"] = [{
                            "keypoints": pose_keypoints,
                            "type": "pose"
                        }]
                        results["total_poses"] = 1
            
            elif model_name == 'hand':
                # è™•ç†æ‰‹éƒ¨æª¢æ¸¬çµæœ
                if len(outputs) >= 1:
                    hand_landmarks = outputs[0]
                    
                    # è§£ææ‰‹éƒ¨é—œéµé»
                    results["detections"] = [{
                        "landmarks": hand_landmarks[0].tolist(),
                        "type": "hand"
                    }]
                    results["total_hands"] = 1
        
        except Exception as e:
            logger.error(f"âŒ çµæœå¾Œè™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def submit_profiling_job(self, model_name: str) -> Optional[str]:
        """æäº¤profiling jobåˆ°QAI Hub"""
        if model_name not in self.compiled_models:
            logger.error(f"âŒ ç·¨è­¯Job {model_name} ä¸å­˜åœ¨")
            return None
        
        try:
            compile_job = self.compiled_models[f'{model_name}_job']
            
            logger.info(f"â³ ç­‰å¾… {model_name} ç·¨è­¯å®Œæˆ...")
            
            # ç­‰å¾…ç·¨è­¯å®Œæˆï¼ˆè¨­ç½®è¶…æ™‚ï¼‰
            try:
                compile_job.wait(timeout=300)  # 5åˆ†é˜è¶…æ™‚
            except Exception as e:
                logger.warning(f"âš ï¸ ç·¨è­¯è¶…æ™‚ï¼Œä½†Jobä»åœ¨é€²è¡Œ: {e}")
                return compile_job.job_id
            
            if compile_job.success:
                logger.info(f"âœ… {model_name} ç·¨è­¯æˆåŠŸï¼Œæäº¤profiling...")
                
                # æº–å‚™æ¸¬è©¦æ•¸æ“š
                if model_name == 'face':
                    test_input = {"image": np.random.randn(1, 3, 192, 192).astype(np.float32)}
                elif model_name == 'pose':
                    test_input = {"image": np.random.randn(1, 3, 256, 256).astype(np.float32)}
                elif model_name == 'hand':
                    test_input = {"image": np.random.randn(1, 3, 224, 224).astype(np.float32)}
                
                # æäº¤profiling job
                profile_job = hub.submit_profile_job(
                    model=compile_job.get_target_model(),
                    input_data=test_input,
                    device=self.target_device,
                )
                
                logger.info(f"âœ… {model_name} Profiling Jobæäº¤: {profile_job.job_id}")
                logger.info(f"ğŸ”— Dashboard: https://aihub.qualcomm.com/jobs/{profile_job.job_id}")
                
                return profile_job.job_id
            else:
                logger.error(f"âŒ {model_name} ç·¨è­¯å¤±æ•—: {compile_job.failure_reason}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Profiling jobæäº¤å¤±æ•—: {e}")
            return None
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """ç²å–Jobç‹€æ…‹"""
        try:
            job = hub.get_job(job_id)
            
            status_info = {
                "job_id": job_id,
                "status": job.status,
                "success": job.success if hasattr(job, 'success') else None,
                "failure_reason": getattr(job, 'failure_reason', None),
                "submitted_at": getattr(job, 'submitted_at', None),
                "dashboard_url": f"https://aihub.qualcomm.com/jobs/{job_id}"
            }
            
            return status_info
            
        except Exception as e:
            logger.error(f"âŒ ç²å–Jobç‹€æ…‹å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def unified_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """çµ±ä¸€æª¢æ¸¬æ¥å£ï¼ˆONNX + QAI Hubï¼‰"""
        results = {
            "timestamp": time.time(),
            "image_shape": image.shape,
            "detections": {},
            "performance": {},
            "qai_hub_jobs": {}
        }
        
        # å¦‚æœONNXæœƒè©±å¯ç”¨ï¼Œä½¿ç”¨ONNX Runtime
        for model_name in ['face', 'pose', 'hand']:
            if model_name in self.onnx_sessions:
                logger.info(f"ğŸ”„ ä½¿ç”¨ONNX Runtimeæª¢æ¸¬: {model_name}")
                detection_result = self.detect_with_onnx(image, model_name)
                results["detections"][model_name] = detection_result
                
                if "inference_time_ms" in detection_result:
                    results["performance"][f"{model_name}_inference_ms"] = detection_result["inference_time_ms"]
        
        # è¨˜éŒ„QAI Hub Jobä¿¡æ¯
        for model_name in ['face', 'pose', 'hand']:
            job_key = f'{model_name}_job'
            if job_key in self.compiled_models:
                job = self.compiled_models[job_key]
                results["qai_hub_jobs"][model_name] = {
                    "compile_job_id": job.job_id,
                    "dashboard_url": f"https://aihub.qualcomm.com/jobs/{job.job_id}"
                }
        
        # çµ±è¨ˆç¸½æª¢æ¸¬æ•¸é‡
        total_detections = {}
        for model_name, detection in results["detections"].items():
            if "total_faces" in detection:
                total_detections["faces"] = detection["total_faces"]
            elif "total_poses" in detection:
                total_detections["poses"] = detection["total_poses"]
            elif "total_hands" in detection:
                total_detections["hands"] = detection["total_hands"]
        
        results["total_detections"] = total_detections
        
        return results

def main():
    """ä¸»å‡½æ•¸ï¼šæ¸¬è©¦çœŸå¯¦QAI Hub + ONNXç³»çµ±"""
    print("ğŸŒ çœŸå¯¦QAI Hub + ONNX Runtimeæª¢æ¸¬ç³»çµ±æ¸¬è©¦")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æª¢æ¸¬å™¨
        detector = RealQAIHubONNXDetector()
        
        # å°å‡ºONNXæ¨¡å‹
        print("\nğŸ“¤ å°å‡ºONNXæ¨¡å‹...")
        for model_name in ['face', 'pose', 'hand']:
            if model_name in detector.qai_hub_models:
                onnx_path = detector.export_to_onnx(model_name)
                if onnx_path:
                    detector.load_onnx_session(model_name, onnx_path)
        
        # æäº¤profiling jobs
        print("\nğŸš€ æäº¤Profiling Jobs...")
        profiling_jobs = {}
        for model_name in ['face', 'pose', 'hand']:
            job_id = detector.submit_profiling_job(model_name)
            if job_id:
                profiling_jobs[model_name] = job_id
        
        # æ¸¬è©¦æª¢æ¸¬
        print("\nğŸ§ª æ¸¬è©¦ONNX Runtimeæª¢æ¸¬...")
        
        # è¼‰å…¥æ¸¬è©¦åœ–åƒ
        test_images = ['andy.jpg', 'official_test_image.jpg']
        for img_path in test_images:
            if os.path.exists(img_path):
                print(f"\nğŸ“· æ¸¬è©¦åœ–åƒ: {img_path}")
                
                image = cv2.imread(img_path)
                if image is not None:
                    # åŸ·è¡Œçµ±ä¸€æª¢æ¸¬
                    results = detector.unified_detection(image)
                    
                    print(f"âœ… æª¢æ¸¬å®Œæˆ:")
                    print(f"   ç¸½æª¢æ¸¬æ•¸é‡: {results.get('total_detections', {})}")
                    print(f"   æ€§èƒ½æŒ‡æ¨™: {results.get('performance', {})}")
                    
                    # é¡¯ç¤ºQAI Hub Jobä¿¡æ¯
                    qai_jobs = results.get('qai_hub_jobs', {})
                    if qai_jobs:
                        print(f"   QAI Hub Jobs:")
                        for model, job_info in qai_jobs.items():
                            print(f"     {model}: {job_info['compile_job_id']}")
                            print(f"       Dashboard: {job_info['dashboard_url']}")
        
        # æª¢æŸ¥Jobç‹€æ…‹
        print("\nğŸ“Š æª¢æŸ¥QAI Hub Jobç‹€æ…‹...")
        for model_name, job_id in profiling_jobs.items():
            status = detector.get_job_status(job_id)
            print(f"   {model_name}: {status.get('status', 'unknown')}")
        
        # ä¿å­˜çµæœ
        results_summary = {
            "system": "Real QAI Hub + ONNX Runtime",
            "timestamp": time.time(),
            "qai_hub_jobs": profiling_jobs,
            "onnx_models": list(detector.onnx_sessions.keys()),
            "target_device": detector.target_device.name if detector.target_device else None,
            "onnx_providers": detector.onnx_providers
        }
        
        with open('real_qai_hub_onnx_results.json', 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        print(f"\nâœ… çµæœå·²ä¿å­˜: real_qai_hub_onnx_results.json")
        print(f"ğŸ¯ çœŸå¯¦QAI Hub + ONNXç³»çµ±æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç³»çµ±æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
