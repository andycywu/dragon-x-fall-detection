#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ† é»‘å®¢æ¾ - QAI Hub MediaPipe è·Œå€’æª¢æ¸¬ç³»çµ±
ä½¿ç”¨ Qualcomm AI Hub çš„ MediaPipe Pose æ¨¡å‹å¯¦ç¾è·Œå€’æª¢æ¸¬
å®Œå…¨è§£æ±º protobuf ç‰ˆæœ¬è¡çªå•é¡Œ
"""

import cv2
import numpy as np
import time
import math
from typing import Tuple, List, Optional, Dict, Any
from dataclasses import dataclass
import logging

# QAI Hub MediaPipe Pose å°å…¥
from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
from qai_hub_models.models.mediapipe_pose.model import MediaPipePose, POSE_LANDMARK_CONNECTIONS

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PoseKeypoint:
    """å§¿æ…‹é—œéµé»"""
    x: float
    y: float
    z: float
    visibility: float

@dataclass
class FallDetectionResult:
    """è·Œå€’æª¢æ¸¬çµæœ"""
    is_fall: bool
    confidence: float
    body_angle: float
    risk_level: str
    reason: str
    timestamp: float

class QAIHubMediaPipeFallDetector:
    """åŸºæ–¼ QAI Hub MediaPipe çš„è·Œå€’æª¢æ¸¬å™¨"""
    
    def __init__(self, 
                 fall_threshold: float = 30.0,
                 position_change_threshold: float = 0.3,
                 confidence_threshold: float = 0.5):
        """
        åˆå§‹åŒ–è·Œå€’æª¢æ¸¬å™¨
        
        Args:
            fall_threshold: è·Œå€’è§’åº¦é–¾å€¼ï¼ˆåº¦ï¼‰
            position_change_threshold: ä½ç½®è®ŠåŒ–é–¾å€¼
            confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼
        """
        self.fall_threshold = fall_threshold
        self.position_change_threshold = position_change_threshold
        self.confidence_threshold = confidence_threshold
        
        # è¼‰å…¥ QAI Hub MediaPipe æ¨¡å‹
        logger.info("ğŸš€ è¼‰å…¥ QAI Hub MediaPipe Pose æ¨¡å‹...")
        self.pose_model = MediaPipePose.from_pretrained()
        self.pose_app = MediaPipePoseApp.from_pretrained(self.pose_model)
        logger.info("âœ… QAI Hub MediaPipe Pose æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # æ­·å²æ•¸æ“šç”¨æ–¼æ™‚åºåˆ†æ
        self.pose_history: List[List[PoseKeypoint]] = []
        self.max_history = 10
        
        # é—œéµé»ç´¢å¼• (MediaPipe 33é»æ¨¡å‹)
        self.NOSE = 0
        self.LEFT_SHOULDER = 11
        self.RIGHT_SHOULDER = 12
        self.LEFT_HIP = 23
        self.RIGHT_HIP = 24
        self.LEFT_KNEE = 25
        self.RIGHT_KNEE = 26
        self.LEFT_ANKLE = 27
        self.RIGHT_ANKLE = 28
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'total_frames': 0,
            'fall_detections': 0,
            'avg_processing_time': 0.0,
            'last_detection_time': None
        }
    
    def detect_pose(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """
        ä½¿ç”¨ QAI Hub MediaPipe æª¢æ¸¬å§¿æ…‹
        
        Args:
            image: è¼¸å…¥åœ–åƒ (BGR format)
            
        Returns:
            å§¿æ…‹é—œéµé»åˆ—è¡¨ï¼Œå¦‚æœæª¢æ¸¬å¤±æ•—å‰‡è¿”å› None
        """
        try:
            start_time = time.time()
            
            # è½‰æ›åœ–åƒæ ¼å¼ (BGR -> RGB)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨ QAI Hub MediaPipe é€²è¡Œå§¿æ…‹æª¢æ¸¬
            # æ³¨æ„ï¼šé€™è£¡éœ€è¦æ ¹æ“šå¯¦éš› API èª¿æ•´
            result = self.pose_app.predict_landmarks_from_image(rgb_image)
            
            processing_time = time.time() - start_time
            self.stats['avg_processing_time'] = (
                self.stats['avg_processing_time'] * self.stats['total_frames'] + processing_time
            ) / (self.stats['total_frames'] + 1)
            
            # è§£æçµæœä¸¦è½‰æ›ç‚ºæˆ‘å€‘çš„æ ¼å¼
            if result and len(result) > 0:
                # é€™è£¡éœ€è¦æ ¹æ“š QAI Hub MediaPipe çš„å¯¦éš›è¼¸å‡ºæ ¼å¼ä¾†è§£æ
                # æš«æ™‚ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šçµæ§‹
                landmarks = self._parse_qai_hub_results(result)
                return landmarks
            
            return None
            
        except Exception as e:
            logger.error(f"å§¿æ…‹æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _parse_qai_hub_results(self, results) -> List[PoseKeypoint]:
        """
        è§£æ QAI Hub MediaPipe çš„æª¢æ¸¬çµæœ
        
        Args:
            results: QAI Hub çš„åŸå§‹æª¢æ¸¬çµæœ
            
        Returns:
            æ¨™æº–åŒ–çš„é—œéµé»åˆ—è¡¨
        """
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ QAI Hub MediaPipe è¼¸å‡ºæ ¼å¼ä¾†å¯¦ç¾
        # ç›®å‰ä½¿ç”¨ç¤ºä¾‹æ•¸æ“šçµæ§‹
        landmarks = []
        
        # å‡è¨­ results åŒ…å«äº† landmarks æ•¸æ“š
        # å¯¦éš›å¯¦ç¾æ™‚éœ€è¦æ ¹æ“šçœŸå¯¦çš„æ•¸æ“šæ ¼å¼èª¿æ•´
        try:
            # æ¨¡æ“¬ 33 å€‹é—œéµé» (MediaPipe Pose æ¨™æº–)
            for i in range(33):
                landmarks.append(PoseKeypoint(
                    x=0.5,  # éœ€è¦å¾å¯¦éš›çµæœä¸­æå–
                    y=0.5,  # éœ€è¦å¾å¯¦éš›çµæœä¸­æå–
                    z=0.0,  # éœ€è¦å¾å¯¦éš›çµæœä¸­æå–
                    visibility=0.8  # éœ€è¦å¾å¯¦éš›çµæœä¸­æå–
                ))
        except Exception as e:
            logger.error(f"è§£æ QAI Hub çµæœéŒ¯èª¤: {e}")
            return []
        
        return landmarks
    
    def calculate_body_angle(self, landmarks: List[PoseKeypoint]) -> float:
        """
        è¨ˆç®—èº«é«”å‚¾æ–œè§’åº¦
        
        Args:
            landmarks: å§¿æ…‹é—œéµé»
            
        Returns:
            èº«é«”å‚¾æ–œè§’åº¦ï¼ˆåº¦ï¼‰
        """
        try:
            # ç²å–è‚©è†€å’Œé«–éƒ¨çš„ä¸­é»
            left_shoulder = landmarks[self.LEFT_SHOULDER]
            right_shoulder = landmarks[self.RIGHT_SHOULDER]
            left_hip = landmarks[self.LEFT_HIP]
            right_hip = landmarks[self.RIGHT_HIP]
            
            # è¨ˆç®—èº«é«”ä¸­ç·šå‘é‡
            shoulder_center = ((left_shoulder.x + right_shoulder.x) / 2,
                             (left_shoulder.y + right_shoulder.y) / 2)
            hip_center = ((left_hip.x + right_hip.x) / 2,
                         (left_hip.y + right_hip.y) / 2)
            
            # è¨ˆç®—èº«é«”å‘é‡èˆ‡å‚ç›´è»¸çš„å¤¾è§’
            body_vector = (hip_center[0] - shoulder_center[0],
                          hip_center[1] - shoulder_center[1])
            
            # è¨ˆç®—è§’åº¦ï¼ˆèˆ‡å‚ç›´è»¸çš„å¤¾è§’ï¼‰
            angle = math.degrees(math.atan2(abs(body_vector[0]), abs(body_vector[1])))
            
            return angle
            
        except Exception as e:
            logger.error(f"è¨ˆç®—èº«é«”è§’åº¦éŒ¯èª¤: {e}")
            return 0.0
    
    def analyze_fall_risk(self, landmarks: List[PoseKeypoint]) -> FallDetectionResult:
        """
        åˆ†æè·Œå€’é¢¨éšª
        
        Args:
            landmarks: å§¿æ…‹é—œéµé»
            
        Returns:
            è·Œå€’æª¢æ¸¬çµæœ
        """
        timestamp = time.time()
        
        # è¨ˆç®—èº«é«”è§’åº¦
        body_angle = self.calculate_body_angle(landmarks)
        
        # åˆ¤æ–·è·Œå€’é¢¨éšª
        is_fall = body_angle > self.fall_threshold
        confidence = min(body_angle / self.fall_threshold, 1.0)
        
        # ç¢ºå®šé¢¨éšªç­‰ç´š
        if body_angle < 15:
            risk_level = "ä½"
            reason = "å§¿æ…‹æ­£å¸¸"
        elif body_angle < 25:
            risk_level = "ä¸­"
            reason = "è¼•å¾®å‚¾æ–œ"
        elif body_angle < self.fall_threshold:
            risk_level = "é«˜"
            reason = "èº«é«”å‚¾æ–œæ˜é¡¯"
        else:
            risk_level = "å±éšª"
            reason = "æª¢æ¸¬åˆ°è·Œå€’"
            self.stats['fall_detections'] += 1
            self.stats['last_detection_time'] = timestamp
        
        return FallDetectionResult(
            is_fall=is_fall,
            confidence=confidence,
            body_angle=body_angle,
            risk_level=risk_level,
            reason=reason,
            timestamp=timestamp
        )
    
    def draw_pose_landmarks(self, image: np.ndarray, landmarks: List[PoseKeypoint]) -> np.ndarray:
        """
        åœ¨åœ–åƒä¸Šç¹ªè£½å§¿æ…‹é—œéµé»
        
        Args:
            image: è¼¸å…¥åœ–åƒ
            landmarks: å§¿æ…‹é—œéµé»
            
        Returns:
            ç¹ªè£½äº†é—œéµé»çš„åœ–åƒ
        """
        output_image = image.copy()
        height, width = image.shape[:2]
        
        # ç¹ªè£½é—œéµé»
        for i, landmark in enumerate(landmarks):
            if landmark.visibility > self.confidence_threshold:
                x = int(landmark.x * width)
                y = int(landmark.y * height)
                cv2.circle(output_image, (x, y), 3, (0, 255, 0), -1)
                cv2.putText(output_image, str(i), (x, y-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ç¹ªè£½éª¨æ¶é€£æ¥
        for connection in POSE_LANDMARK_CONNECTIONS:
            start_idx, end_idx = connection
            if (start_idx < len(landmarks) and end_idx < len(landmarks) and
                landmarks[start_idx].visibility > self.confidence_threshold and
                landmarks[end_idx].visibility > self.confidence_threshold):
                
                start_x = int(landmarks[start_idx].x * width)
                start_y = int(landmarks[start_idx].y * height)
                end_x = int(landmarks[end_idx].x * width)
                end_y = int(landmarks[end_idx].y * height)
                
                cv2.line(output_image, (start_x, start_y), (end_x, end_y), (255, 0, 0), 2)
        
        return output_image
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, FallDetectionResult]:
        """
        è™•ç†å–®å€‹å½±åƒå¹€
        
        Args:
            frame: è¼¸å…¥å½±åƒå¹€
            
        Returns:
            è™•ç†å¾Œçš„å½±åƒå’Œæª¢æ¸¬çµæœ
        """
        self.stats['total_frames'] += 1
        
        # æª¢æ¸¬å§¿æ…‹
        landmarks = self.detect_pose(frame)
        
        if landmarks is None:
            # å¦‚æœæª¢æ¸¬å¤±æ•—ï¼Œè¿”å›åŸå§‹å½±åƒå’Œå®‰å…¨çµæœ
            result = FallDetectionResult(
                is_fall=False,
                confidence=0.0,
                body_angle=0.0,
                risk_level="æœªçŸ¥",
                reason="å§¿æ…‹æª¢æ¸¬å¤±æ•—",
                timestamp=time.time()
            )
            return frame, result
        
        # åˆ†æè·Œå€’é¢¨éšª
        fall_result = self.analyze_fall_risk(landmarks)
        
        # ç¹ªè£½å§¿æ…‹
        output_frame = self.draw_pose_landmarks(frame, landmarks)
        
        # æ·»åŠ æª¢æ¸¬ä¿¡æ¯åˆ°å½±åƒ
        self._draw_detection_info(output_frame, fall_result)
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        self.pose_history.append(landmarks)
        if len(self.pose_history) > self.max_history:
            self.pose_history.pop(0)
        
        return output_frame, fall_result
    
    def _draw_detection_info(self, image: np.ndarray, result: FallDetectionResult):
        """åœ¨å½±åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = image.shape[:2]
        
        # æ ¹æ“šé¢¨éšªç­‰ç´šé¸æ“‡é¡è‰²
        color_map = {
            "ä½": (0, 255, 0),      # ç¶ è‰²
            "ä¸­": (0, 255, 255),    # é»ƒè‰²
            "é«˜": (0, 165, 255),    # æ©™è‰²
            "å±éšª": (0, 0, 255),    # ç´…è‰²
            "æœªçŸ¥": (128, 128, 128) # ç°è‰²
        }
        color = color_map.get(result.risk_level, (255, 255, 255))
        
        # ç¹ªè£½æª¢æ¸¬ä¿¡æ¯
        info_texts = [
            f"é¢¨éšªç­‰ç´š: {result.risk_level}",
            f"èº«é«”è§’åº¦: {result.body_angle:.1f}Â°",
            f"ç½®ä¿¡åº¦: {result.confidence:.2f}",
            f"ç‹€æ…‹: {result.reason}"
        ]
        
        for i, text in enumerate(info_texts):
            cv2.putText(image, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ·»åŠ è­¦å‘Š
        if result.is_fall:
            cv2.putText(image, "!! è·Œå€’è­¦å ± !!", (width//2 - 100, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return self.stats.copy()

def demo_webcam():
    """æ”åƒé ­å³æ™‚æ¼”ç¤º"""
    print("ğŸ¥ å•Ÿå‹•æ”åƒé ­å³æ™‚è·Œå€’æª¢æ¸¬æ¼”ç¤º...")
    print("æŒ‰ 'q' éµé€€å‡º")
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = QAIHubMediaPipeFallDetector()
    
    # æ‰“é–‹æ”åƒé ­
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ç„¡æ³•æ‰“é–‹æ”åƒé ­")
        return
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ ç„¡æ³•è®€å–æ”åƒé ­æ•¸æ“š")
                break
            
            # è™•ç†å¹€
            output_frame, result = detector.process_frame(frame)
            
            # é¡¯ç¤ºçµæœ
            cv2.imshow('QAI Hub MediaPipe è·Œå€’æª¢æ¸¬', output_frame)
            
            # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ‰“å°è­¦å‘Š
            if result.is_fall:
                print(f"ğŸš¨ è·Œå€’è­¦å ±ï¼è§’åº¦: {result.body_angle:.1f}Â°, "
                      f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            
            # æŒ‰ 'q' é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        stats = detector.get_stats()
        print(f"\nğŸ“Š æª¢æ¸¬çµ±è¨ˆ:")
        print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
        print(f"   è·Œå€’æª¢æ¸¬æ¬¡æ•¸: {stats['fall_detections']}")
        print(f"   å¹³å‡è™•ç†æ™‚é–“: {stats['avg_processing_time']:.3f}ç§’")

def demo_image(image_path: str):
    """å–®å¼µåœ–åƒæ¼”ç¤º"""
    print(f"ğŸ–¼ï¸  è™•ç†åœ–åƒ: {image_path}")
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = QAIHubMediaPipeFallDetector()
    
    # è¼‰å…¥åœ–åƒ
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ ç„¡æ³•è¼‰å…¥åœ–åƒ: {image_path}")
        return
    
    # è™•ç†åœ–åƒ
    output_image, result = detector.process_frame(image)
    
    # é¡¯ç¤ºçµæœ
    print(f"ğŸ“Š æª¢æ¸¬çµæœ:")
    print(f"   è·Œå€’æª¢æ¸¬: {'æ˜¯' if result.is_fall else 'å¦'}")
    print(f"   é¢¨éšªç­‰ç´š: {result.risk_level}")
    print(f"   èº«é«”è§’åº¦: {result.body_angle:.1f}Â°")
    print(f"   ç½®ä¿¡åº¦: {result.confidence:.2f}")
    print(f"   åŸå› : {result.reason}")
    
    # ä¿å­˜çµæœ
    output_path = "fall_detection_result.jpg"
    cv2.imwrite(output_path, output_image)
    print(f"âœ… çµæœå·²ä¿å­˜è‡³: {output_path}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ† QAI Hub MediaPipe è·Œå€’æª¢æ¸¬ç³»çµ±")
    print("=" * 50)
    print("é¸æ“‡æ¼”ç¤ºæ¨¡å¼:")
    print("1. æ”åƒé ­å³æ™‚æª¢æ¸¬")
    print("2. åœ–åƒæª”æ¡ˆæª¢æ¸¬")
    print("3. æ¸¬è©¦æ¨¡å‹è¼‰å…¥")
    
    try:
        choice = input("è«‹è¼¸å…¥é¸æ“‡ (1-3): ").strip()
        
        if choice == "1":
            demo_webcam()
        elif choice == "2":
            image_path = input("è«‹è¼¸å…¥åœ–åƒè·¯å¾‘: ").strip()
            demo_image(image_path)
        elif choice == "3":
            print("ğŸ§ª æ¸¬è©¦æ¨¡å‹è¼‰å…¥...")
            detector = QAIHubMediaPipeFallDetector()
            print("âœ… QAI Hub MediaPipe æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
            print("ğŸ¯ ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é€²è¡Œè·Œå€’æª¢æ¸¬")
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
