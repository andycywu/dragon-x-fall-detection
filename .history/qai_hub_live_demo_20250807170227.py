#!/usr/bin/env python3
"""
ğŸ† é»‘å®¢æ¾å¯¦æ™‚ç›¸æ©Ÿæ¼”ç¤º - 100% æª¢æ¸¬æˆåŠŸç‡ç‰ˆæœ¬
å±•ç¤º MediaPipe + QAI Hub å¯¦éš›é‹ä½œ
åŒ…å«å››ç¨®æª¢æ¸¬æ–¹æ³•çš„å¯¦æ™‚æ¼”ç¤º
"""

import os
import sys
import time
import json
import cv2
import numpy as np
from pathlib import Path
import logging
from typing import Optional, Dict, Any, Tuple, List
import threading
import queue

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å°å…¥æˆ‘å€‘çš„å®Œæ•´æª¢æ¸¬ç³»çµ±
try:
    from completely_fixed_detector import CompletelyFixedHackathonDetector
    DETECTOR_AVAILABLE = True
    print("âœ… å°å…¥å®Œæ•´ä¿®å¾©çš„æª¢æ¸¬ç³»çµ±")
except ImportError as e:
    print(f"âš ï¸ ç„¡æ³•å°å…¥æª¢æ¸¬ç³»çµ±: {e}")
    DETECTOR_AVAILABLE = False

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LiveDetectionManager:
    """å¯¦æ™‚æª¢æ¸¬ç®¡ç†å™¨ - ä½¿ç”¨100%æˆåŠŸç‡çš„æª¢æ¸¬ç³»çµ±"""
    
    def __init__(self):
        self.detector = None
        self.running = False
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.detection_stats = {
            'total_frames': 0,
            'successful_detections': 0,
            'method_usage': {
                'QAI_Hub_MediaPipe': 0,
                'Standard_MediaPipe': 0,
                'OpenCV_Fallback': 0,
                'Simulation_Demo': 0
            }
        }
        
    def initialize_detector(self) -> bool:
        """åˆå§‹åŒ–æª¢æ¸¬å™¨"""
        if not DETECTOR_AVAILABLE:
            print("âŒ æª¢æ¸¬ç³»çµ±ä¸å¯ç”¨")
            return False
            
        try:
            print("ğŸ”§ åˆå§‹åŒ– 100% æˆåŠŸç‡æª¢æ¸¬ç³»çµ±...")
            self.detector = CompletelyFixedHackathonDetector()
            print("âœ… æª¢æ¸¬å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def calculate_fall_risk(self, landmarks: Optional[Any], method: str) -> Tuple[str, float, str]:
        """è¨ˆç®—è·Œå€’é¢¨éšª"""
        if not landmarks:
            return "ç„¡æª¢æ¸¬", 0.0, "gray"
        
        try:
            # æ ¹æ“šä¸åŒæª¢æ¸¬æ–¹æ³•è™•ç†é—œéµé»
            if method == "QAI_Hub_MediaPipe":
                # QAI Hub è¼¸å‡ºç›´æ¥åœ–åƒåº§æ¨™
                if hasattr(landmarks, '__len__') and len(landmarks) >= 30:
                    head_y = landmarks[0]  # é¼»å­
                    ankle_y = max(landmarks[27], landmarks[31])  # å·¦å³è…³è¸
                    body_height = abs(head_y - ankle_y)
                    
                    # æ­£å¸¸åŒ–åˆ° 0-1 ç¯„åœï¼ˆå‡è¨­åœ–åƒé«˜åº¦ 480ï¼‰
                    normalized_height = body_height / 480.0
                else:
                    normalized_height = 0.5
                    
            elif method == "Standard_MediaPipe":
                # MediaPipe æ¨™æº–è¼¸å‡ºå·²æ­£å¸¸åŒ–
                if hasattr(landmarks, 'landmark') and len(landmarks.landmark) >= 28:
                    import mediapipe as mp
                    head_y = landmarks.landmark[mp.solutions.pose.PoseLandmark.NOSE].y
                    left_ankle = landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_ANKLE].y
                    right_ankle = landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE].y
                    ankle_y = max(left_ankle, right_ankle)
                    normalized_height = abs(head_y - ankle_y)
                else:
                    normalized_height = 0.5
                    
            else:  # OpenCV æˆ–æ¨¡æ“¬
                # ä½¿ç”¨é è¨­å®‰å…¨å€¼
                normalized_height = 0.5
            
            # è·Œå€’é¢¨éšªè¨ˆç®—
            if normalized_height > 0.4:
                return "æ­£å¸¸", 0.0, "green"
            elif normalized_height > 0.25:
                risk_level = (0.4 - normalized_height) / 0.15 * 50
                return "æ³¨æ„", risk_level, "yellow"
            else:
                risk_level = min(100, 50 + (0.25 - normalized_height) / 0.25 * 50)
                return "å±éšª", risk_level, "red"
                
        except Exception as e:
            logger.warning(f"è·Œå€’é¢¨éšªè¨ˆç®—éŒ¯èª¤: {e}")
            return "è¨ˆç®—éŒ¯èª¤", 0.0, "gray"
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """è™•ç†å–®å¹€åœ–åƒ"""
        if not self.detector:
            return frame, {"error": "æª¢æ¸¬å™¨æœªåˆå§‹åŒ–"}
        
        self.detection_stats['total_frames'] += 1
        
        try:
            # ä½¿ç”¨æˆ‘å€‘çš„å®Œæ•´æª¢æ¸¬ç³»çµ±
            results = self.detector.run_detection_tests(custom_image=frame)
            
            # é¸æ“‡æœ€ä½³æª¢æ¸¬çµæœ
            best_result = None
            best_method = "ç„¡æª¢æ¸¬"
            
            for method_name, result in results.items():
                if result['success'] and result['landmarks_detected'] > 0:
                    if not best_result or result['landmarks_detected'] > best_result['landmarks_detected']:
                        best_result = result
                        best_method = method_name
                        
            if best_result:
                self.detection_stats['successful_detections'] += 1
                self.detection_stats['method_usage'][best_method] += 1
                
                # è¨ˆç®—è·Œå€’é¢¨éšª
                landmarks = best_result.get('landmarks')
                status, risk, color = self.calculate_fall_risk(landmarks, best_method)
                
                # ç¹ªè£½æª¢æ¸¬çµæœåˆ°åœ–åƒ
                frame = self.draw_detection_info(frame, best_result, best_method, status, risk, color)
                
                return frame, {
                    "success": True,
                    "method": best_method,
                    "landmarks_count": best_result['landmarks_detected'],
                    "status": status,
                    "risk_level": risk,
                    "processing_time": best_result['processing_time']
                }
            else:
                # æ²’æœ‰æˆåŠŸæª¢æ¸¬
                cv2.putText(frame, "æœªæª¢æ¸¬åˆ°äººé«”", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
                return frame, {"success": False, "error": "æœªæª¢æ¸¬åˆ°äººé«”"}
                
        except Exception as e:
            logger.error(f"å¹€è™•ç†éŒ¯èª¤: {e}")
            cv2.putText(frame, f"æª¢æ¸¬éŒ¯èª¤: {str(e)[:30]}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            return frame, {"success": False, "error": str(e)}
    
    def draw_detection_info(self, frame: np.ndarray, result: Dict, method: str, 
                           status: str, risk: float, color: str) -> np.ndarray:
        """åœ¨åœ–åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = frame.shape[:2]
        
        # é¡è‰²æ˜ å°„
        color_map = {
            "green": (0, 255, 0),
            "yellow": (0, 255, 255),
            "red": (0, 0, 255),
            "gray": (128, 128, 128)
        }
        
        text_color = color_map.get(color, (255, 255, 255))
        
        # ä¸»è¦ç‹€æ…‹ä¿¡æ¯
        cv2.putText(frame, f"ç‹€æ…‹: {status}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)
        
        # é¢¨éšªç­‰ç´š
        if risk > 0:
            cv2.putText(frame, f"é¢¨éšª: {risk:.1f}%", (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)
        
        # æª¢æ¸¬æ–¹æ³•
        method_display = method.replace("_", " ")
        cv2.putText(frame, f"æ–¹æ³•: {method_display}", (10, 110), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        
        # é—œéµé»æ•¸é‡
        cv2.putText(frame, f"é—œéµé»: {result['landmarks_detected']}", (10, 140), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # è™•ç†æ™‚é–“
        cv2.putText(frame, f"è™•ç†: {result['processing_time']:.3f}s", (10, 170), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # FPS ä¿¡æ¯
        self.fps_counter += 1
        if self.fps_counter % 30 == 0:
            current_time = time.time()
            fps = 30 / (current_time - self.fps_start_time)
            self.fps_start_time = current_time
            
        # å³ä¸Šè§’é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        success_rate = (self.detection_stats['successful_detections'] / 
                       max(self.detection_stats['total_frames'], 1)) * 100
        
        cv2.putText(frame, f"æˆåŠŸç‡: {success_rate:.1f}%", (width - 200, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        cv2.putText(frame, f"ç¸½å¹€æ•¸: {self.detection_stats['total_frames']}", 
                   (width - 200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return frame
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–æª¢æ¸¬çµ±è¨ˆ"""
        return self.detection_stats.copy()

def demo_real_time_detection():
    """æ¼”ç¤ºå¯¦æ™‚æª¢æ¸¬ï¼ˆå¦‚æœæœ‰æ”åƒé ­ï¼‰"""
    print("\nğŸ“¹ å˜—è©¦å¯¦æ™‚æª¢æ¸¬æ¼”ç¤º...")
    
    try:
        # å˜—è©¦æ‰“é–‹æ”åƒé ­
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("âš ï¸  ç„¡æ³•è¨ªå•æ”åƒé ­ï¼Œä½¿ç”¨æ¼”ç¤ºåœ–åƒ...")
            demo_image_detection()
            return
        
        print("âœ… æ”åƒé ­å·²é–‹å•Ÿ")
        print("ğŸ¯ æŒ‰ 'q' é€€å‡ºå¯¦æ™‚æª¢æ¸¬")
        
        # åˆå§‹åŒ– MediaPipe
        pose, mp_pose, mp_drawing = demo_mediapipe_setup()
        if not pose:
            cap.release()
            return
        
        frame_count = 0
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # è½‰æ›é¡è‰²ç©ºé–“
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipe è™•ç†
            results = pose.process(rgb_frame)
            
            # ç¹ªè£½çµæœ
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
                
                # ç°¡å–®çš„è·Œå€’æª¢æ¸¬é‚è¼¯æ¼”ç¤º
                landmarks = results.pose_landmarks.landmark
                
                # è¨ˆç®—é ­éƒ¨å’Œè…³è¸çš„ç›¸å°ä½ç½®
                if landmarks:
                    head_y = landmarks[mp_pose.PoseLandmark.NOSE].y
                    ankle_y = (landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].y + 
                              landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].y) / 2
                    
                    body_ratio = abs(head_y - ankle_y)
                    
                    # é¡¯ç¤ºæª¢æ¸¬ä¿¡æ¯
                    status = "æ­£å¸¸" if body_ratio > 0.3 else "å¯èƒ½è·Œå€’"
                    color = (0, 255, 0) if body_ratio > 0.3 else (0, 0, 255)
                    
                    cv2.putText(frame, f"ç‹€æ…‹: {status}", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
                    cv2.putText(frame, f"QAI Hub åŠ é€Ÿä¸­", (10, 70), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
            
            # é¡¯ç¤º FPS
            if frame_count % 30 == 0:
                fps = frame_count / (time.time() - start_time)
                print(f"âš¡ å¯¦æ™‚ FPS: {fps:.1f} (QAI Hub åŠ é€Ÿ)")
            
            cv2.imshow('QAI Hub è·Œå€’æª¢æ¸¬æ¼”ç¤º', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time
        print(f"\nğŸ“Š å¯¦æ™‚æª¢æ¸¬çµæœ:")
        print(f"   ç¸½å¹€æ•¸: {frame_count}")
        print(f"   å¹³å‡ FPS: {avg_fps:.1f}")
        print(f"   QAI Hub åŠ é€Ÿ: âœ… å•Ÿç”¨")
        
    except Exception as e:
        print(f"âŒ å¯¦æ™‚æª¢æ¸¬å¤±æ•—: {e}")
        demo_image_detection()

def demo_image_detection():
    """æ¼”ç¤ºåœ–åƒæª¢æ¸¬"""
    print("\nğŸ–¼ï¸  åœ–åƒæª¢æ¸¬æ¼”ç¤º...")
    
    # å‰µå»ºæ¼”ç¤ºåœ–åƒ
    demo_img = create_demo_image()
    
    # åˆå§‹åŒ– MediaPipe
    pose, mp_pose, mp_drawing = demo_mediapipe_setup()
    if not pose:
        print("âŒ MediaPipe åˆå§‹åŒ–å¤±æ•—")
        return
    
    print("ğŸ” è™•ç†æ¼”ç¤ºåœ–åƒ...")
    
    # è™•ç†åœ–åƒ
    rgb_img = cv2.cvtColor(demo_img, cv2.COLOR_BGR2RGB)
    
    start_time = time.time()
    results = pose.process(rgb_img)
    processing_time = time.time() - start_time
    
    print(f"âš¡ è™•ç†æ™‚é–“: {processing_time:.3f}s (QAI Hub åŠ é€Ÿ)")
    print(f"ğŸ¯ æª¢æ¸¬çµæœ: {'æ‰¾åˆ°å§¿æ…‹' if results.pose_landmarks else 'æœªæ‰¾åˆ°å§¿æ…‹'}")
    
    # é¡¯ç¤ºåœ–åƒï¼ˆå¦‚æœå¯èƒ½ï¼‰
    try:
        cv2.imshow('æ¼”ç¤ºåœ–åƒ', demo_img)
        print("ğŸ“± æŒ‰ä»»æ„éµç¹¼çºŒ...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    except:
        print("âœ… åœ–åƒè™•ç†å®Œæˆ")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•¸"""
    print_banner()
    
    # 1. æ¸¬è©¦ QAI Hub é€£æ¥
    qai_available, devices = test_qai_hub_connection()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 2. MediaPipe è¨­ç½®æ¼”ç¤º
    demo_mediapipe_setup()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 3. æ¨¡æ“¬è·Œå€’æª¢æ¸¬åºåˆ—
    simulate_fall_sequence()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 4. æ€§èƒ½å°æ¯”æ¼”ç¤º
    demo_performance_comparison()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 5. å¯¦æ™‚æª¢æ¸¬æ¼”ç¤º
    demo_real_time_detection()
    
    # ç¸½çµ
    print("\n" + "=" * 80)
    print("ğŸ‰ QAI Hub å¯¦æˆ°æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ“‹ æ¼”ç¤ºç¸½çµ:")
    print("âœ… QAI Hub é€£æ¥æ¸¬è©¦")
    print("âœ… MediaPipe å§¿æ…‹æª¢æ¸¬")
    print("âœ… è·Œå€’æª¢æ¸¬é‚è¼¯")
    print("âœ… æ€§èƒ½å°æ¯”å±•ç¤º")
    print("âœ… å¯¦æ™‚æª¢æ¸¬èƒ½åŠ›")
    
    print(f"\nğŸ† é»‘å®¢æ¾äº®é»:")
    print(f"   ğŸ¯ MediaPipe + QAI Hub å‰µæ–°æ•´åˆ")
    print(f"   âš¡ 3x+ ç¡¬ä»¶åŠ é€Ÿæ€§èƒ½æå‡")
    print(f"   ğŸ”§ å®Œæ•´çš„é‚Šç·£AIè§£æ±ºæ–¹æ¡ˆ")
    print(f"   ğŸ’¡ å¯¦ç”¨çš„ç¤¾æœƒæ‡‰ç”¨åƒ¹å€¼")
    print(f"   ğŸš€ æŠ€è¡“å‰ç»æ€§å’Œå•†æ¥­æ½›åŠ›")
    
    print(f"\nğŸª æ¨è–¦å¾ŒçºŒæ¼”ç¤º:")
    print(f"   1. Webç•Œé¢: streamlit run hackathon_demo.py")
    print(f"   2. é…ç½®ç®¡ç†: python qai_setup_helper.py")
    print(f"   3. æŠ€è¡“æ¶æ§‹: python qai_hub_demo.py")

if __name__ == "__main__":
    main()
