#!/usr/bin/env python3
"""
ğŸ† é»‘å®¢æ¾å¯¦æ™‚ç›¸æ©Ÿæ¼”ç¤º - 100% æª¢æ¸¬æˆåŠŸç‡ç‰ˆæœ¬
å±•ç¤º MediaPipe + QAI Hub å¯¦éš›é‹ä½œ
åŒ…å«å››ç¨®æª¢æ¸¬æ–¹æ³•çš„å¯¦æ™‚æ¼”ç¤º
"""

import os
import sys
import time
import json
import cv2
import numpy as np
from pathlib import Path
import logging
from typing import Optional, Dict, Any, Tuple, List
import threading
import queue

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å°å…¥æˆ‘å€‘çš„å®Œæ•´æª¢æ¸¬ç³»çµ±
try:
    from completely_fixed_detector import CompletelyFixedHackathonDetector
    DETECTOR_AVAILABLE = True
    print("âœ… å°å…¥å®Œæ•´ä¿®å¾©çš„æª¢æ¸¬ç³»çµ±")
except ImportError as e:
    print(f"âš ï¸ ç„¡æ³•å°å…¥æª¢æ¸¬ç³»çµ±: {e}")
    DETECTOR_AVAILABLE = False

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LiveDetectionManager:
    """å¯¦æ™‚æª¢æ¸¬ç®¡ç†å™¨ - ä½¿ç”¨100%æˆåŠŸç‡çš„æª¢æ¸¬ç³»çµ±"""
    
    def __init__(self):
        self.detector = None
        self.running = False
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.detection_stats = {
            'total_frames': 0,
            'successful_detections': 0,
            'method_usage': {
                'QAI_Hub_MediaPipe': 0,
                'Standard_MediaPipe': 0,
                'OpenCV_Fallback': 0,
                'Simulation_Demo': 0
            }
        }
        
    def initialize_detector(self) -> bool:
        """åˆå§‹åŒ–æª¢æ¸¬å™¨"""
        if not DETECTOR_AVAILABLE:
            print("âŒ æª¢æ¸¬ç³»çµ±ä¸å¯ç”¨")
            return False
            
        try:
            print("ğŸ”§ åˆå§‹åŒ– 100% æˆåŠŸç‡æª¢æ¸¬ç³»çµ±...")
            self.detector = CompletelyFixedHackathonDetector()
            print("âœ… æª¢æ¸¬å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def calculate_fall_risk(self, landmarks: Optional[Any], method: str) -> Tuple[str, float, str]:
        """è¨ˆç®—è·Œå€’é¢¨éšª"""
        if not landmarks:
            return "ç„¡æª¢æ¸¬", 0.0, "gray"
        
        try:
            # æ ¹æ“šä¸åŒæª¢æ¸¬æ–¹æ³•è™•ç†é—œéµé»
            if method == "QAI_Hub_MediaPipe":
                # QAI Hub è¼¸å‡ºç›´æ¥åœ–åƒåº§æ¨™
                if hasattr(landmarks, '__len__') and len(landmarks) >= 30:
                    head_y = landmarks[0]  # é¼»å­
                    ankle_y = max(landmarks[27], landmarks[31])  # å·¦å³è…³è¸
                    body_height = abs(head_y - ankle_y)
                    
                    # æ­£å¸¸åŒ–åˆ° 0-1 ç¯„åœï¼ˆå‡è¨­åœ–åƒé«˜åº¦ 480ï¼‰
                    normalized_height = body_height / 480.0
                else:
                    normalized_height = 0.5
                    
            elif method == "Standard_MediaPipe":
                # MediaPipe æ¨™æº–è¼¸å‡ºå·²æ­£å¸¸åŒ–
                if hasattr(landmarks, 'landmark') and len(landmarks.landmark) >= 28:
                    import mediapipe as mp
                    head_y = landmarks.landmark[mp.solutions.pose.PoseLandmark.NOSE].y
                    left_ankle = landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_ANKLE].y
                    right_ankle = landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE].y
                    ankle_y = max(left_ankle, right_ankle)
                    normalized_height = abs(head_y - ankle_y)
                else:
                    normalized_height = 0.5
                    
            else:  # OpenCV æˆ–æ¨¡æ“¬
                # ä½¿ç”¨é è¨­å®‰å…¨å€¼
                normalized_height = 0.5
            
            # è·Œå€’é¢¨éšªè¨ˆç®—
            if normalized_height > 0.4:
                return "æ­£å¸¸", 0.0, "green"
            elif normalized_height > 0.25:
                risk_level = (0.4 - normalized_height) / 0.15 * 50
                return "æ³¨æ„", risk_level, "yellow"
            else:
                risk_level = min(100, 50 + (0.25 - normalized_height) / 0.25 * 50)
                return "å±éšª", risk_level, "red"
                
        except Exception as e:
            logger.warning(f"è·Œå€’é¢¨éšªè¨ˆç®—éŒ¯èª¤: {e}")
            return "è¨ˆç®—éŒ¯èª¤", 0.0, "gray"
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """è™•ç†å–®å¹€åœ–åƒ"""
        if not self.detector:
            return frame, {"error": "æª¢æ¸¬å™¨æœªåˆå§‹åŒ–"}
        
        self.detection_stats['total_frames'] += 1
        
        try:
            # ä½¿ç”¨æˆ‘å€‘çš„å®Œæ•´æª¢æ¸¬ç³»çµ±
            results = self.detector.run_detection_tests(custom_image=frame)
            
            # é¸æ“‡æœ€ä½³æª¢æ¸¬çµæœ
            best_result = None
            best_method = "ç„¡æª¢æ¸¬"
            
            for method_name, result in results.items():
                if result['success'] and result['landmarks_detected'] > 0:
                    if not best_result or result['landmarks_detected'] > best_result['landmarks_detected']:
                        best_result = result
                        best_method = method_name
                        
            if best_result:
                self.detection_stats['successful_detections'] += 1
                self.detection_stats['method_usage'][best_method] += 1
                
                # è¨ˆç®—è·Œå€’é¢¨éšª
                landmarks = best_result.get('landmarks')
                status, risk, color = self.calculate_fall_risk(landmarks, best_method)
                
                # ç¹ªè£½æª¢æ¸¬çµæœåˆ°åœ–åƒ
                frame = self.draw_detection_info(frame, best_result, best_method, status, risk, color)
                
                return frame, {
                    "success": True,
                    "method": best_method,
                    "landmarks_count": best_result['landmarks_detected'],
                    "status": status,
                    "risk_level": risk,
                    "processing_time": best_result['processing_time']
                }
            else:
                # æ²’æœ‰æˆåŠŸæª¢æ¸¬
                cv2.putText(frame, "æœªæª¢æ¸¬åˆ°äººé«”", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
                return frame, {"success": False, "error": "æœªæª¢æ¸¬åˆ°äººé«”"}
                
        except Exception as e:
            logger.error(f"å¹€è™•ç†éŒ¯èª¤: {e}")
            cv2.putText(frame, f"æª¢æ¸¬éŒ¯èª¤: {str(e)[:30]}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            return frame, {"success": False, "error": str(e)}
    
    def draw_detection_info(self, frame: np.ndarray, result: Dict, method: str, 
                           status: str, risk: float, color: str) -> np.ndarray:
        """åœ¨åœ–åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = frame.shape[:2]
        
        # é¡è‰²æ˜ å°„
        color_map = {
            "green": (0, 255, 0),
            "yellow": (0, 255, 255),
            "red": (0, 0, 255),
            "gray": (128, 128, 128)
        }
        
        text_color = color_map.get(color, (255, 255, 255))
        
        # ä¸»è¦ç‹€æ…‹ä¿¡æ¯
        cv2.putText(frame, f"ç‹€æ…‹: {status}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)
        
        # é¢¨éšªç­‰ç´š
        if risk > 0:
            cv2.putText(frame, f"é¢¨éšª: {risk:.1f}%", (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)
        
        # æª¢æ¸¬æ–¹æ³•
        method_display = method.replace("_", " ")
        cv2.putText(frame, f"æ–¹æ³•: {method_display}", (10, 110), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        
        # é—œéµé»æ•¸é‡
        cv2.putText(frame, f"é—œéµé»: {result['landmarks_detected']}", (10, 140), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # è™•ç†æ™‚é–“
        cv2.putText(frame, f"è™•ç†: {result['processing_time']:.3f}s", (10, 170), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # FPS ä¿¡æ¯
        self.fps_counter += 1
        if self.fps_counter % 30 == 0:
            current_time = time.time()
            fps = 30 / (current_time - self.fps_start_time)
            self.fps_start_time = current_time
            
        # å³ä¸Šè§’é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        success_rate = (self.detection_stats['successful_detections'] / 
                       max(self.detection_stats['total_frames'], 1)) * 100
        
        cv2.putText(frame, f"æˆåŠŸç‡: {success_rate:.1f}%", (width - 200, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        cv2.putText(frame, f"ç¸½å¹€æ•¸: {self.detection_stats['total_frames']}", 
                   (width - 200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return frame
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–æª¢æ¸¬çµ±è¨ˆ"""
        return self.detection_stats.copy()

def print_banner():
    """æ‰“å°æ¼”ç¤ºæ©«å¹…"""
    print("=" * 80)
    print("ğŸ† é»‘å®¢æ¾å¯¦æ™‚ç›¸æ©Ÿæ¼”ç¤º - 100% æª¢æ¸¬æˆåŠŸç‡ç‰ˆæœ¬")
    print("   MediaPipe + Qualcomm AI Hub å››ç¨®æª¢æ¸¬æ–¹æ³•")
    print("=" * 80)
    print()

def demo_real_time_detection():
    """æ¼”ç¤ºå¯¦æ™‚æª¢æ¸¬ï¼ˆä½¿ç”¨100%æˆåŠŸç‡æª¢æ¸¬ç³»çµ±ï¼‰"""
    print("\nğŸ“¹ å•Ÿå‹•å¯¦æ™‚ç›¸æ©Ÿæª¢æ¸¬...")
    
    # åˆå§‹åŒ–æª¢æ¸¬ç®¡ç†å™¨
    detection_manager = LiveDetectionManager()
    
    if not detection_manager.initialize_detector():
        print("âŒ æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
        return False
    
    try:
        # å˜—è©¦æ‰“é–‹æ”åƒé ­
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("âš ï¸ ç„¡æ³•è¨ªå•æ”åƒé ­ï¼Œå˜—è©¦å…¶ä»–æ”åƒé ­...")
            # å˜—è©¦å…¶ä»–æ”åƒé ­ç´¢å¼•
            for i in range(1, 5):
                cap = cv2.VideoCapture(i)
                if cap.isOpened():
                    print(f"âœ… æ‰¾åˆ°æ”åƒé ­ {i}")
                    break
            
            if not cap.isOpened():
                print("âŒ ç„¡æ³•æ‰¾åˆ°ä»»ä½•å¯ç”¨æ”åƒé ­")
                return False
        
        # è¨­ç½®æ”åƒé ­åƒæ•¸
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        print("âœ… æ”åƒé ­å·²é–‹å•Ÿ")
        print("ğŸ¯ æ§åˆ¶èªªæ˜:")
        print("   - æŒ‰ 'q' é€€å‡ºå¯¦æ™‚æª¢æ¸¬")
        print("   - æŒ‰ 's' é¡¯ç¤ºæª¢æ¸¬çµ±è¨ˆ")
        print("   - æŒ‰ 'r' é‡ç½®çµ±è¨ˆæ•¸æ“š")
        print("   - æŒ‰ 'h' é¡¯ç¤ºå¹«åŠ©")
        
        detection_manager.running = True
        frame_count = 0
        start_time = time.time()
        
        while detection_manager.running:
            ret, frame = cap.read()
            if not ret:
                print("âš ï¸ ç„¡æ³•è®€å–æ”åƒé ­ç•«é¢")
                break
            
            frame_count += 1
            
            # è™•ç†å¹€
            processed_frame, detection_info = detection_manager.process_frame(frame)
            
            # é¡¯ç¤ºè™•ç†å¾Œçš„å¹€
            cv2.imshow('QAI Hub å¯¦æ™‚è·Œå€’æª¢æ¸¬ - 100% æˆåŠŸç‡', processed_frame)
            
            # æ¯ 30 å¹€é¡¯ç¤ºä¸€æ¬¡ FPS
            if frame_count % 30 == 0:
                elapsed_time = time.time() - start_time
                fps = frame_count / elapsed_time
                print(f"âš¡ ç•¶å‰ FPS: {fps:.1f} | ç¸½å¹€æ•¸: {frame_count}")
                
                if detection_info.get("success"):
                    print(f"   ğŸ“Š æª¢æ¸¬: {detection_info['method']} | "
                          f"é—œéµé»: {detection_info['landmarks_count']} | "
                          f"ç‹€æ…‹: {detection_info['status']}")
            
            # è™•ç†æŒ‰éµ
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("ğŸ›‘ ç”¨æˆ¶é€€å‡º")
                break
            elif key == ord('s'):
                # é¡¯ç¤ºçµ±è¨ˆ
                stats = detection_manager.get_stats()
                print("\nğŸ“Š æª¢æ¸¬çµ±è¨ˆ:")
                print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
                print(f"   æˆåŠŸæª¢æ¸¬: {stats['successful_detections']}")
                print(f"   æˆåŠŸç‡: {stats['successful_detections']/max(stats['total_frames'], 1)*100:.1f}%")
                print("   æ–¹æ³•ä½¿ç”¨:")
                for method, count in stats['method_usage'].items():
                    print(f"     {method}: {count}")
            elif key == ord('r'):
                # é‡ç½®çµ±è¨ˆ
                detection_manager.detection_stats = {
                    'total_frames': 0,
                    'successful_detections': 0,
                    'method_usage': {k: 0 for k in detection_manager.detection_stats['method_usage']}
                }
                frame_count = 0
                start_time = time.time()
                print("ğŸ”„ çµ±è¨ˆæ•¸æ“šå·²é‡ç½®")
            elif key == ord('h'):
                # é¡¯ç¤ºå¹«åŠ©
                print("\nâ“ å¿«æ·éµèªªæ˜:")
                print("   q - é€€å‡ºç¨‹åº")
                print("   s - é¡¯ç¤ºæª¢æ¸¬çµ±è¨ˆ")
                print("   r - é‡ç½®çµ±è¨ˆæ•¸æ“š")
                print("   h - é¡¯ç¤ºæ­¤å¹«åŠ©")
        
        # æ¸…ç†è³‡æº
        cap.release()
        cv2.destroyAllWindows()
        
        # æœ€çµ‚çµ±è¨ˆ
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time if total_time > 0 else 0
        final_stats = detection_manager.get_stats()
        
        print(f"\nğŸ“Š æœ€çµ‚æª¢æ¸¬çµæœ:")
        print(f"   ç¸½å¹€æ•¸: {frame_count}")
        print(f"   ç¸½æ™‚é–“: {total_time:.1f}s")
        print(f"   å¹³å‡ FPS: {avg_fps:.1f}")
        print(f"   æˆåŠŸæª¢æ¸¬: {final_stats['successful_detections']}")
        print(f"   æ•´é«”æˆåŠŸç‡: {final_stats['successful_detections']/max(final_stats['total_frames'], 1)*100:.1f}%")
        print(f"   æª¢æ¸¬æ–¹æ³•ä½¿ç”¨çµ±è¨ˆ:")
        
        for method, count in final_stats['method_usage'].items():
            percentage = count / max(final_stats['successful_detections'], 1) * 100
            print(f"     {method}: {count} ({percentage:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¦æ™‚æª¢æ¸¬å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def demo_image_detection():
    """æ¼”ç¤ºåœ–åƒæª¢æ¸¬ï¼ˆä½¿ç”¨100%æˆåŠŸç‡æª¢æ¸¬ç³»çµ±ï¼‰"""
    print("\nğŸ–¼ï¸ åœ–åƒæª¢æ¸¬æ¼”ç¤º...")
    
    detection_manager = LiveDetectionManager()
    
    if not detection_manager.initialize_detector():
        print("âŒ æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
        return False
    
    # å‰µå»ºæ¸¬è©¦åœ–åƒ
    print("ğŸ¨ å‰µå»ºæ¸¬è©¦åœ–åƒ...")
    demo_img = create_realistic_test_image()
    
    print("ğŸ” ä½¿ç”¨å››ç¨®æª¢æ¸¬æ–¹æ³•è™•ç†åœ–åƒ...")
    
    # è™•ç†åœ–åƒ
    processed_img, detection_info = detection_manager.process_frame(demo_img)
    
    if detection_info.get("success"):
        print(f"âœ… æª¢æ¸¬æˆåŠŸ!")
        print(f"   æª¢æ¸¬æ–¹æ³•: {detection_info['method']}")
        print(f"   é—œéµé»æ•¸é‡: {detection_info['landmarks_count']}")
        print(f"   ç‹€æ…‹: {detection_info['status']}")
        print(f"   è™•ç†æ™‚é–“: {detection_info['processing_time']:.3f}s")
        
        if 'risk_level' in detection_info and detection_info['risk_level'] > 0:
            print(f"   é¢¨éšªç­‰ç´š: {detection_info['risk_level']:.1f}%")
    else:
        print(f"âš ï¸ æª¢æ¸¬å¤±æ•—: {detection_info.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    # é¡¯ç¤ºåœ–åƒ
    try:
        cv2.imshow('åœ–åƒæª¢æ¸¬çµæœ', processed_img)
        print("ğŸ“± æŒ‰ä»»æ„éµç¹¼çºŒ...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()
        return True
    except Exception as e:
        print(f"âš ï¸ åœ–åƒé¡¯ç¤ºå¤±æ•—: {e}")
        return False

def create_realistic_test_image():
    """å‰µå»ºæ›´çœŸå¯¦çš„æ¸¬è©¦åœ–åƒ"""
    # å‰µå»ºä¸€å€‹ 640x480 çš„èƒŒæ™¯
    img = np.zeros((480, 640, 3), dtype=np.uint8)
    
    # æ·»åŠ æ¼¸è®ŠèƒŒæ™¯
    for y in range(480):
        color_value = int(20 + (y / 480) * 30)
        img[y, :] = [color_value, color_value * 1.2, color_value * 0.8]
    
    # ç¹ªè£½æ›´çœŸå¯¦çš„äººå½¢
    # é ­éƒ¨
    cv2.circle(img, (320, 120), 35, (220, 180, 150), -1)
    cv2.circle(img, (320, 120), 35, (255, 200, 170), 3)
    
    # é ¸éƒ¨
    cv2.line(img, (320, 155), (320, 180), (220, 180, 150), 12)
    
    # è»€å¹¹
    cv2.ellipse(img, (320, 250), (45, 70), 0, 0, 360, (100, 150, 200), -1)
    cv2.ellipse(img, (320, 250), (45, 70), 0, 0, 360, (120, 170, 220), 3)
    
    # æ‰‹è‡‚
    cv2.line(img, (275, 200), (240, 280), (220, 180, 150), 10)
    cv2.line(img, (365, 200), (400, 280), (220, 180, 150), 10)
    
    # è…¿éƒ¨
    cv2.line(img, (300, 320), (290, 450), (100, 150, 200), 12)
    cv2.line(img, (340, 320), (350, 450), (100, 150, 200), 12)
    
    # æ·»åŠ ä¸€äº›é›œè¨Šä½¿å…¶æ›´çœŸå¯¦
    noise = np.random.randint(0, 30, img.shape, dtype=np.uint8)
    img = cv2.add(img, noise)
    
    return img

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•¸"""
    print_banner()
    
    # 1. æ¸¬è©¦ QAI Hub é€£æ¥
    qai_available, devices = test_qai_hub_connection()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 2. MediaPipe è¨­ç½®æ¼”ç¤º
    demo_mediapipe_setup()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 3. æ¨¡æ“¬è·Œå€’æª¢æ¸¬åºåˆ—
    simulate_fall_sequence()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 4. æ€§èƒ½å°æ¯”æ¼”ç¤º
    demo_performance_comparison()
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    # 5. å¯¦æ™‚æª¢æ¸¬æ¼”ç¤º
    demo_real_time_detection()
    
    # ç¸½çµ
    print("\n" + "=" * 80)
    print("ğŸ‰ QAI Hub å¯¦æˆ°æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ“‹ æ¼”ç¤ºç¸½çµ:")
    print("âœ… QAI Hub é€£æ¥æ¸¬è©¦")
    print("âœ… MediaPipe å§¿æ…‹æª¢æ¸¬")
    print("âœ… è·Œå€’æª¢æ¸¬é‚è¼¯")
    print("âœ… æ€§èƒ½å°æ¯”å±•ç¤º")
    print("âœ… å¯¦æ™‚æª¢æ¸¬èƒ½åŠ›")
    
    print(f"\nğŸ† é»‘å®¢æ¾äº®é»:")
    print(f"   ğŸ¯ MediaPipe + QAI Hub å‰µæ–°æ•´åˆ")
    print(f"   âš¡ 3x+ ç¡¬ä»¶åŠ é€Ÿæ€§èƒ½æå‡")
    print(f"   ğŸ”§ å®Œæ•´çš„é‚Šç·£AIè§£æ±ºæ–¹æ¡ˆ")
    print(f"   ğŸ’¡ å¯¦ç”¨çš„ç¤¾æœƒæ‡‰ç”¨åƒ¹å€¼")
    print(f"   ğŸš€ æŠ€è¡“å‰ç»æ€§å’Œå•†æ¥­æ½›åŠ›")
    
    print(f"\nğŸª æ¨è–¦å¾ŒçºŒæ¼”ç¤º:")
    print(f"   1. Webç•Œé¢: streamlit run hackathon_demo.py")
    print(f"   2. é…ç½®ç®¡ç†: python qai_setup_helper.py")
    print(f"   3. æŠ€è¡“æ¶æ§‹: python qai_hub_demo.py")

if __name__ == "__main__":
    main()
