#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ† é»‘å®¢æ¾ - QAI Hub MediaPipe è·Œå€’æª¢æ¸¬ç³»çµ± (ä¿®æ­£ç‰ˆ)
ä½¿ç”¨ Qualcomm AI Hub çš„ MediaPipe Pose æ¨¡å‹å¯¦ç¾è·Œå€’æª¢æ¸¬
å®Œå…¨è§£æ±º protobuf ç‰ˆæœ¬è¡çªå•é¡Œï¼Œæ­£ç¢ºè§£æ QAI Hub è¼¸å‡ºæ ¼å¼
"""

import cv2
import numpy as np
import time
import math
from typing import Tuple, List, Optional, Dict, Any, Union
from dataclasses import dataclass
import logging
from PIL import Image
import torch

# QAI Hub MediaPipe Pose å°å…¥
from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
from qai_hub_models.models.mediapipe_pose.model import MediaPipePose, POSE_LANDMARK_CONNECTIONS

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PoseKeypoint:
    """å§¿æ…‹é—œéµé»"""
    x: float
    y: float
    z: float
    visibility: float

@dataclass
class FallDetectionResult:
    """è·Œå€’æª¢æ¸¬çµæœ"""
    is_fall: bool
    confidence: float
    body_angle: float
    risk_level: str
    reason: str
    timestamp: float
    landmarks_count: int

class QAIHubMediaPipeFallDetector:
    """åŸºæ–¼ QAI Hub MediaPipe çš„è·Œå€’æª¢æ¸¬å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, 
                 fall_threshold: float = 30.0,
                 position_change_threshold: float = 0.3,
                 confidence_threshold: float = 0.5):
        """
        åˆå§‹åŒ–è·Œå€’æª¢æ¸¬å™¨
        
        Args:
            fall_threshold: è·Œå€’è§’åº¦é–¾å€¼ï¼ˆåº¦ï¼‰
            position_change_threshold: ä½ç½®è®ŠåŒ–é–¾å€¼
            confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼
        """
        self.fall_threshold = fall_threshold
        self.position_change_threshold = position_change_threshold
        self.confidence_threshold = confidence_threshold
        
        # è¼‰å…¥ QAI Hub MediaPipe æ¨¡å‹
        logger.info("ğŸš€ è¼‰å…¥ QAI Hub MediaPipe Pose æ¨¡å‹...")
        self.pose_model = MediaPipePose.from_pretrained()
        self.pose_app = MediaPipePoseApp.from_pretrained(self.pose_model)
        logger.info("âœ… QAI Hub MediaPipe Pose æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # æ­·å²æ•¸æ“šç”¨æ–¼æ™‚åºåˆ†æ
        self.pose_history: List[List[PoseKeypoint]] = []
        self.max_history = 10
        
        # MediaPipe é—œéµé»ç´¢å¼• (33é»æ¨¡å‹)
        self.NOSE = 0
        self.LEFT_EYE_INNER = 1
        self.LEFT_EYE = 2
        self.LEFT_EYE_OUTER = 3
        self.RIGHT_EYE_INNER = 4
        self.RIGHT_EYE = 5
        self.RIGHT_EYE_OUTER = 6
        self.LEFT_EAR = 7
        self.RIGHT_EAR = 8
        self.MOUTH_LEFT = 9
        self.MOUTH_RIGHT = 10
        self.LEFT_SHOULDER = 11
        self.RIGHT_SHOULDER = 12
        self.LEFT_ELBOW = 13
        self.RIGHT_ELBOW = 14
        self.LEFT_WRIST = 15
        self.RIGHT_WRIST = 16
        self.LEFT_PINKY = 17
        self.RIGHT_PINKY = 18
        self.LEFT_INDEX = 19
        self.RIGHT_INDEX = 20
        self.LEFT_THUMB = 21
        self.RIGHT_THUMB = 22
        self.LEFT_HIP = 23
        self.RIGHT_HIP = 24
        self.LEFT_KNEE = 25
        self.RIGHT_KNEE = 26
        self.LEFT_ANKLE = 27
        self.RIGHT_ANKLE = 28
        self.LEFT_HEEL = 29
        self.RIGHT_HEEL = 30
        self.LEFT_FOOT_INDEX = 31
        self.RIGHT_FOOT_INDEX = 32
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'total_frames': 0,
            'successful_detections': 0,
            'fall_detections': 0,
            'avg_processing_time': 0.0,
            'last_detection_time': None
        }
    
    def detect_pose(self, image: np.ndarray) -> Optional[List[PoseKeypoint]]:
        """
        ä½¿ç”¨ QAI Hub MediaPipe æª¢æ¸¬å§¿æ…‹
        
        Args:
            image: è¼¸å…¥åœ–åƒ (BGR format)
            
        Returns:
            å§¿æ…‹é—œéµé»åˆ—è¡¨ï¼Œå¦‚æœæª¢æ¸¬å¤±æ•—å‰‡è¿”å› None
        """
        try:
            start_time = time.time()
            
            # è½‰æ›åœ–åƒæ ¼å¼ (BGR -> RGB)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
            
            # ä½¿ç”¨ QAI Hub MediaPipe é€²è¡Œå§¿æ…‹æª¢æ¸¬ (raw_output=True ç²å–åŸå§‹æ•¸æ“š)
            result = self.pose_app.predict_landmarks_from_image(pil_image, raw_output=True)
            
            processing_time = time.time() - start_time
            self.stats['avg_processing_time'] = (
                self.stats['avg_processing_time'] * self.stats['total_frames'] + processing_time
            ) / (self.stats['total_frames'] + 1)
            
            # è§£æ QAI Hub çš„è¼¸å‡º
            if result and len(result) >= 4:
                landmarks = self._parse_qai_hub_results(result)
                if landmarks:
                    self.stats['successful_detections'] += 1
                return landmarks
            
            return None
            
        except Exception as e:
            logger.error(f"å§¿æ…‹æª¢æ¸¬éŒ¯èª¤: {e}")
            return None
    
    def _parse_qai_hub_results(self, results) -> Optional[List[PoseKeypoint]]:
        """
        è§£æ QAI Hub MediaPipe çš„æª¢æ¸¬çµæœ
        
        Args:
            results: QAI Hub çš„åŸå§‹æª¢æ¸¬çµæœ
                     (batched_selected_boxes, batched_selected_keypoints, batched_roi_4corners, landmarks_out)
            
        Returns:
            æ¨™æº–åŒ–çš„é—œéµé»åˆ—è¡¨
        """
        try:
            batched_selected_boxes, batched_selected_keypoints, batched_roi_4corners, *landmarks_out = results
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æª¢æ¸¬åˆ°çš„å§¿æ…‹
            if not landmarks_out or len(landmarks_out) == 0:
                return None
            
            # ç²å–ç¬¬ä¸€æ‰¹çš„ç¬¬ä¸€å€‹æª¢æ¸¬çµæœ
            batch_landmarks = landmarks_out[0]  # ç¬¬ä¸€æ‰¹
            if not isinstance(batch_landmarks, list) or len(batch_landmarks) == 0:
                return None
            
            first_person_landmarks = batch_landmarks[0]  # ç¬¬ä¸€å€‹äºº
            if not isinstance(first_person_landmarks, torch.Tensor):
                return None
            
            # è½‰æ› torch tensor åˆ° numpy
            landmarks_array = first_person_landmarks.cpu().numpy()
            
            # ç¢ºä¿æœ‰æ­£ç¢ºçš„å½¢ç‹€ [num_landmarks, 3] å…¶ä¸­ 3 = (x, y, confidence)
            if landmarks_array.shape[1] != 3:
                logger.warning(f"æ„å¤–çš„ landmarks å½¢ç‹€: {landmarks_array.shape}")
                return None
            
            # è½‰æ›ç‚ºæˆ‘å€‘çš„ PoseKeypoint æ ¼å¼
            landmarks = []
            for i in range(landmarks_array.shape[0]):
                x, y, confidence = landmarks_array[i]
                landmarks.append(PoseKeypoint(
                    x=float(x),
                    y=float(y),
                    z=0.0,  # QAI Hub MediaPipe å¯èƒ½ä¸æä¾› z åº§æ¨™
                    visibility=float(confidence)
                ))
            
            return landmarks
            
        except Exception as e:
            logger.error(f"è§£æ QAI Hub çµæœéŒ¯èª¤: {e}")
            return None
    
    def calculate_body_angle(self, landmarks: List[PoseKeypoint]) -> float:
        """
        è¨ˆç®—èº«é«”å‚¾æ–œè§’åº¦
        
        Args:
            landmarks: å§¿æ…‹é—œéµé»
            
        Returns:
            èº«é«”å‚¾æ–œè§’åº¦ï¼ˆåº¦ï¼‰
        """
        try:
            # ç¢ºä¿æœ‰è¶³å¤ çš„é—œéµé»
            if len(landmarks) < 33:
                return 0.0
            
            # ç²å–è‚©è†€å’Œé«–éƒ¨çš„é—œéµé»
            left_shoulder = landmarks[self.LEFT_SHOULDER]
            right_shoulder = landmarks[self.RIGHT_SHOULDER]
            left_hip = landmarks[self.LEFT_HIP]
            right_hip = landmarks[self.RIGHT_HIP]
            
            # æª¢æŸ¥é—œéµé»å¯è¦‹æ€§
            min_visibility = 0.3
            if (left_shoulder.visibility < min_visibility or 
                right_shoulder.visibility < min_visibility or
                left_hip.visibility < min_visibility or 
                right_hip.visibility < min_visibility):
                return 0.0
            
            # è¨ˆç®—èº«é«”ä¸­ç·šå‘é‡
            shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
            shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
            hip_center_x = (left_hip.x + right_hip.x) / 2
            hip_center_y = (left_hip.y + right_hip.y) / 2
            
            # è¨ˆç®—èº«é«”å‘é‡ï¼ˆå¾è‚©è†€åˆ°é«–éƒ¨ï¼‰
            body_vector_x = hip_center_x - shoulder_center_x
            body_vector_y = hip_center_y - shoulder_center_y
            
            # è¨ˆç®—èˆ‡å‚ç›´è»¸çš„å¤¾è§’
            # å‚ç›´å‘é‡ç‚º (0, 1)
            angle_rad = math.atan2(abs(body_vector_x), abs(body_vector_y))
            angle_deg = math.degrees(angle_rad)
            
            return angle_deg
            
        except Exception as e:
            logger.error(f"è¨ˆç®—èº«é«”è§’åº¦éŒ¯èª¤: {e}")
            return 0.0
    
    def analyze_fall_risk(self, landmarks: List[PoseKeypoint]) -> FallDetectionResult:
        """
        åˆ†æè·Œå€’é¢¨éšª
        
        Args:
            landmarks: å§¿æ…‹é—œéµé»
            
        Returns:
            è·Œå€’æª¢æ¸¬çµæœ
        """
        timestamp = time.time()
        landmarks_count = len(landmarks) if landmarks else 0
        
        if not landmarks or landmarks_count == 0:
            return FallDetectionResult(
                is_fall=False,
                confidence=0.0,
                body_angle=0.0,
                risk_level="æœªçŸ¥",
                reason="ç„¡å§¿æ…‹æª¢æ¸¬",
                timestamp=timestamp,
                landmarks_count=0
            )
        
        # è¨ˆç®—èº«é«”è§’åº¦
        body_angle = self.calculate_body_angle(landmarks)
        
        # åˆ¤æ–·è·Œå€’é¢¨éšª
        is_fall = body_angle > self.fall_threshold
        confidence = min(body_angle / self.fall_threshold, 1.0) if self.fall_threshold > 0 else 0.0
        
        # ç¢ºå®šé¢¨éšªç­‰ç´š
        if body_angle < 10:
            risk_level = "ä½"
            reason = "å§¿æ…‹æ­£å¸¸"
        elif body_angle < 20:
            risk_level = "ä¸­"
            reason = "è¼•å¾®å‚¾æ–œ"
        elif body_angle < self.fall_threshold:
            risk_level = "é«˜"
            reason = "èº«é«”å‚¾æ–œæ˜é¡¯"
        else:
            risk_level = "å±éšª"
            reason = "æª¢æ¸¬åˆ°è·Œå€’"
            self.stats['fall_detections'] += 1
            self.stats['last_detection_time'] = timestamp
        
        return FallDetectionResult(
            is_fall=is_fall,
            confidence=confidence,
            body_angle=body_angle,
            risk_level=risk_level,
            reason=reason,
            timestamp=timestamp,
            landmarks_count=landmarks_count
        )
    
    def draw_pose_landmarks(self, image: np.ndarray, landmarks: List[PoseKeypoint]) -> np.ndarray:
        """
        åœ¨åœ–åƒä¸Šç¹ªè£½å§¿æ…‹é—œéµé»
        
        Args:
            image: è¼¸å…¥åœ–åƒ
            landmarks: å§¿æ…‹é—œéµé»
            
        Returns:
            ç¹ªè£½äº†é—œéµé»çš„åœ–åƒ
        """
        if not landmarks:
            return image
        
        output_image = image.copy()
        height, width = image.shape[:2]
        
        # ç¹ªè£½é—œéµé»
        for i, landmark in enumerate(landmarks):
            if landmark.visibility > self.confidence_threshold:
                x = int(landmark.x * width)
                y = int(landmark.y * height)
                
                # ç¢ºä¿åº§æ¨™åœ¨åœ–åƒç¯„åœå…§
                if 0 <= x < width and 0 <= y < height:
                    cv2.circle(output_image, (x, y), 3, (0, 255, 0), -1)
                    # åªåœ¨é‡è¦é—œéµé»ä¸Šé¡¯ç¤ºç·¨è™Ÿ
                    if i in [self.NOSE, self.LEFT_SHOULDER, self.RIGHT_SHOULDER, 
                            self.LEFT_HIP, self.RIGHT_HIP, self.LEFT_KNEE, self.RIGHT_KNEE]:
                        cv2.putText(output_image, str(i), (x, y-5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ç¹ªè£½éª¨æ¶é€£æ¥
        for connection in POSE_LANDMARK_CONNECTIONS:
            start_idx, end_idx = connection
            if (start_idx < len(landmarks) and end_idx < len(landmarks) and
                landmarks[start_idx].visibility > self.confidence_threshold and
                landmarks[end_idx].visibility > self.confidence_threshold):
                
                start_x = int(landmarks[start_idx].x * width)
                start_y = int(landmarks[start_idx].y * height)
                end_x = int(landmarks[end_idx].x * width)
                end_y = int(landmarks[end_idx].y * height)
                
                # ç¢ºä¿åº§æ¨™åœ¨åœ–åƒç¯„åœå…§
                if (0 <= start_x < width and 0 <= start_y < height and
                    0 <= end_x < width and 0 <= end_y < height):
                    cv2.line(output_image, (start_x, start_y), (end_x, end_y), (255, 0, 0), 2)
        
        return output_image
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, FallDetectionResult]:
        """
        è™•ç†å–®å€‹å½±åƒå¹€
        
        Args:
            frame: è¼¸å…¥å½±åƒå¹€
            
        Returns:
            è™•ç†å¾Œçš„å½±åƒå’Œæª¢æ¸¬çµæœ
        """
        self.stats['total_frames'] += 1
        
        # æª¢æ¸¬å§¿æ…‹
        landmarks = self.detect_pose(frame)
        
        # åˆ†æè·Œå€’é¢¨éšª
        fall_result = self.analyze_fall_risk(landmarks)
        
        # ç¹ªè£½å§¿æ…‹
        output_frame = self.draw_pose_landmarks(frame, landmarks) if landmarks else frame
        
        # æ·»åŠ æª¢æ¸¬ä¿¡æ¯åˆ°å½±åƒ
        self._draw_detection_info(output_frame, fall_result)
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        if landmarks:
            self.pose_history.append(landmarks)
            if len(self.pose_history) > self.max_history:
                self.pose_history.pop(0)
        
        return output_frame, fall_result
    
    def _draw_detection_info(self, image: np.ndarray, result: FallDetectionResult):
        """åœ¨å½±åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = image.shape[:2]
        
        # æ ¹æ“šé¢¨éšªç­‰ç´šé¸æ“‡é¡è‰²
        color_map = {
            "ä½": (0, 255, 0),      # ç¶ è‰²
            "ä¸­": (0, 255, 255),    # é»ƒè‰²
            "é«˜": (0, 165, 255),    # æ©™è‰²
            "å±éšª": (0, 0, 255),    # ç´…è‰²
            "æœªçŸ¥": (128, 128, 128) # ç°è‰²
        }
        color = color_map.get(result.risk_level, (255, 255, 255))
        
        # ç¹ªè£½æª¢æ¸¬ä¿¡æ¯
        info_texts = [
            f"é¢¨éšªç­‰ç´š: {result.risk_level}",
            f"èº«é«”è§’åº¦: {result.body_angle:.1f}Â°",
            f"ç½®ä¿¡åº¦: {result.confidence:.2f}",
            f"é—œéµé»æ•¸: {result.landmarks_count}",
            f"ç‹€æ…‹: {result.reason}"
        ]
        
        for i, text in enumerate(info_texts):
            cv2.putText(image, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ·»åŠ è­¦å‘Š
        if result.is_fall:
            cv2.putText(image, "!! è·Œå€’è­¦å ± !!", (width//2 - 100, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
        
        # æ·»åŠ çµ±è¨ˆä¿¡æ¯
        stats_text = f"æª¢æ¸¬æˆåŠŸç‡: {self.stats['successful_detections']}/{self.stats['total_frames']}"
        cv2.putText(image, stats_text, (10, height - 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        stats = self.stats.copy()
        if stats['total_frames'] > 0:
            stats['success_rate'] = stats['successful_detections'] / stats['total_frames']
        else:
            stats['success_rate'] = 0.0
        return stats

def demo_webcam():
    """æ”åƒé ­å³æ™‚æ¼”ç¤º"""
    print("ğŸ¥ å•Ÿå‹•æ”åƒé ­å³æ™‚è·Œå€’æª¢æ¸¬æ¼”ç¤º...")
    print("æŒ‰ 'q' éµé€€å‡º")
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    detector = QAIHubMediaPipeFallDetector()
    
    # æ‰“é–‹æ”åƒé ­
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ç„¡æ³•æ‰“é–‹æ”åƒé ­")
        return
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ ç„¡æ³•è®€å–æ”åƒé ­æ•¸æ“š")
                break
            
            # è™•ç†å¹€
            output_frame, result = detector.process_frame(frame)
            
            # é¡¯ç¤ºçµæœ
            cv2.imshow('QAI Hub MediaPipe è·Œå€’æª¢æ¸¬', output_frame)
            
            # å¦‚æœæª¢æ¸¬åˆ°è·Œå€’ï¼Œæ‰“å°è­¦å‘Š
            if result.is_fall:
                print(f"ğŸš¨ è·Œå€’è­¦å ±ï¼è§’åº¦: {result.body_angle:.1f}Â°, "
                      f"ç½®ä¿¡åº¦: {result.confidence:.2f}, é—œéµé»: {result.landmarks_count}")
            
            # æŒ‰ 'q' é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        stats = detector.get_stats()
        print(f"\nğŸ“Š æª¢æ¸¬çµ±è¨ˆ:")
        print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
        print(f"   æˆåŠŸæª¢æ¸¬: {stats['successful_detections']}")
        print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"   è·Œå€’æª¢æ¸¬æ¬¡æ•¸: {stats['fall_detections']}")
        print(f"   å¹³å‡è™•ç†æ™‚é–“: {stats['avg_processing_time']:.3f}ç§’")

def demo_test():
    """æ¸¬è©¦æ¨¡å‹è¼‰å…¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ QAI Hub MediaPipe æ¨¡å‹è¼‰å…¥...")
    
    try:
        detector = QAIHubMediaPipeFallDetector()
        print("âœ… QAI Hub MediaPipe æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        print("ğŸ¯ ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é€²è¡Œè·Œå€’æª¢æ¸¬")
        
        # æ¸¬è©¦ä¸€å€‹è™›æ“¬å¹€
        test_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        output_frame, result = detector.process_frame(test_frame)
        
        print(f"ğŸ“Š æ¸¬è©¦çµæœ:")
        print(f"   æª¢æ¸¬ç‹€æ…‹: {result.reason}")
        print(f"   é—œéµé»æ•¸: {result.landmarks_count}")
        print(f"   è™•ç†æ™‚é–“: {detector.stats['avg_processing_time']:.3f}ç§’")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ† QAI Hub MediaPipe è·Œå€’æª¢æ¸¬ç³»çµ± (ä¿®æ­£ç‰ˆ)")
    print("=" * 55)
    print("é¸æ“‡æ¼”ç¤ºæ¨¡å¼:")
    print("1. æ”åƒé ­å³æ™‚æª¢æ¸¬")
    print("2. æ¸¬è©¦æ¨¡å‹è¼‰å…¥")
    print("3. é€€å‡º")
    
    try:
        choice = input("è«‹è¼¸å…¥é¸æ“‡ (1-3): ").strip()
        
        if choice == "1":
            demo_webcam()
        elif choice == "2":
            demo_test()
        elif choice == "3":
            print("ğŸ‘‹ å†è¦‹ï¼")
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
