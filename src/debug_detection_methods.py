#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QAI Hub MediaPipe èª¿è©¦å’Œä¿®å¾©å·¥å…·
è§£æ±ºè§£æçµæœå’Œæª¢æ¸¬å•é¡Œ
"""

import cv2
import numpy as np
from PIL import Image
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def debug_qai_hub_detailed():
    """è©³ç´°èª¿è©¦ QAI Hub MediaPipe"""
    print("ğŸ” è©³ç´°èª¿è©¦ QAI Hub MediaPipe...")
    
    try:
        from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
        from qai_hub_models.models.mediapipe_pose.model import MediaPipePose
        
        # è¼‰å…¥æ¨¡å‹
        print("ğŸ“¥ è¼‰å…¥æ¨¡å‹...")
        pose_model = MediaPipePose.from_pretrained()
        pose_app = MediaPipePoseApp.from_pretrained(pose_model)
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # æ¸¬è©¦ä¸åŒé¡å‹çš„åœ–åƒ
        test_images = []
        
        # 1. å‰µå»ºä¸€å€‹æ›´çœŸå¯¦çš„äººå½¢åœ–åƒ
        human_image = create_realistic_human_image()
        test_images.append(("realistic_human", human_image))
        
        # 2. ä½¿ç”¨å®˜æ–¹æ¸¬è©¦åœ–åƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            from qai_hub_models.models.mediapipe_pose.model import MODEL_ID, MODEL_ASSET_VERSION
            from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
            
            official_image_asset = CachedWebModelAsset.from_asset_store(
                MODEL_ID, MODEL_ASSET_VERSION, "pose.jpeg"
            )
            official_image = load_image(official_image_asset)
            if isinstance(official_image, Image.Image):
                official_image = np.array(official_image)
            test_images.append(("official_test", official_image))
            print("âœ… è¼‰å…¥å®˜æ–¹æ¸¬è©¦åœ–åƒ")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è¼‰å…¥å®˜æ–¹æ¸¬è©¦åœ–åƒ: {e}")
        
        # 3. å‰µå»ºç°¡å–®çš„é»‘ç™½äººå½¢
        simple_image = create_simple_human_silhouette()
        test_images.append(("simple_silhouette", simple_image))
        
        # æ¸¬è©¦æ¯å€‹åœ–åƒ
        for image_name, test_image in test_images:
            print(f"\nğŸ§ª æ¸¬è©¦åœ–åƒ: {image_name}")
            print("-" * 40)
            
            try:
                # ç¢ºä¿åœ–åƒæ ¼å¼æ­£ç¢º
                if test_image.dtype != np.uint8:
                    test_image = (test_image * 255).astype(np.uint8)
                
                if len(test_image.shape) == 3 and test_image.shape[2] == 3:
                    # BGR to RGB
                    rgb_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)
                else:
                    rgb_image = test_image
                
                pil_image = Image.fromarray(rgb_image)
                print(f"  åœ–åƒå°ºå¯¸: {pil_image.size}")
                print(f"  åœ–åƒæ¨¡å¼: {pil_image.mode}")
                
                # æ¸¬è©¦ raw_output=True
                print("  æ¸¬è©¦ raw_output=True...")
                result_raw = pose_app.predict_landmarks_from_image(pil_image, raw_output=True)
                print(f"  Raw çµæœé¡å‹: {type(result_raw)}")
                print(f"  Raw çµæœé•·åº¦: {len(result_raw) if hasattr(result_raw, '__len__') else 'N/A'}")
                
                if isinstance(result_raw, tuple) and len(result_raw) >= 4:
                    batched_selected_boxes, batched_selected_keypoints, batched_roi_4corners, *landmarks_out = result_raw
                    
                    print(f"  Boxes: {len(batched_selected_boxes)} batches")
                    print(f"  Keypoints: {len(batched_selected_keypoints)} batches")
                    print(f"  ROI corners: {len(batched_roi_4corners)} batches")
                    print(f"  Landmarks out: {len(landmarks_out)} outputs")
                    
                    # æª¢æŸ¥æ¯å€‹ batch çš„å…§å®¹
                    for i, (boxes, keypoints, roi, landmarks) in enumerate(zip(
                        batched_selected_boxes, batched_selected_keypoints, 
                        batched_roi_4corners, landmarks_out if landmarks_out else [None]
                    )):
                        print(f"    Batch {i}:")
                        print(f"      Boxes shape: {boxes.shape if hasattr(boxes, 'shape') else type(boxes)}")
                        print(f"      Keypoints shape: {keypoints.shape if hasattr(keypoints, 'shape') else type(keypoints)}")
                        print(f"      ROI shape: {roi.shape if hasattr(roi, 'shape') else type(roi)}")
                        if landmarks is not None:
                            print(f"      Landmarks type: {type(landmarks)}")
                            if hasattr(landmarks, 'shape'):
                                print(f"      Landmarks shape: {landmarks.shape}")
                            elif isinstance(landmarks, list):
                                print(f"      Landmarks list length: {len(landmarks)}")
                                if landmarks:
                                    print(f"      First landmark type: {type(landmarks[0])}")
                                    if hasattr(landmarks[0], 'shape'):
                                        print(f"      First landmark shape: {landmarks[0].shape}")
                
                # æ¸¬è©¦ raw_output=False
                print("  æ¸¬è©¦ raw_output=False...")
                result_normal = pose_app.predict_landmarks_from_image(pil_image, raw_output=False)
                print(f"  Normal çµæœé¡å‹: {type(result_normal)}")
                if isinstance(result_normal, list):
                    print(f"  Normal çµæœé•·åº¦: {len(result_normal)}")
                    if result_normal:
                        print(f"  ç¬¬ä¸€å€‹çµæœé¡å‹: {type(result_normal[0])}")
                        if hasattr(result_normal[0], 'shape'):
                            print(f"  ç¬¬ä¸€å€‹çµæœå½¢ç‹€: {result_normal[0].shape}")
                
                print(f"  âœ… {image_name} æ¸¬è©¦å®Œæˆ")
                
                # ä¿å­˜æ¸¬è©¦åœ–åƒä»¥ä¾›æª¢æŸ¥
                cv2.imwrite(f"debug_{image_name}.jpg", test_image)
                
            except Exception as e:
                print(f"  âŒ {image_name} æ¸¬è©¦å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
    
    except Exception as e:
        print(f"âŒ æ•´é«”æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

def create_realistic_human_image():
    """å‰µå»ºä¸€å€‹æ›´çœŸå¯¦çš„äººå½¢åœ–åƒ"""
    img = np.zeros((480, 640, 3), dtype=np.uint8)
    
    # èƒŒæ™¯è¨­ç‚ºç°è‰²
    img[:] = (128, 128, 128)
    
    center_x, center_y = 320, 240
    
    # é ­éƒ¨ (æ©¢åœ“å½¢)
    cv2.ellipse(img, (center_x, center_y - 120), (25, 35), 0, 0, 360, (220, 180, 150), -1)
    
    # è„–å­
    cv2.rectangle(img, (center_x - 8, center_y - 85), (center_x + 8, center_y - 70), (220, 180, 150), -1)
    
    # èº«é«” (çŸ©å½¢)
    cv2.rectangle(img, (center_x - 30, center_y - 70), (center_x + 30, center_y + 80), (100, 150, 200), -1)
    
    # æ‰‹è‡‚
    cv2.rectangle(img, (center_x - 70, center_y - 50), (center_x - 30, center_y + 20), (220, 180, 150), -1)
    cv2.rectangle(img, (center_x + 30, center_y - 50), (center_x + 70, center_y + 20), (220, 180, 150), -1)
    
    # å‰è‡‚
    cv2.rectangle(img, (center_x - 75, center_y + 20), (center_x - 55, center_y + 70), (220, 180, 150), -1)
    cv2.rectangle(img, (center_x + 55, center_y + 20), (center_x + 75, center_y + 70), (220, 180, 150), -1)
    
    # è…¿éƒ¨
    cv2.rectangle(img, (center_x - 20, center_y + 80), (center_x - 5, center_y + 160), (50, 100, 150), -1)
    cv2.rectangle(img, (center_x + 5, center_y + 80), (center_x + 20, center_y + 160), (50, 100, 150), -1)
    
    # å°è…¿
    cv2.rectangle(img, (center_x - 18, center_y + 160), (center_x - 7, center_y + 220), (220, 180, 150), -1)
    cv2.rectangle(img, (center_x + 7, center_y + 160), (center_x + 18, center_y + 220), (220, 180, 150), -1)
    
    # è…³
    cv2.ellipse(img, (center_x - 12, center_y + 230), (15, 8), 0, 0, 360, (0, 0, 0), -1)
    cv2.ellipse(img, (center_x + 12, center_y + 230), (15, 8), 0, 0, 360, (0, 0, 0), -1)
    
    return img

def create_simple_human_silhouette():
    """å‰µå»ºç°¡å–®çš„äººå½¢è¼ªå»“"""
    img = np.zeros((480, 640, 3), dtype=np.uint8)
    
    # ç™½è‰²èƒŒæ™¯
    img[:] = (255, 255, 255)
    
    center_x, center_y = 320, 240
    
    # é»‘è‰²äººå½¢è¼ªå»“
    # é ­
    cv2.circle(img, (center_x, center_y - 100), 30, (0, 0, 0), -1)
    
    # èº«é«”
    cv2.rectangle(img, (center_x - 25, center_y - 70), (center_x + 25, center_y + 50), (0, 0, 0), -1)
    
    # æ‰‹è‡‚
    cv2.rectangle(img, (center_x - 60, center_y - 50), (center_x - 25, center_y + 10), (0, 0, 0), -1)
    cv2.rectangle(img, (center_x + 25, center_y - 50), (center_x + 60, center_y + 10), (0, 0, 0), -1)
    
    # è…¿
    cv2.rectangle(img, (center_x - 15, center_y + 50), (center_x - 5, center_y + 150), (0, 0, 0), -1)
    cv2.rectangle(img, (center_x + 5, center_y + 50), (center_x + 15, center_y + 150), (0, 0, 0), -1)
    
    return img

def test_standard_mediapipe():
    """æ¸¬è©¦æ¨™æº– MediaPipe"""
    print("\nğŸ¯ æ¸¬è©¦æ¨™æº– MediaPipe...")
    
    try:
        import mediapipe as mp
        
        mp_pose = mp.solutions.pose
        mp_drawing = mp.solutions.drawing_utils
        
        pose = mp_pose.Pose(
            static_image_mode=True,  # æ”¹ç‚º True ç”¨æ–¼åœ–åƒæª¢æ¸¬
            model_complexity=2,      # æé«˜æ¨¡å‹è¤‡é›œåº¦
            smooth_landmarks=True,
            min_detection_confidence=0.3,  # é™ä½æª¢æ¸¬é–¾å€¼
            min_tracking_confidence=0.3    # é™ä½è¿½è¹¤é–¾å€¼
        )
        
        print("âœ… æ¨™æº– MediaPipe åˆå§‹åŒ–æˆåŠŸ")
        
        # æ¸¬è©¦åœ–åƒ
        test_images = [
            ("realistic_human", create_realistic_human_image()),
            ("simple_silhouette", create_simple_human_silhouette())
        ]
        
        for image_name, test_image in test_images:
            print(f"\n  æ¸¬è©¦åœ–åƒ: {image_name}")
            
            try:
                # è½‰æ›ç‚º RGB
                rgb_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)
                
                # è™•ç†åœ–åƒ
                results = pose.process(rgb_image)
                
                if results.pose_landmarks:
                    landmarks = results.pose_landmarks.landmark
                    print(f"    âœ… æª¢æ¸¬åˆ° {len(landmarks)} å€‹é—œéµé»")
                    
                    # æª¢æŸ¥é—œéµé»è³ªé‡
                    visible_landmarks = sum(1 for lm in landmarks if lm.visibility > 0.5)
                    print(f"    å¯è¦‹é—œéµé»: {visible_landmarks}/{len(landmarks)}")
                    
                    # ç¹ªè£½çµæœ
                    annotated_image = test_image.copy()
                    mp_drawing.draw_landmarks(
                        annotated_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS
                    )
                    cv2.imwrite(f"mediapipe_result_{image_name}.jpg", annotated_image)
                    print(f"    ä¿å­˜çµæœåœ–åƒ: mediapipe_result_{image_name}.jpg")
                    
                else:
                    print(f"    âŒ æœªæª¢æ¸¬åˆ°å§¿æ…‹")
                    
            except Exception as e:
                print(f"    âŒ è™•ç† {image_name} æ™‚å‡ºéŒ¯: {e}")
        
        pose.close()
        
    except Exception as e:
        print(f"âŒ æ¨™æº– MediaPipe æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_qai_hub_detailed()
    test_standard_mediapipe()
