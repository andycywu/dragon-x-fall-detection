#!/usr/bin/env python3
"""
ğŸš€ å®Œæ•´çš„QAI Hub + ONNX Runtimeé›†æˆç³»çµ±
æ­£ç¢ºè™•ç†MediaPipeæ¨¡å‹çµ„ä»¶ä¸¦å¯¦ç¾çœŸå¯¦QAI Hubé€£æ¥
"""

import os
import qai_hub as hub
import numpy as np
import cv2
import onnxruntime as ort
import torch
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional
import time
import json

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class QAIHubONNXComplete:
    """å®Œæ•´çš„QAI Hub + ONNX Runtimeé›†æˆç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»çµ±"""
        self.api_token = os.getenv('QAI_HUB_API_TOKEN')
        if not self.api_token:
            raise ValueError("âŒ è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®QAI_HUB_API_TOKEN")
        
        # ONNX Runtimeé…ç½®
        self.onnx_providers = self._setup_onnx_providers()
        self.onnx_sessions = {}
        
        # QAI Hubç›¸é—œ
        self.mediapipe_models = {}
        self.qai_hub_models = {}
        self.compile_jobs = {}
        
        logger.info("ğŸš€ åˆå§‹åŒ–å®Œæ•´QAI Hub + ONNXç³»çµ±...")
        self._verify_qai_hub_connection()
        
    def _verify_qai_hub_connection(self):
        """é©—è­‰QAI Hubé€£æ¥"""
        try:
            devices = hub.get_devices()
            logger.info(f"âœ… QAI Hubé€£æ¥æˆåŠŸï¼Œå¯ç”¨è¨­å‚™: {len(devices)}")
            
            # é¸æ“‡Snapdragonè¨­å‚™
            preferred_devices = [d for d in devices if any(keyword in d.name for keyword in 
                                                         ['Snapdragon', 'Samsung', 'Galaxy'])]
            if preferred_devices:
                self.target_device = preferred_devices[0]
                logger.info(f"ğŸ¯ é¸æ“‡ç›®æ¨™è¨­å‚™: {self.target_device.name}")
            else:
                self.target_device = devices[0] if devices else None
                logger.info(f"ğŸ¯ ä½¿ç”¨è¨­å‚™: {self.target_device.name if self.target_device else 'None'}")
                
        except Exception as e:
            logger.error(f"âŒ QAI Hubé€£æ¥å¤±æ•—: {e}")
            raise
    
    def _setup_onnx_providers(self):
        """è¨­ç½®ONNX Runtimeæä¾›å•†"""
        providers = []
        available = ort.get_available_providers()
        
        logger.info(f"ğŸ“‹ å¯ç”¨ONNXåŸ·è¡Œæä¾›å•†: {available}")
        
        # æŒ‰å„ªå…ˆç´šæ·»åŠ æä¾›å•†
        if 'CUDAExecutionProvider' in available:
            providers.append('CUDAExecutionProvider')
            logger.info("âœ… å•Ÿç”¨CUDA GPUåŠ é€Ÿ")
        elif 'DmlExecutionProvider' in available:
            providers.append('DmlExecutionProvider')
            logger.info("âœ… å•Ÿç”¨DirectML GPUåŠ é€Ÿ")
        elif 'CoreMLExecutionProvider' in available:
            providers.append('CoreMLExecutionProvider')
            logger.info("âœ… å•Ÿç”¨CoreMLåŠ é€Ÿ")
        
        providers.append('CPUExecutionProvider')
        logger.info("âœ… CPUåŸ·è¡Œæä¾›å•†å·²æ·»åŠ ")
        
        return providers
    
    def load_mediapipe_models(self):
        """è¼‰å…¥MediaPipeæ¨¡å‹çµ„ä»¶"""
        logger.info("ğŸ“¥ è¼‰å…¥QAI Hub MediaPipeæ¨¡å‹çµ„ä»¶...")
        
        model_configs = {
            'face_detector': {
                'module': 'qai_hub_models.models.mediapipe_face',
                'component': 'face_detector',
                'description': 'MediaPipe Face Detector',
                'input_size': (192, 192)
            },
            'face_landmark': {
                'module': 'qai_hub_models.models.mediapipe_face',
                'component': 'face_landmark_detector',
                'description': 'MediaPipe Face Landmark Detector',
                'input_size': (192, 192)
            },
            'pose_detector': {
                'module': 'qai_hub_models.models.mediapipe_pose',
                'component': 'pose_detector',
                'description': 'MediaPipe Pose Detector',
                'input_size': (256, 256)
            },
            'hand_detector': {
                'module': 'qai_hub_models.models.mediapipe_hand',
                'component': 'hand_detector',
                'description': 'MediaPipe Hand Detector',
                'input_size': (224, 224)
            }
        }
        
        for model_name, config in model_configs.items():
            try:
                logger.info(f"ğŸ“± è¼‰å…¥ {config['description']}...")
                
                # å‹•æ…‹å°å…¥æ¨¡å‹
                module = __import__(config['module'], fromlist=['Model'])
                ModelClass = getattr(module, 'Model')
                
                # å‰µå»ºé è¨“ç·´æ¨¡å‹
                full_model = ModelClass.from_pretrained()
                
                # æå–æŒ‡å®šçµ„ä»¶
                if config['component'] == 'face_detector':
                    component = full_model.face_detector
                elif config['component'] == 'face_landmark_detector':
                    component = full_model.face_landmark_detector
                elif config['component'] == 'pose_detector':
                    # å°æ–¼poseæ¨¡å‹ï¼Œä½¿ç”¨æ•´å€‹æ¨¡å‹
                    component = full_model
                elif config['component'] == 'hand_detector':
                    # å°æ–¼handæ¨¡å‹ï¼Œä½¿ç”¨æ•´å€‹æ¨¡å‹
                    component = full_model
                else:
                    component = full_model
                
                self.mediapipe_models[model_name] = {
                    'component': component,
                    'config': config,
                    'loaded': True
                }
                
                logger.info(f"âœ… {config['description']} è¼‰å…¥æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} è¼‰å…¥å¤±æ•—: {e}")
                self.mediapipe_models[model_name] = {
                    'component': None,
                    'config': config,
                    'loaded': False,
                    'error': str(e)
                }
    
    def convert_to_torchscript(self):
        """å°‡æ¨¡å‹çµ„ä»¶è½‰æ›ç‚ºTorchScript"""
        logger.info("ğŸ“¤ è½‰æ›æ¨¡å‹çµ„ä»¶ç‚ºTorchScript...")
        
        for model_name, model_info in self.mediapipe_models.items():
            if not model_info['loaded']:
                continue
                
            try:
                component = model_info['component']
                config = model_info['config']
                
                logger.info(f"ğŸ”„ è½‰æ› {config['description']} ç‚ºTorchScript...")
                
                # è¨­ç½®è©•ä¼°æ¨¡å¼
                component.eval()
                
                # æº–å‚™ç¤ºä¾‹è¼¸å…¥
                input_size = config['input_size']
                sample_input = torch.randn(1, 3, input_size[1], input_size[0])
                
                # è½‰æ›ç‚ºTorchScript
                with torch.no_grad():
                    if hasattr(component, 'convert_to_torchscript'):
                        # ä½¿ç”¨QAI Hub Modelsçš„è½‰æ›æ–¹æ³•
                        torchscript_model = component.convert_to_torchscript(sample_inputs=[sample_input])
                    else:
                        # ä½¿ç”¨æ¨™æº–PyTorchè½‰æ›
                        torchscript_model = torch.jit.trace(component, sample_input)
                
                # ä¿å­˜TorchScriptæ¨¡å‹
                torchscript_path = f"qai_hub_{model_name}_torchscript.pt"
                torchscript_model.save(torchscript_path)
                
                logger.info(f"âœ… {config['description']} TorchScriptå·²ä¿å­˜: {torchscript_path}")
                
                # æ›´æ–°æ¨¡å‹ä¿¡æ¯
                model_info['torchscript_model'] = torchscript_model
                model_info['torchscript_path'] = torchscript_path
                model_info['sample_input'] = sample_input
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} TorchScriptè½‰æ›å¤±æ•—: {e}")
                model_info['torchscript_error'] = str(e)
    
    def upload_to_qai_hub(self):
        """ä¸Šå‚³TorchScriptæ¨¡å‹åˆ°QAI Hub"""
        logger.info("â˜ï¸ ä¸Šå‚³TorchScriptæ¨¡å‹åˆ°QAI Hub...")
        
        for model_name, model_info in self.mediapipe_models.items():
            if not model_info.get('torchscript_path'):
                logger.warning(f"âš ï¸ {model_name} æ²’æœ‰TorchScriptæ¨¡å‹ï¼Œè·³éä¸Šå‚³")
                continue
                
            try:
                torchscript_path = model_info['torchscript_path']
                config = model_info['config']
                
                logger.info(f"ğŸ“¤ ä¸Šå‚³ {config['description']} åˆ°QAI Hub...")
                
                # ä¸Šå‚³æ¨¡å‹
                qai_model = hub.upload_model(torchscript_path)
                
                logger.info(f"âœ… {config['description']} ä¸Šå‚³æˆåŠŸ")
                logger.info(f"   æ¨¡å‹ID: {qai_model.model_id}")
                
                # ä¿å­˜QAI Hubæ¨¡å‹å¼•ç”¨
                self.qai_hub_models[model_name] = {
                    'qai_model': qai_model,
                    'model_id': qai_model.model_id,
                    'config': config
                }
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ä¸Šå‚³å¤±æ•—: {e}")
                model_info['upload_error'] = str(e)
    
    def submit_compile_jobs(self):
        """æäº¤ç·¨è­¯Jobsåˆ°QAI Hub"""
        logger.info("ğŸš€ æäº¤ç·¨è­¯Jobsåˆ°QAI Hub...")
        
        for model_name, qai_model_info in self.qai_hub_models.items():
            try:
                qai_model = qai_model_info['qai_model']
                config = qai_model_info['config']
                input_size = config['input_size']
                
                logger.info(f"ğŸ”„ æäº¤ {config['description']} ç·¨è­¯Job...")
                
                # è¨­ç½®è¼¸å…¥è¦æ ¼
                input_specs = {
                    "image": ((1, 3, input_size[1], input_size[0]), "float32")
                }
                
                # æäº¤ç·¨è­¯Job
                compile_job = hub.submit_compile_job(
                    model=qai_model,
                    input_specs=input_specs,
                    device=self.target_device
                )
                
                self.compile_jobs[model_name] = {
                    'job': compile_job,
                    'config': config
                }
                
                logger.info(f"âœ… {config['description']} ç·¨è­¯Jobå·²æäº¤")
                logger.info(f"   Job ID: {compile_job.job_id}")
                logger.info(f"   Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ç·¨è­¯Jobæäº¤å¤±æ•—: {e}")
    
    def convert_to_onnx(self):
        """è½‰æ›TorchScriptæ¨¡å‹ç‚ºONNX"""
        logger.info("ğŸ”„ è½‰æ›TorchScriptæ¨¡å‹ç‚ºONNX...")
        
        for model_name, model_info in self.mediapipe_models.items():
            if not model_info.get('torchscript_model'):
                continue
                
            try:
                config = model_info['config']
                torchscript_model = model_info['torchscript_model']
                sample_input = model_info['sample_input']
                
                logger.info(f"ğŸ”„ è½‰æ› {config['description']} ç‚ºONNX...")
                
                # å°å‡ºç‚ºONNX
                onnx_path = f"qai_hub_{model_name}_optimized.onnx"
                torch.onnx.export(
                    torchscript_model,
                    sample_input,
                    onnx_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['image'],
                    output_names=['output'],
                    dynamic_axes={
                        'image': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                logger.info(f"âœ… {config['description']} ONNXå·²ä¿å­˜: {onnx_path}")
                
                # è¼‰å…¥ONNX Runtimeæœƒè©±
                self._create_onnx_session(model_name, onnx_path, config)
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ONNXè½‰æ›å¤±æ•—: {e}")
    
    def _create_onnx_session(self, model_name: str, onnx_path: str, config: Dict):
        """å‰µå»ºONNX Runtimeæœƒè©±"""
        try:
            logger.info(f"ğŸ”„ å‰µå»º {model_name} ONNX Runtimeæœƒè©±...")
            
            # è¨­ç½®æœƒè©±é¸é …
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            sess_options.enable_cpu_mem_arena = True
            sess_options.enable_mem_pattern = True
            
            # å‰µå»ºæ¨ç†æœƒè©±
            session = ort.InferenceSession(
                onnx_path,
                sess_options=sess_options,
                providers=self.onnx_providers
            )
            
            self.onnx_sessions[model_name] = {
                'session': session,
                'config': config,
                'onnx_path': onnx_path
            }
            
            # ç²å–æœƒè©±ä¿¡æ¯
            input_info = session.get_inputs()[0]
            output_info = session.get_outputs()
            
            logger.info(f"âœ… {model_name} ONNXæœƒè©±å‰µå»ºæˆåŠŸ")
            logger.info(f"   è¼¸å…¥: {input_info.name} {input_info.shape} {input_info.type}")
            logger.info(f"   è¼¸å‡ºæ•¸é‡: {len(output_info)}")
            
        except Exception as e:
            logger.error(f"âŒ {model_name} ONNXæœƒè©±å‰µå»ºå¤±æ•—: {e}")
    
    def detect_with_onnx(self, image: np.ndarray, model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨ONNX RuntimeåŸ·è¡Œæª¢æ¸¬"""
        if model_name not in self.onnx_sessions:
            return {"error": f"ONNXæœƒè©± {model_name} ä¸å­˜åœ¨"}
        
        try:
            session_info = self.onnx_sessions[model_name]
            session = session_info['session']
            config = session_info['config']
            
            # é è™•ç†åœ–åƒ
            processed_image = self._preprocess_image(image, config['input_size'])
            
            # åŸ·è¡Œæ¨ç†
            input_name = session.get_inputs()[0].name
            start_time = time.time()
            outputs = session.run(None, {input_name: processed_image})
            inference_time = (time.time() - start_time) * 1000
            
            # å¾Œè™•ç†çµæœ
            results = self._postprocess_outputs(outputs, model_name, config)
            results.update({
                'inference_time_ms': round(inference_time, 2),
                'model_type': f"QAI_Hub_{model_name}_ONNX",
                'description': config['description'],
                'success': True
            })
            
            logger.info(f"âš¡ {model_name}æª¢æ¸¬å®Œæˆ: {inference_time:.2f}ms")
            return results
            
        except Exception as e:
            logger.error(f"âŒ {model_name}æª¢æ¸¬å¤±æ•—: {e}")
            return {
                "error": str(e),
                "success": False,
                "model_name": model_name
            }
    
    def _preprocess_image(self, image: np.ndarray, input_size: tuple) -> np.ndarray:
        """é è™•ç†åœ–åƒ"""
        # èª¿æ•´åœ–åƒå¤§å°
        resized = cv2.resize(image, input_size)
        
        # è½‰æ›é¡è‰²ç©ºé–“ BGR -> RGB
        if len(resized.shape) == 3:
            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # æ­£è¦åŒ–åˆ° [0, 1]
        normalized = resized.astype(np.float32) / 255.0
        
        # èª¿æ•´ç¶­åº¦é †åº HWC -> CHW ä¸¦æ·»åŠ batchç¶­åº¦
        preprocessed = np.transpose(normalized, (2, 0, 1))
        preprocessed = np.expand_dims(preprocessed, axis=0)
        
        return preprocessed
    
    def _postprocess_outputs(self, outputs: list, model_name: str, config: Dict) -> Dict[str, Any]:
        """å¾Œè™•ç†æ¨¡å‹è¼¸å‡º"""
        results = {
            "detections": [],
            "output_shapes": [output.shape for output in outputs],
            "num_outputs": len(outputs)
        }
        
        try:
            # æ ¹æ“šæ¨¡å‹é¡å‹è™•ç†
            if 'face' in model_name:
                results = self._process_face_detection(outputs, results)
            elif 'pose' in model_name:
                results = self._process_pose_detection(outputs, results)
            elif 'hand' in model_name:
                results = self._process_hand_detection(outputs, results)
            
        except Exception as e:
            logger.error(f"âŒ {model_name}è¼¸å‡ºå¾Œè™•ç†å¤±æ•—: {e}")
            results["postprocess_error"] = str(e)
        
        return results
    
    def _process_face_detection(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†äººè‡‰æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) >= 2:
            # å‡è¨­ç¬¬ä¸€å€‹è¼¸å‡ºæ˜¯é‚Šç•Œæ¡†ï¼Œç¬¬äºŒå€‹æ˜¯ç½®ä¿¡åº¦
            detections = []
            confidence_threshold = 0.5
            
            # ç°¡åŒ–è™•ç† - å¯¦éš›éœ€è¦æ ¹æ“šå…·é«”æ¨¡å‹è¼¸å‡ºæ ¼å¼èª¿æ•´
            try:
                boxes_output = outputs[0]
                scores_output = outputs[1] if len(outputs) > 1 else None
                
                if scores_output is not None:
                    # éæ¿¾é«˜ç½®ä¿¡åº¦æª¢æ¸¬
                    high_conf_indices = np.where(scores_output > confidence_threshold)[0]
                    detections = [{
                        "type": "face",
                        "confidence": float(scores_output.flat[i]),
                        "detection_index": int(i)
                    } for i in high_conf_indices[:5]]  # æœ€å¤š5å€‹æª¢æ¸¬
                
                results["detections"] = detections
                results["total_faces"] = len(detections)
                
            except Exception as e:
                results["detection_parsing_error"] = str(e)
        
        return results
    
    def _process_pose_detection(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†å§¿æ…‹æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) >= 1:
            # å§¿æ…‹é—œéµé»è™•ç†
            keypoints_output = outputs[0]
            
            try:
                # ç°¡åŒ–è™•ç† - æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å§¿æ…‹æª¢æ¸¬
                if keypoints_output.size > 0:
                    results["detections"] = [{
                        "type": "pose",
                        "keypoints_shape": keypoints_output.shape,
                        "has_detection": True
                    }]
                    results["total_poses"] = 1
                else:
                    results["total_poses"] = 0
                    
            except Exception as e:
                results["pose_parsing_error"] = str(e)
        
        return results
    
    def _process_hand_detection(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†æ‰‹éƒ¨æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) >= 1:
            # æ‰‹éƒ¨é—œéµé»è™•ç†
            landmarks_output = outputs[0]
            
            try:
                if landmarks_output.size > 0:
                    results["detections"] = [{
                        "type": "hand",
                        "landmarks_shape": landmarks_output.shape,
                        "has_detection": True
                    }]
                    results["total_hands"] = 1
                else:
                    results["total_hands"] = 0
                    
            except Exception as e:
                results["hand_parsing_error"] = str(e)
        
        return results
    
    def run_unified_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """é‹è¡Œçµ±ä¸€æª¢æ¸¬"""
        unified_results = {
            "timestamp": time.time(),
            "image_shape": image.shape,
            "detections": {},
            "performance": {},
            "summary": {}
        }
        
        total_faces = 0
        total_poses = 0
        total_hands = 0
        total_inference_time = 0
        
        # åŸ·è¡Œæ‰€æœ‰å¯ç”¨çš„æª¢æ¸¬
        for model_name in self.onnx_sessions.keys():
            detection_result = self.detect_with_onnx(image, model_name)
            unified_results["detections"][model_name] = detection_result
            
            # æ”¶é›†æ€§èƒ½æŒ‡æ¨™
            if "inference_time_ms" in detection_result:
                unified_results["performance"][f"{model_name}_ms"] = detection_result["inference_time_ms"]
                total_inference_time += detection_result["inference_time_ms"]
            
            # çµ±è¨ˆæª¢æ¸¬æ•¸é‡
            if "total_faces" in detection_result:
                total_faces += detection_result["total_faces"]
            elif "total_poses" in detection_result:
                total_poses += detection_result["total_poses"]
            elif "total_hands" in detection_result:
                total_hands += detection_result["total_hands"]
        
        # ç”Ÿæˆæ‘˜è¦
        unified_results["summary"] = {
            "total_faces": total_faces,
            "total_poses": total_poses,
            "total_hands": total_hands,
            "total_inference_time_ms": round(total_inference_time, 2),
            "models_used": list(self.onnx_sessions.keys())
        }
        
        return unified_results
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            "mediapipe_models": {
                name: {
                    "loaded": info["loaded"],
                    "description": info["config"]["description"],
                    "has_torchscript": "torchscript_path" in info,
                    "error": info.get("error")
                }
                for name, info in self.mediapipe_models.items()
            },
            "qai_hub_models": {
                name: {
                    "model_id": info["model_id"],
                    "description": info["config"]["description"]
                }
                for name, info in self.qai_hub_models.items()
            },
            "compile_jobs": {
                name: {
                    "job_id": info["job"]["job_id"],
                    "description": info["config"]["description"],
                    "dashboard": f"https://aihub.qualcomm.com/jobs/{info['job'].job_id}"
                }
                for name, info in self.compile_jobs.items()
            },
            "onnx_sessions": list(self.onnx_sessions.keys()),
            "target_device": self.target_device.name if self.target_device else None,
            "onnx_providers": self.onnx_providers
        }

def main():
    """ä¸»å‡½æ•¸ï¼šæ¼”ç¤ºå®Œæ•´QAI Hub + ONNXç³»çµ±"""
    print("ğŸš€ å®Œæ•´QAI Hub + ONNX Runtimeé›†æˆç³»çµ±æ¼”ç¤º")
    print("=" * 70)
    
    try:
        # åˆå§‹åŒ–ç³»çµ±
        system = QAIHubONNXComplete()
        
        # Step 1: è¼‰å…¥MediaPipeæ¨¡å‹
        print("\nğŸ“¥ Step 1: è¼‰å…¥MediaPipeæ¨¡å‹çµ„ä»¶...")
        system.load_mediapipe_models()
        
        # Step 2: è½‰æ›ç‚ºTorchScript
        print("\nğŸ“¤ Step 2: è½‰æ›ç‚ºTorchScript...")
        system.convert_to_torchscript()
        
        # Step 3: ä¸Šå‚³åˆ°QAI Hub
        print("\nâ˜ï¸ Step 3: ä¸Šå‚³åˆ°QAI Hub...")
        system.upload_to_qai_hub()
        
        # Step 4: æäº¤ç·¨è­¯Jobs
        print("\nğŸš€ Step 4: æäº¤ç·¨è­¯Jobs...")
        system.submit_compile_jobs()
        
        # Step 5: è½‰æ›ç‚ºONNX
        print("\nğŸ”„ Step 5: è½‰æ›ç‚ºONNX...")
        system.convert_to_onnx()
        
        # Step 6: æ¸¬è©¦æª¢æ¸¬
        if system.onnx_sessions:
            print("\nğŸ§ª Step 6: æ¸¬è©¦ONNXæª¢æ¸¬...")
            
            test_images = ['andy.jpg', 'official_test_image.jpg']
            for img_path in test_images:
                if os.path.exists(img_path):
                    print(f"\nğŸ“· æ¸¬è©¦åœ–åƒ: {img_path}")
                    image = cv2.imread(img_path)
                    
                    if image is not None:
                        # åŸ·è¡Œçµ±ä¸€æª¢æ¸¬
                        results = system.run_unified_detection(image)
                        
                        print(f"   æª¢æ¸¬æ‘˜è¦: {results['summary']}")
                        print(f"   æ€§èƒ½æŒ‡æ¨™: {results['performance']}")
        
        # ç²å–ç³»çµ±ç‹€æ…‹
        print("\nğŸ“Š ç³»çµ±ç‹€æ…‹å ±å‘Š:")
        status = system.get_system_status()
        
        print(f"   MediaPipeæ¨¡å‹: {len(status['mediapipe_models'])}")
        print(f"   QAI Hubæ¨¡å‹: {len(status['qai_hub_models'])}")
        print(f"   ç·¨è­¯Jobs: {len(status['compile_jobs'])}")
        print(f"   ONNXæœƒè©±: {len(status['onnx_sessions'])}")
        print(f"   ç›®æ¨™è¨­å‚™: {status['target_device']}")
        
        # QAI Hub Jobsè©³æƒ…
        if status['compile_jobs']:
            print("\nğŸ“‹ QAI Hubç·¨è­¯Jobs:")
            for model_name, job_info in status['compile_jobs'].items():
                print(f"   {model_name}:")
                print(f"     Job ID: {job_info['job_id']}")
                print(f"     Dashboard: {job_info['dashboard']}")
        
        # ä¿å­˜è©³ç´°çµæœ
        results_file = 'complete_qai_hub_onnx_results.json'
        with open(results_file, 'w') as f:
            json.dump(status, f, indent=2, default=str)
        
        print(f"\nâœ… å®Œæ•´æ¼”ç¤ºå®Œæˆï¼è©³ç´°çµæœå·²ä¿å­˜åˆ° {results_file}")
        print(f"ğŸ¯ çœŸæ­£çš„QAI Hub + ONNX Runtimeé›†æˆç³»çµ±é‹è¡ŒæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
