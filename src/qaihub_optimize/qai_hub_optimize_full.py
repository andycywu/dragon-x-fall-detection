#!/usr/bin/env python3
"""
QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…· (æ¨¡çµ„åŒ–ç‰ˆæœ¬)
æ”¯æ´ Compileã€Profileã€Inferã€Demoã€Test ç­‰å¤šæ¨¡å¼å…¥å£
ä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹ï¼Œæé«˜ç¨‹å¼ç¢¼å¯ç¶­è­·æ€§å’Œé‡ç”¨æ€§
"""
import argparse
import sys
import os
import time
from pathlib import Path

# åŒ¯å…¥æ¨¡çµ„åŒ–çµ„ä»¶
from modules.scanner import ModelScanner
from modules.conversion import ModelConverter
from modules.format_check import FormatChecker
from modules.qaihub_client import QAIHubClient
from modules.pipeline import QAIHubPipeline
from modules.job_monitor import QAIHubJobMonitor

# åŒ¯å…¥å…¶ä»–å¿…è¦çš„åŠŸèƒ½æ¨¡çµ„
from practical_qai_hub_onnx import PracticalQAIHubONNX
from final_qai_hub_onnx_system import FinalQAIHubONNXSystem
from qai_hub_unified_detector import QAIHubUnifiedDetector, demo_qai_hub_detection, test_live_detection
from official_qai_hub_detector import OfficialQAIHubDetector, demo_official_qai_hub_detection


def get_models_dir():
    """å–å¾— models ç›®éŒ„è·¯å¾‘"""
    return Path(__file__).parent.parent / 'models'


def run_compile(source='dlc'):
    """ç·¨è­¯ä¸¦æœ€ä½³åŒ–æ¨¡å‹ï¼ˆè‡ªå‹•æƒæ org ç›®éŒ„ï¼‰"""
    print(f"\n[Compile] QAI Hub Compile Pipeline (Practical) | source={source}")
    
    # åˆå§‹åŒ–æ¨¡çµ„
    scanner = ModelScanner()
    converter = ModelConverter()
    format_checker = FormatChecker()
    qaihub_client = QAIHubClient()
    pipeline = QAIHubPipeline()
    
    # æƒæ org ç›®éŒ„
    model_files = scanner.scan_org_directory()
    counts = scanner.get_model_counts(model_files)
    
    if counts['total'] == 0:
        print("âŒ org ç›®éŒ„ä¸‹æ²’æœ‰å¯ç”¨æ¨¡å‹æª”æ¡ˆï¼")
        return
    
    print(f"\nå…±åµæ¸¬åˆ°ï¼š{counts['tflite']} å€‹ tflite, {counts['onnx']} å€‹ onnx, {counts['dlc']} å€‹ dlc æª”æ¡ˆ")
    
    # è‡ªå‹•è½‰æ› TFLite åˆ° ONNX
    if counts['tflite'] > 0:
        tflite_files = model_files['tflite']
        results, failed_conversions = converter.batch_convert_tflite(tflite_files, get_models_dir() / 'onnx')
        
        successful_conversions = [r for r in results if r["status"] == "ok"]
        if successful_conversions:
            print(f"âœ… æ‰¹æ¬¡è½‰æ›å®Œæˆï¼Œ{len(successful_conversions)} å€‹ onnx æª”æ¡ˆå·²å­˜å…¥ {get_models_dir() / 'onnx'}")
            # è½‰æ›å®Œè‡ªå‹•åˆ‡æ› source ç‚º onnx
            source = 'onnx'
        if failed_conversions:
            print(f"âš ï¸  æœ‰ {len(failed_conversions)} å€‹æª”æ¡ˆè½‰æ›å¤±æ•—")
    
    # æ ¹æ“š source æ±ºå®šç›®éŒ„èˆ‡æ ¼å¼
    source_map = {
        'onnx':      ('onnx', '.onnx'),
        'tflite':    ('org-tflite', '.tflite'),
        'dlc':       ('org-dlc', '.dlc'),
        'org-onnx':  ('onnx', '.onnx'),
        'org-tflite':('org-tflite', '.tflite'),
        'org-dlc':   ('org-dlc', '.dlc'),
        'original':  ('original', '.tflite'),
    }
    
    if source not in source_map:
        print(f"âŒ ä¸æ”¯æ´çš„ source: {source}")
        return
    
    model_dir, ext = source_map[source]
    
    # è‹¥æ˜¯ onnxï¼Œå…ˆè‡ªå‹•ä¿®æ­£æ‰€æœ‰ onnx æª”æ¡ˆ
    if ext == '.onnx':
        onnx_dir = get_models_dir() / model_dir
        format_checker.batch_fix_onnx_value_info(onnx_dir)
    
    # ä½¿ç”¨ pipeline é€²è¡Œå®Œæ•´çš„ç·¨è­¯æµç¨‹
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models(source=source, model_dir=model_dir, ext=ext)
    
    # ONNX æª¢æŸ¥
    if ext == '.onnx':
        invalid_models = format_checker.check_onnx_models(system.qai_hub_models)
        if invalid_models:
            print("\nâŒ åµæ¸¬åˆ°ä¸‹åˆ— ONNX æ¨¡å‹æª”æ¡ˆæ ¼å¼ç•°å¸¸ï¼Œè«‹ä¿®æ­£å¾Œå†åŸ·è¡Œï¼š")
            for name, path, err in invalid_models:
                print(f"   - {name}: {path}\n     éŒ¯èª¤: {err}")
            print("\næµç¨‹ä¸­æ­¢ã€‚")
            return
    
    # åˆ—å‡ºè¦è™•ç†çš„æ¨¡å‹
    models_to_process = [k for k, v in system.qai_hub_models.items() if v.get('loaded')]
    print(f"\nğŸ” åµæ¸¬åˆ° {len(models_to_process)} å€‹æ¨¡å‹å°‡é€²è¡Œ QAI Hub æœ€ä½³åŒ–ï¼š")
    for m in models_to_process:
        print(f"   - {m}")
    
    # æç¤º QAI Hub ä½œæ¥­æµç¨‹
    print("""
ğŸ“‹ QAI Hub é›²ç«¯æœ€ä½³åŒ–ä½œæ¥­æµç¨‹ï¼š
1. ä¸Šå‚³æ¨¡å‹ï¼ˆUpload Modelï¼‰
2. æäº¤ç·¨è­¯ä»»å‹™ï¼ˆSubmit Compile Jobï¼‰
3. ç­‰å¾…é›²ç«¯å®Œæˆæœ€ä½³åŒ–ï¼ˆJob Queue & Compileï¼‰
4. ä¸‹è¼‰æœ€ä½³åŒ–æ¨¡å‹ï¼ˆDownloadï¼‰
5. å¯é€²è¡Œ Profileã€Inferã€Demo ç­‰å¾ŒçºŒæ¸¬è©¦
""")
    
    # åŸ·è¡Œç·¨è­¯æµç¨‹
    system.upload_models_to_qai_hub()
    system.submit_compilation_jobs()
    
    # ä½¿ç”¨ JobMonitor ç­‰å¾…æ‰€æœ‰ compile job å®Œæˆ
    job_monitor = QAIHubJobMonitor()
    job_monitor.wait_for_compile_jobs(system.qai_hub_models, timeout_minutes=30)
    
    # ç”¢ç”Ÿ HTML å ±å‘Š
    job_monitor.generate_compile_report(system.qai_hub_models, 'practical_qai_hub_compile_report.html')
    print(f"\nâœ… Compile å®Œæˆï¼HTML å ±å‘Šå·²ç”¢ç”Ÿ practical_qai_hub_compile_report.html")


def run_profile():
    """é€²è¡Œæ¨¡å‹æ•ˆèƒ½åˆ†æï¼ˆè‡ªå‹•æƒæ org ç›®éŒ„ï¼‰"""
    print("\n[Profile] QAI Hub Profile Pipeline (Practical)")
    
    # åˆå§‹åŒ–æ¨¡çµ„
    scanner = ModelScanner()
    converter = ModelConverter()
    qaihub_client = QAIHubClient()
    job_monitor = QAIHubJobMonitor(qaihub_client)
    
    # æƒæ org ç›®éŒ„
    model_files = scanner.scan_org_directory()
    counts = scanner.get_model_counts(model_files)
    
    if counts['total'] == 0:
        print("âŒ org ç›®éŒ„ä¸‹æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆï¼Œç„¡æ³•é€²è¡Œ profileã€‚")
        return
    
    # åˆä½µæ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
    all_model_files = model_files['tflite'] + model_files['onnx'] + model_files['dlc']
    
    print(f"ğŸ” åµæ¸¬åˆ° {counts['total']} å€‹æ¨¡å‹æª”æ¡ˆï¼š")
    for file_list in [model_files['tflite'], model_files['onnx'], model_files['dlc']]:
        for f in file_list:
            print(f"   - {f.name}")
    
    # å•Ÿå‹•ç³»çµ±ï¼Œå…ˆå–å¾—è£ç½®æ”¯æ´æ ¼å¼
    system = PracticalQAIHubONNX()
    device = system.target_device
    if not device:
        print("âŒ ç„¡æ³•å–å¾—ç›®æ¨™è£ç½®ï¼Œç„¡æ³•é€²è¡Œ profileã€‚")
        return
    
    # æª¢æŸ¥è£ç½®æ”¯æ´çš„æ¡†æ¶
    device_attrs = getattr(device, 'attributes', [])
    support_onnx = any('framework:onnx' in a for a in device_attrs)
    support_tflite = any('framework:tflite' in a for a in device_attrs)
    support_dlc = any('framework:dlc' in a for a in device_attrs)
    
    print(f"\n[è£ç½®æ”¯æ´æ ¼å¼] ONNX={support_onnx}, TFLite={support_tflite}, DLC={support_dlc}")
    
    # è™•ç† TFLite è½‰æ›
    tflite_models = model_files['tflite']
    convert_tflite = False
    
    if tflite_models and not support_tflite:
        convert_tflite = converter.ask_for_conversion(len(tflite_models))
    
    # ç¯©é¸å¯ profile çš„æ¨¡å‹
    models_to_profile = []
    for f in all_model_files:
        ext = f.suffix.lower()
        if ext == '.onnx' and not support_onnx:
            print(f"â­ï¸ è·³é {f.name}ï¼Œå› ç›®æ¨™è£ç½®ä¸æ”¯æ´ ONNX")
            continue
        if ext == '.dlc' and not support_dlc:
            print(f"â­ï¸ è·³é {f.name}ï¼Œå› ç›®æ¨™è£ç½®ä¸æ”¯æ´ DLC")
            continue
        if ext == '.tflite':
            if support_tflite:
                models_to_profile.append(f)
            elif convert_tflite:
                # å˜—è©¦è½‰æ›
                result = converter.convert_tflite_to_onnx(f, get_models_dir() / 'onnx')
                if result["status"] == "ok" and result["onnx_path"]:
                    models_to_profile.append(Path(result["onnx_path"]))
                    print(f"âœ… è½‰æ›æˆåŠŸ: {f.name} -> {result['onnx_path'].name}")
                else:
                    print(f"âŒ è½‰æ›å¤±æ•—: {f.name} - {result['message']}")
            else:
                print(f"â­ï¸ è·³é {f.name}ï¼Œå› ç›®æ¨™è£ç½®ä¸æ”¯æ´ TFLite ä¸”æœªé¸æ“‡è‡ªå‹•è½‰æ›")
        else:
            models_to_profile.append(f)
    
    if not models_to_profile:
        print("âŒ ç„¡å¯ profile çš„æ¨¡å‹ã€‚æµç¨‹çµæŸã€‚")
        return
    
    print(f"\nâœ… æœ€çµ‚å°‡é€²è¡Œ profile çš„æ¨¡å‹ï¼š")
    for f in models_to_profile:
        print(f"   - {f.name}")
    
    # è¼‰å…¥ã€ä¸Šå‚³ã€profile
    for ext, source in [('.onnx', 'onnx'), ('.tflite', 'tflite'), ('.dlc', 'dlc')]:
        files = [f for f in models_to_profile if f.suffix.lower() == ext]
        if files:
            system.load_mediapipe_models(source=source, model_dir='org', ext=ext)
    
    system.upload_models_to_qai_hub()
    system.submit_profile_jobs()
    
    # ä½¿ç”¨ JobMonitor ç­‰å¾…æ‰€æœ‰ profile job å®Œæˆ
    job_monitor.wait_for_profile_jobs(system.qai_hub_models, timeout_minutes=30)
    
    # ç”¢ç”Ÿ HTML å ±å‘Š
    job_monitor.generate_profile_report(system.qai_hub_models, 'practical_qai_hub_profile_report.html')
    print(f"\nâœ… Profile å®Œæˆï¼HTML å ±å‘Šå·²ç”¢ç”Ÿ practical_qai_hub_profile_report.html")


def run_infer():
    """é€²è¡Œæ¨¡å‹æ¨è«–æ¸¬è©¦"""
    print("\n[Infer] QAI Hub Inference Demo (Practical)")
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models()
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æ¨¡å‹æ¨è«–ï¼Œç„¡è‡ªå‹•è½‰æ›)")
    print("\nInfer å®Œæˆï¼")


def run_demo():
    """å•Ÿå‹•äº’å‹•å¼ Demo"""
    print("\n[Demo] QAI Hub Unified Detection Demo")
    demo_qai_hub_detection()
    print("\nDemo å®Œæˆï¼")


def run_official():
    """å®˜æ–¹æ¨¡å¼ï¼ˆç‰¹æ®Šç”¨é€”ï¼‰"""
    print("\n[Official] å®˜æ–¹ QAI Hub Detector Demo")
    demo_official_qai_hub_detection()
    print("\nOfficial Demo å®Œæˆï¼")


def run_test():
    """åŸ·è¡Œæ¸¬è©¦æµç¨‹"""
    print("\n[Test] QAI Hub Unified Detector æ¸¬è©¦ (Live)")
    test_live_detection()
    print("\nTest å®Œæˆï¼")


def run_compile_profile_jobs(source='dlc'):
    """æ‰¹æ¬¡ç·¨è­¯ä¸¦åˆ†æå¤šæ¨¡å‹ï¼ˆè‡ªå‹•è™•ç†å¤šå€‹æ¨¡å‹ï¼‰"""
    print(f"\n[Compile+Profile] QAI Hub Compile+Profile Pipeline (Full)")
    
    # ä½¿ç”¨ pipeline æ¨¡çµ„åŸ·è¡Œå®Œæ•´çš„ç·¨è­¯+åˆ†ææµç¨‹ï¼Œå‚³éæ­£ç¢ºçš„åŸºç¤ç›®éŒ„
    pipeline = QAIHubPipeline(get_models_dir())
    pipeline.run_compile_profile_pipeline(do_infer=False)
    print("\nCompile+Profile Jobs å…¨éƒ¨å®Œæˆï¼")


def run_compile_profile(source='dlc'):
    """å–®ä¸€æ¨¡å‹ç·¨è­¯èˆ‡åˆ†æï¼ˆé‡å°å–®ä¸€æ¨¡å‹ï¼‰"""
    print(f"\n[Compile+Profile+Infer] QAI Hub Compile+Profile+Infer Pipeline (Full Run)")
    
    # ä½¿ç”¨ pipeline æ¨¡çµ„åŸ·è¡Œå®Œæ•´çš„ç·¨è­¯+åˆ†æ+æ¨è«–æµç¨‹ï¼Œå‚³éæ­£ç¢ºçš„åŸºç¤ç›®éŒ„
    pipeline = QAIHubPipeline(get_models_dir())
    pipeline.run_compile_profile_pipeline(do_infer=True)
    print("\nCompile+Profile+Infer å®Œæˆï¼")


def run_link(source='dlc'):
    """Link Jobï¼ˆé€²éšç”¨é€”ï¼‰"""
    print(f"\n[Link] QAI Hub Link Job Pipeline | source={source}")
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æª”æ¡ˆï¼Œä¸åšè‡ªå‹•è½‰æ›)")
    print("\nLink Job å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(
        description=(
            """
    QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…· (æ¨¡çµ„åŒ–ç‰ˆæœ¬)

    ç”¨æ³• (Usage):
        python qai_hub_optimize_full.py <mode>

    å¯ç”¨å­å‘½ä»¤ (mode)ï¼š
        compile                ç·¨è­¯ä¸¦æœ€ä½³åŒ–æ¨¡å‹ï¼ˆè‡ªå‹•æƒæ org ç›®éŒ„ï¼‰
        profile                é€²è¡Œæ¨¡å‹æ•ˆèƒ½åˆ†æï¼ˆè‡ªå‹•æƒæ org ç›®éŒ„ï¼‰
        compile_profile_jobs   æ‰¹æ¬¡ç·¨è­¯ä¸¦åˆ†æå¤šæ¨¡å‹ï¼ˆè‡ªå‹•è™•ç†å¤šå€‹æ¨¡å‹ï¼‰ï¼ˆè‡ªå‹•æƒæ org ç›®éŒ„ï¼‰
        compile_profile        å–®ä¸€æ¨¡å‹ç·¨è­¯èˆ‡åˆ†æï¼ˆé‡å°å–®ä¸€æ¨¡å‹ï¼ˆè‡ªå‹•æƒæ org ç›®éŒ„ï¼‰ï¼‰
        infer                  é€²è¡Œæ¨¡å‹æ¨è«–æ¸¬è©¦
        demo                   å•Ÿå‹•äº’å‹•å¼ Demo
        official               å®˜æ–¹æ¨¡å¼ï¼ˆç‰¹æ®Šç”¨é€”ï¼‰
        test                   åŸ·è¡Œæ¸¬è©¦æµç¨‹
        link                   Link Jobï¼ˆé€²éšç”¨é€”ï¼‰

    ç¯„ä¾‹ (Examples):
        python qai_hub_optimize_full.py compile
        python qai_hub_optimize_full.py profile

    èªªæ˜ï¼š
        - æ‰€æœ‰æ¨¡å‹ä¾†æºå·²çµ±ä¸€è‡ªå‹•å¾ org ç›®éŒ„å–å¾—ï¼Œç„¡éœ€æ‰‹å‹•åˆ‡æ›ä¾†æºåƒæ•¸ã€‚
        - å„æ¨¡å¼ç´°ç¯€è«‹åƒè€ƒ doc/QAI_HUB_README.mdã€‚
    """
        ),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('mode', choices=['compile', 'profile', 'infer', 'demo', 'official', 'test', 'compile_profile_jobs', 'compile_profile', 'link'],
                        help="å­å‘½ä»¤: compile | profile | infer | demo | official | test | compile_profile_jobs | compile_profile | link")
    args = parser.parse_args()

    if args.mode == 'compile':
        run_compile()
    elif args.mode == 'profile':
        run_profile()
    elif args.mode == 'infer':
        run_infer()
    elif args.mode == 'demo':
        run_demo()
    elif args.mode == 'official':
        run_official()
    elif args.mode == 'test':
        run_test()
    elif args.mode == 'compile_profile_jobs':
        run_compile_profile_jobs()
    elif args.mode == 'compile_profile':
        run_compile_profile()
    elif args.mode == 'link':
        run_link()
    else:
        print("æœªçŸ¥æ¨¡å¼ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()
