#!/usr/bin/env python3
"""
QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…·
æ”¯æ´ Compileã€Profileã€Inferã€Demoã€Test ç­‰å¤šæ¨¡å¼å…¥å£
"""
import argparse
import sys
import os

# åŒ¯å…¥å„åŠŸèƒ½æ¨¡çµ„ï¼ˆå‡è¨­éƒ½åœ¨åŒç›®éŒ„æˆ– PYTHONPATH ä¸‹ï¼‰
from practical_qai_hub_onnx import PracticalQAIHubONNX
from final_qai_hub_onnx_system import FinalQAIHubONNXSystem
from qai_hub_unified_detector import QAIHubUnifiedDetector, demo_qai_hub_detection, test_live_detection
from official_qai_hub_detector import OfficialQAIHubDetector, demo_official_qai_hub_detection


def run_compile(source='onnx'):
    print(f"\n[Compile] QAI Hub Compile Pipeline (Practical) | source={source}")
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models(source=source)
    # ONNX æª¢æŸ¥
    if source == 'onnx':
        invalid = system.check_onnx_models()
        if invalid:
            print("\nâŒ åµæ¸¬åˆ°ä¸‹åˆ— ONNX æ¨¡å‹æª”æ¡ˆæ ¼å¼ç•°å¸¸ï¼Œè«‹ä¿®æ­£å¾Œå†åŸ·è¡Œï¼š")
            for name, path, err in invalid:
                print(f"   - {name}: {path}\n     éŒ¯èª¤: {err}")
            print("\nè«‹åƒè€ƒ onnx.checker.check_model('your_model.onnx', full_check=True) é€²è¡Œæœ¬åœ°æª¢æŸ¥ã€‚\næµç¨‹ä¸­æ­¢ã€‚")
            return
    # åˆ—å‡ºè¦è™•ç†çš„æ¨¡å‹
    models_to_process = [k for k, v in system.qai_hub_models.items() if v.get('loaded')]
    print(f"\nğŸ” åµæ¸¬åˆ° {len(models_to_process)} å€‹æ¨¡å‹å°‡é€²è¡Œ QAI Hub æœ€ä½³åŒ–ï¼š")
    for m in models_to_process:
        print(f"   - {m}")
    # æç¤º QAI Hub ä½œæ¥­æµç¨‹
    print("""
ğŸ“‹ QAI Hub é›²ç«¯æœ€ä½³åŒ–ä½œæ¥­æµç¨‹ï¼š
1. ä¸Šå‚³æ¨¡å‹ï¼ˆUpload Modelï¼‰
2. æäº¤ç·¨è­¯ä»»å‹™ï¼ˆSubmit Compile Jobï¼‰
3. ç­‰å¾…é›²ç«¯å®Œæˆæœ€ä½³åŒ–ï¼ˆJob Queue & Compileï¼‰
4. ä¸‹è¼‰æœ€ä½³åŒ–æ¨¡å‹ï¼ˆDownloadï¼‰
5. å¯é€²è¡Œ Profileã€Inferã€Demo ç­‰å¾ŒçºŒæ¸¬è©¦
\nè©³ç´°èªªæ˜èˆ‡ API ç¯„ä¾‹è«‹åƒè€ƒï¼š
https://app.aihub.qualcomm.com/docs/hub/index.html#examples
""")
    system.export_models_to_torchscript()
    system.upload_models_to_qai_hub()
    system.submit_compilation_jobs()
    # ç·¨è­¯çµæœå ±å‘Š
    compiled = [k for k, v in system.qai_hub_models.items() if v.get('compile_job')]
    failed = [k for k, v in system.qai_hub_models.items() if v.get('loaded') and not v.get('compile_job')]
    print(f"\nâœ… å·²æˆåŠŸæäº¤ {len(compiled)} å€‹æ¨¡å‹é€²è¡Œ QAI Hub ç·¨è­¯ï¼š")
    for m in compiled:
        print(f"   - {m}")
    if failed:
        print(f"\nâš ï¸ ä¸‹åˆ—æ¨¡å‹æœªèƒ½æˆåŠŸæäº¤ç·¨è­¯ï¼ˆå¯èƒ½ç¼ºæª”æˆ–ä¸Šå‚³å¤±æ•—ï¼‰ï¼š")
        for m in failed:
            print(f"   - {m}")
    print("\nCompile å®Œæˆï¼")

def run_profile():
    print("\n[Profile] QAI Hub Compile+Profile Pipeline (Final)")
    system = FinalQAIHubONNXSystem()
    system.load_mediapipe_components()
    system.convert_to_torchscript_and_upload()
    system.submit_qai_hub_jobs()
    print("\nProfile å®Œæˆï¼")

def run_infer():
    print("\n[Infer] QAI Hub ONNX Inference Demo (Practical)")
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models()
    system.export_models_to_torchscript()
    system.convert_torchscript_to_onnx()
    test_images = ['andy.jpg', 'official_test_image.jpg']
    for img_path in test_images:
        if os.path.exists(img_path):
            print(f"\nğŸ“· æ¸¬è©¦åœ–åƒ: {img_path}")
            import cv2
            image = cv2.imread(img_path)
            if image is not None:
                for model_name in system.onnx_sessions.keys():
                    result = system.detect_with_onnx(image, model_name)
                    print(f"   {model_name}: {result.get('inference_time_ms', 'N/A')}ms")
    print("\nInfer å®Œæˆï¼")

def run_demo():
    print("\n[Demo] QAI Hub Unified Detection Demo")
    demo_qai_hub_detection()
    print("\nDemo å®Œæˆï¼")

def run_official():
    print("\n[Official] å®˜æ–¹ QAI Hub Detector Demo")
    demo_official_qai_hub_detection()
    print("\nOfficial Demo å®Œæˆï¼")

def run_test():
    print("\n[Test] QAI Hub Unified Detector æ¸¬è©¦ (Live)")
    test_live_detection()
    print("\nTest å®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description="QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…·")
    parser.add_argument('mode', choices=['compile', 'profile', 'infer', 'demo', 'official', 'test'],
                        help="åŸ·è¡Œæ¨¡å¼: compile | profile | infer | demo | official | test")
    parser.add_argument('--source', choices=['onnx', 'original'], default='onnx',
                        help="æ¨¡å‹ä¾†æº: onnx (é è¨­) æˆ– original (tflite)")
    args = parser.parse_args()

    if args.mode == 'compile':
        run_compile(source=args.source)
    elif args.mode == 'profile':
        run_profile()
    elif args.mode == 'infer':
        run_infer()
    elif args.mode == 'demo':
        run_demo()
    elif args.mode == 'official':
        run_official()
    elif args.mode == 'test':
        run_test()
    else:
        print("æœªçŸ¥æ¨¡å¼ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main()
