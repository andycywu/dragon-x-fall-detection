#!/usr/bin/env python3
"""
QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…· (æ¨¡çµ„åŒ–ç‰ˆæœ¬)
æ”¯æ´ Compileã€Profileã€Inferã€Demoã€Test ç­‰å¤šæ¨¡å¼å…¥å£
ä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹ï¼Œæé«˜ç¨‹å¼ç¢¼å¯ç¶­è­·æ€§å’Œé‡ç”¨æ€§
"""
import argparse
import sys
import os
import time
from pathlib import Path
from dotenv import load_dotenv
import subprocess
import shutil
import re
import logging

# åŒ¯å…¥æ¨¡çµ„åŒ–çµ„ä»¶
from modules.scanner import ModelScanner
from modules.conversion import ModelConverter
from modules.advanced_conversion import AdvancedModelConverter, get_advanced_converter
from modules.format_check import FormatChecker
from modules.qaihub_client import QAIHubClient
from modules.pipeline import QAIHubPipeline
from modules.job_monitor import QAIHubJobMonitor, get_job_monitor

# åŒ¯å…¥å…¶ä»–å¿…è¦çš„åŠŸèƒ½æ¨¡çµ„
from practical_qai_hub_onnx import PracticalQAIHubONNX
from final_qai_hub_onnx_system import FinalQAIHubONNXSystem
from qai_hub_unified_detector import QAIHubUnifiedDetector, demo_qai_hub_detection, test_live_detection
from official_qai_hub_detector import OfficialQAIHubDetector, demo_official_qai_hub_detection

# åŠ è¼‰ .env é…ç½®
load_dotenv()

# ç²å–æ¨¡å‹ç›¸é—œç›®éŒ„
MODELS_BASE_DIR = os.getenv("MODELS_BASE_DIR")
ONNX_MODEL_DIR = os.path.join(MODELS_BASE_DIR, os.getenv("ONNX_MODEL_DIR"))
RAW_MODEL_DIR = os.path.join(MODELS_BASE_DIR, os.getenv("MODEL_SOURCE_DIR"))
OPTIMIZED_MODEL_DIR = os.path.join(MODELS_BASE_DIR, os.getenv("OPTIMIZED_MODEL_DIR"))


def get_models_dir():
    """å–å¾— models ç›®éŒ„è·¯å¾‘"""
    # å„ªå…ˆä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„è·¯å¾‘
    models_base_dir = os.getenv('MODELS_BASE_DIR')
    if models_base_dir:
        return Path(models_base_dir)
    # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç›¸å°è·¯å¾‘
    return Path(__file__).parent.parent / 'models'


# æ–°å¢æª¢æŸ¥ .env è®Šæ•¸æ˜¯å¦æ­£ç¢ºè¼‰å…¥
if not MODELS_BASE_DIR or not ONNX_MODEL_DIR or not RAW_MODEL_DIR or not OPTIMIZED_MODEL_DIR:
    print("âŒ .env é…ç½®éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ä»¥ä¸‹è®Šæ•¸æ˜¯å¦æ­£ç¢ºè¨­ç½®ï¼š")
    print("   - MODELS_BASE_DIR")
    print("   - ONNX_MODEL_DIR")
    print("   - MODEL_SOURCE_DIR")
    print("   - OPTIMIZED_MODEL_DIR")
    sys.exit(1)


def ensure_directory_exists(directory):
    """
    ç¢ºä¿ç›®éŒ„å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å»ºç«‹ã€‚

    Args:
        directory (str): ç›®éŒ„è·¯å¾‘ã€‚

    Returns:
        None
    """
    try:
        if not os.path.exists(directory):
            print(f"ğŸ“ ç›®éŒ„ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹: {directory}")
            os.makedirs(directory, exist_ok=True)
    except Exception as e:
        print(f"âŒ å»ºç«‹ç›®éŒ„å¤±æ•—: {directory}ï¼ŒéŒ¯èª¤: {e}")
        sys.exit(1)


def run_compile():
    """ç·¨è­¯ä¸¦æœ€ä½³åŒ–æ¨¡å‹ï¼ˆè‡ªå‹•æƒæ raw ç›®éŒ„ï¼‰"""
    print(f"\n[Compile] QAI Hub Compile Pipeline")
    
    # ä½¿ç”¨ QAIHubPipeline é€²è¡Œå®Œæ•´çš„ç·¨è­¯æµç¨‹
    pipeline = QAIHubPipeline(get_models_dir())
    
    
    # åŸ·è¡Œç·¨è­¯æµç¨‹ï¼Œä½¿ç”¨æ–°çš„ job monitor æœƒè‡ªå‹•è™•ç†ç‹€æ…‹ç›£æ§å’Œæ¨¡å‹ä¸‹è¼‰
    # è‡ªå‹•æƒæ raw ç›®éŒ„ä¸¦æ ¹æ“šç›®æ¨™è£ç½®æ”¯æ´æ±ºå®šä¾†æºé¡å‹
    # ä½¿ç”¨ source åƒæ•¸è€Œä¸æ˜¯ quantize
    success = pipeline.run_compile_pipeline(source='onnx')
    
    if success:
        # æª¢æŸ¥ä¸¦é¡¯ç¤ºä¸‹è¼‰çš„å„ªåŒ–æ¨¡å‹
        downloaded_models = []
        for model_name, model_info in pipeline.qaihub_client.qai_hub_models.items():
            if model_info.get('optimized_model_downloaded', False):
                downloaded_models.append(model_name)
        
        if downloaded_models:
            print(f"\nğŸ’¾ å„ªåŒ–æ¨¡å‹å·²ä¸‹è¼‰åˆ° src/models/qaihub_optimized/ ({len(downloaded_models)} å€‹):")
            for model_name in downloaded_models:
                model_path = pipeline.qaihub_client.qai_hub_models[model_name].get('optimized_model_path', 'æœªçŸ¥è·¯å¾‘')
                print(f"   - {model_name} -> {model_path}")
        else:
            print("\nâš ï¸  æ²’æœ‰ä¸‹è¼‰å„ªåŒ–æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ç·¨è­¯ä»»å‹™æ˜¯å¦æˆåŠŸå®Œæˆ")
        
        print(f"\nâœ… Compile å®Œæˆï¼")
    else:
        print(f"\nâŒ Compile å¤±æ•—ï¼")


def run_profile():
    """é€²è¡Œæ¨¡å‹æ•ˆèƒ½åˆ†æï¼ˆè‡ªå‹•æƒæ raw ç›®éŒ„ï¼‰"""
    print("\n[Profile] QAI Hub Profile Pipeline")
    
    # ä½¿ç”¨ QAIHubPipeline é€²è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
    pipeline = QAIHubPipeline(get_models_dir())
    
    # åŸ·è¡Œåˆ†ææµç¨‹ï¼Œä½¿ç”¨æ–°çš„ job monitor æœƒè‡ªå‹•è™•ç†ç‹€æ…‹ç›£æ§
    # è‡ªå‹•æƒæ raw ç›®éŒ„ä¸¦æ ¹æ“šç›®æ¨™è£ç½®æ”¯æ´æ±ºå®šä¾†æºé¡å‹
    success = pipeline.run_profile_pipeline()
    
    if success:
        print(f"\nâœ… Profile å®Œæˆï¼")
    else:
        print(f"\nâŒ Profile å¤±æ•—ï¼")


def run_infer():
    """é€²è¡Œæ¨¡å‹æ¨è«–æ¸¬è©¦"""
    print("\n[Infer] QAI Hub Inference Demo (Practical)")
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models()
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æ¨¡å‹æ¨è«–ï¼Œç„¡è‡ªå‹•è½‰æ›)")
    print("\nInfer å®Œæˆï¼")


def run_demo():
    """å•Ÿå‹•äº’å‹•å¼ Demo"""
    print("\n[Demo] QAI Hub Unified Detection Demo")
    demo_qai_hub_detection()
    print("\nDemo å®Œæˆï¼")


def run_official():
    """å®˜æ–¹æ¨¡å¼ï¼ˆç‰¹æ®Šç”¨é€”ï¼‰"""
    print("\n[Official] å®˜æ–¹ QAI Hub Detector Demo")
    demo_official_qai_hub_detection()
    print("\nOfficial Demo å®Œæˆï¼")


def run_test():
    """åŸ·è¡Œæ¸¬è©¦æµç¨‹"""
    print("\n[Test] QAI Hub Unified Detector æ¸¬è©¦ (Live)")
    test_live_detection()
    print("\nTest å®Œæˆï¼")


def run_compile_profile_jobs(source='dlc'):
    """æ‰¹æ¬¡ç·¨è­¯ä¸¦åˆ†æå¤šæ¨¡å‹ï¼ˆè‡ªå‹•è™•ç†å¤šå€‹æ¨¡å‹ï¼‰"""
    print(f"\n[Compile+Profile] QAI Hub Compile+Profile Pipeline (Full)")
    
    # ä½¿ç”¨ pipeline æ¨¡çµ„åŸ·è¡Œå®Œæ•´çš„ç·¨è­¯+åˆ†ææµç¨‹ï¼Œå‚³éæ­£ç¢ºçš„åŸºç¤ç›®éŒ„
    pipeline = QAIHubPipeline(get_models_dir())
    pipeline.run_compile_profile_pipeline(do_infer=False)
    print("\nCompile+Profile Jobs å…¨éƒ¨å®Œæˆï¼")


def run_compile_profile(source='dlc'):
    """å–®ä¸€æ¨¡å‹ç·¨è­¯èˆ‡åˆ†æï¼ˆé‡å°å–®ä¸€æ¨¡å‹ï¼‰"""
    print(f"\n[Compile+Profile+Infer] QAI Hub Compile+Profile+Infer Pipeline (Full Run)")
    
    # ä½¿ç”¨ pipeline æ¨¡çµ„åŸ·è¡Œå®Œæ•´çš„ç·¨è­¯+åˆ†æ+æ¨è«–æµç¨‹ï¼Œå‚³éæ­£ç¢ºçš„åŸºç¤ç›®éŒ„
    pipeline = QAIHubPipeline(get_models_dir())
    pipeline.run_compile_profile_pipeline(do_infer=True)
    print("\nCompile+Profile+Infer å®Œæˆï¼")


def run_link():
    """Link Jobï¼ˆæ¨¡å‹ä¸²æ¥ï¼‰"""
    print(f"\n[Link] QAI Hub Link Job Pipeline")
    
    # ä½¿ç”¨ QAIHubPipeline é€²è¡Œæ¨¡å‹ä¸²æ¥æµç¨‹
    pipeline = QAIHubPipeline(get_models_dir())
    
    # é€™è£¡ç¤ºç¯„ä¸€å€‹åŸºæœ¬çš„ä¸²æ¥é…ç½®ç¯„ä¾‹
    # å¯¦éš›ä½¿ç”¨æ™‚æ‡‰è©²å¾é…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨è€…è¼¸å…¥ç²å–
    link_config = {
        "models": [
            {
                "id": "DETECT_MODEL_ID",  # éœ€è¦æ›¿æ›ç‚ºå¯¦éš›çš„æ¨¡å‹ ID
                "alias": "detector"
            },
            {
                "id": "LANDMARK_MODEL_ID",  # éœ€è¦æ›¿æ›ç‚ºå¯¦éš›çš„æ¨¡å‹ ID
                "alias": "landmark"
            }
        ],
        "connections": [
            {
                "from": "detector:output_boxes",
                "to": "landmark:input_boxes"
            }
        ]
    }
    
    print("ğŸ”— æ¨¡å‹ä¸²æ¥é…ç½®ç¯„ä¾‹:")
    print(f"   - æ¨¡å‹æ•¸é‡: {len(link_config['models'])}")
    print(f"   - é€£æ¥æ•¸é‡: {len(link_config['connections'])}")
    
    # æç¤ºä½¿ç”¨è€…è¼¸å…¥å¯¦éš›çš„æ¨¡å‹ ID
    print("\nâš ï¸  è«‹æ³¨æ„ï¼šéœ€è¦å…ˆå®Œæˆæ¨¡å‹çš„ç·¨è­¯ï¼Œå–å¾—æ¨¡å‹ ID å¾Œæ‰èƒ½é€²è¡Œä¸²æ¥")
    print("   è«‹å°‡é…ç½®ä¸­çš„ 'DETECT_MODEL_ID' å’Œ 'LANDMARK_MODEL_ID' æ›¿æ›ç‚ºå¯¦éš›çš„æ¨¡å‹ ID")
    
    # è©¢å•ä½¿ç”¨è€…æ˜¯å¦è¦ä½¿ç”¨ç¯„ä¾‹é…ç½®æˆ–å¾æ–‡ä»¶è¼‰å…¥
    response = input("\næ˜¯å¦è¦ä½¿ç”¨ç¯„ä¾‹é…ç½®é€²è¡Œä¸²æ¥ï¼Ÿ(y/n): ").strip().lower()
    if response == 'y':
        # æäº¤ä¸²æ¥ä»»å‹™
        link_job = pipeline.qaihub_client.submit_link_job(link_config, "fall_detection_pipeline")
        if link_job:
            print(f"\nâœ… Link Job æäº¤æˆåŠŸï¼Job ID: {link_job.job_id}")
            print("   è«‹ç­‰å¾…ä¸²æ¥ä»»å‹™å®Œæˆ...")
        else:
            print("\nâŒ Link Job æäº¤å¤±æ•—")
    else:
        print("\nâ­ï¸ è·³éä¸²æ¥ä»»å‹™ï¼Œè«‹æ‰‹å‹•æº–å‚™ link_config.json å¾Œå†åŸ·è¡Œ")
    
    print("\nLink Job æµç¨‹å®Œæˆï¼")


# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger('qaihub_optimize')


def find_executable_in_venv(exe_name):
    """å˜—è©¦åœ¨å°ˆæ¡ˆ virtualenv æˆ–ç³»çµ± PATH ä¸­å°‹æ‰¾å¯åŸ·è¡Œæª”ã€‚å›å‚³å®Œæ•´è·¯å¾‘æˆ– Noneã€‚"""
    # 1. æª¢æŸ¥è™›æ“¬ç’°å¢ƒ .venv/bin
    venv_bin = Path(__file__).parent / '.venv' / 'bin'
    candidate = venv_bin / exe_name
    if candidate.exists():
        return str(candidate)
    # 2. æª¢æŸ¥ç•¶å‰ç’°å¢ƒ PATH
    which = shutil.which(exe_name)
    if which:
        return which
    return None


def convert_tflite_to_onnx_advanced(tflite_path, onnx_path):
    """
    ä½¿ç”¨é€²éšè½‰æ›æ–¹æ³•é€²è¡Œ TFLite åˆ° ONNX è½‰æ›
    
    å›å‚³å€¼ï¼š (success: bool, err_msg: str)
    """
    try:
        converter = get_advanced_converter()
        tflite_path_obj = Path(tflite_path)
        onnx_dir = Path(onnx_path).parent
        
        result = converter.convert_tflite_to_onnx_fixed(tflite_path_obj, onnx_dir)
        
        if result["status"] == "ok":
            logger.info(f'è½‰æ›æˆåŠŸ: {tflite_path} -> {onnx_path}')
            return True, result["message"]
        elif result["status"] == "warning":
            logger.warning(f'è½‰æ›å®Œæˆä½†æœ‰è­¦å‘Š: {tflite_path} -> {result["message"]}')
            return True, result["message"]
        else:
            logger.error(f'è½‰æ›å¤±æ•—: {tflite_path} -> {result["message"]}')
            return False, result["message"]
            
    except Exception as e:
        logger.exception(f'åŸ·è¡Œè½‰æ›æ™‚ç™¼ç”Ÿä¾‹å¤–: {str(e)}')
        return False, str(e)


# æ›¿æ›åŸå…ˆçš„è½‰æ›å‡½æ•¸
def convert_tflite_to_onnx(tflite_path, onnx_path):
    # ç¢ºä¿ onnx ç›®éŒ„å­˜åœ¨
    onnx_dir = os.path.dirname(onnx_path)
    try:
        os.makedirs(onnx_dir, exist_ok=True)
    except Exception as e:
        return False

    success, msg = convert_tflite_to_onnx_advanced(tflite_path, onnx_path)
    if not success:
        # å°‡éŒ¯èª¤å¯«å…¥ log æª”ï¼Œæ–¹ä¾¿å¾ŒçºŒåˆ†æ
        log_path = os.path.join(onnx_dir, 'conversion_errors.log')
        with open(log_path, 'a') as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {tflite_path} -> {msg}\n")
    return success


def main():
    parser = argparse.ArgumentParser(
        description=(
            """
    QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…· (æ¨¡çµ„åŒ–ç‰ˆæœ¬)

    ç”¨æ³• (Usage):
        python qai_hub_optimize_full.py <mode>

    å¯ç”¨å­å‘½ä»¤ (mode)ï¼š
        compile                ç·¨è­¯ä¸¦æœ€ä½³åŒ–æ¨¡å‹ï¼ˆè‡ªå‹•æƒæ raw ç›®éŒ„ï¼‰
        profile                é€²è¡Œæ¨¡å‹æ•ˆèƒ½åˆ†æï¼ˆè‡ªå‹•æƒæ raw ç›®éŒ„ï¼‰
        compile_profile_jobs   æ‰¹æ¬¡ç·¨è­¯ä¸¦åˆ†æå¤šæ¨¡å‹ï¼ˆè‡ªå‹•è™•ç†å¤šå€‹æ¨¡å‹ï¼‰ï¼ˆè‡ªå‹•æƒæ raw ç›®éŒ„ï¼‰
        compile_profile        å–®ä¸€æ¨¡å‹ç·¨è­¯èˆ‡åˆ†æï¼ˆé‡å°å–®ä¸€æ¨¡å‹ï¼ˆè‡ªå‹•æƒæ raw ç›®éŒ„ï¼‰ï¼‰
        infer                  é€²è¡Œæ¨¡å‹æ¨è«–æ¸¬è©¦
        demo                   å•Ÿå‹•äº’å‹•å¼ Demo
        official               å®˜æ–¹æ¨¡å¼ï¼ˆç‰¹æ®Šç”¨é€”ï¼‰
        test                   åŸ·è¡Œæ¸¬è©¦æµç¨‹
        link                   Link Jobï¼ˆé€²éšç”¨é€”ï¼‰

    ç¯„ä¾‹ (Examples):
        python qai_hub_optimize_full.py compile
        python qai_hub_optimize_full.py profile

    èªªæ˜ï¼š
        - æ‰€æœ‰æ¨¡å‹ä¾†æºå·²çµ±ä¸€è‡ªå‹•å¾ raw ç›®éŒ„å–å¾—ï¼Œç„¡éœ€æ‰‹å‹•åˆ‡æ›ä¾†æºåƒæ•¸ã€‚
        - å„æ¨¡å¼ç´°ç¯€è«‹åƒè€ƒ doc/QAI_HUB_README.mdã€‚
    """
        ),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('mode', choices=['compile', 'profile', 'infer', 'demo', 'official', 'test', 'compile_profile_jobs', 'compile_profile', 'link'],
                        help="å­å‘½ä»¤: compile | profile | infer | demo | official | test | compile_profile_jobs | compile_profile | link")
    args = parser.parse_args()

    if args.mode == 'compile':
        run_compile()
    elif args.mode == 'profile':
        run_profile()
    elif args.mode == 'infer':
        run_infer()
    elif args.mode == 'demo':
        run_demo()
    elif args.mode == 'official':
        run_official()
    elif args.mode == 'test':
        run_test()
    elif args.mode == 'compile_profile_jobs':
        run_compile_profile_jobs()
    elif args.mode == 'compile_profile':
        run_compile_profile()
    elif args.mode == 'link':
        run_link()
    else:
        print("æœªçŸ¥æ¨¡å¼ï¼")
        sys.exit(1)


# åœ¨ä¸»ç¨‹å¼ä¸­åŠ å…¥ç›®éŒ„æª¢æŸ¥
if __name__ == "__main__":
    # ç¢ºä¿æ‰€æœ‰å¿…è¦ç›®éŒ„å­˜åœ¨
    # MODELS_BASE_DIR å¿…é ˆç‚ºçµ•å°è·¯å¾‘
    MODELS_BASE_DIR = os.path.expanduser(MODELS_BASE_DIR)
    ONNX_MODEL_DIR = os.path.join(MODELS_BASE_DIR, os.getenv("ONNX_MODEL_DIR"))
    RAW_MODEL_DIR = os.path.join(MODELS_BASE_DIR, os.getenv("MODEL_SOURCE_DIR"))
    OPTIMIZED_MODEL_DIR = os.path.join(MODELS_BASE_DIR, os.getenv("OPTIMIZED_MODEL_DIR"))
    
    ensure_directory_exists(MODELS_BASE_DIR)
    ensure_directory_exists(os.path.dirname(ONNX_MODEL_DIR) or ONNX_MODEL_DIR)
    ensure_directory_exists(ONNX_MODEL_DIR)
    ensure_directory_exists(RAW_MODEL_DIR)
    ensure_directory_exists(OPTIMIZED_MODEL_DIR)

    # æ¸¬è©¦è½‰æ›æµç¨‹
    tflite_model_path = os.path.join(RAW_MODEL_DIR, "face_detector.tflite")
    onnx_model_path = os.path.join(ONNX_MODEL_DIR, "face_detector.onnx")
    convert_tflite_to_onnx(tflite_model_path, onnx_model_path)

    try:
        main()
    except FileNotFoundError as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
    except Exception as e:
        print(f"âŒ æœªçŸ¥éŒ¯èª¤: {e}")
