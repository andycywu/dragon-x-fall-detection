#!/usr/bin/env python3
"""
QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…·
æ”¯æ´ Compileã€Profileã€Inferã€Demoã€Test ç­‰å¤šæ¨¡å¼å…¥å£
"""
import argparse
import sys
import os

# åŒ¯å…¥å„åŠŸèƒ½æ¨¡çµ„ï¼ˆå‡è¨­éƒ½åœ¨åŒç›®éŒ„æˆ– PYTHONPATH ä¸‹ï¼‰
from practical_qai_hub_onnx import PracticalQAIHubONNX
from final_qai_hub_onnx_system import FinalQAIHubONNXSystem
from qai_hub_unified_detector import QAIHubUnifiedDetector, demo_qai_hub_detection, test_live_detection
from official_qai_hub_detector import OfficialQAIHubDetector, demo_official_qai_hub_detection


def run_compile(source='dlc'):
    # --- è‡ªå‹•ä¿®æ­£ onnx value_info é‡è¤‡å•é¡Œ ---
    def fix_onnx_value_info(path):
        import onnx
        model = onnx.load(path)
        io_names = set()
        for t in list(model.graph.input) + list(model.graph.output):
            io_names.add(t.name)
        # ç§»é™¤ value_info ä¸­èˆ‡ input/output é‡è¤‡çš„ tensor
        new_value_info = [vi for vi in model.graph.value_info if vi.name not in io_names]
        del model.graph.value_info[:]
        model.graph.value_info.extend(new_value_info)
        onnx.save(model, path)
    print(f"\n[Compile] QAI Hub Compile Pipeline (Practical) | source={source}")
    # æ ¹æ“š source æ±ºå®šç›®éŒ„èˆ‡æ ¼å¼
    source_map = {
        'onnx':      ('onnx', '.onnx'),
        'tflite':    ('org-tflite', '.tflite'),
        'dlc':       ('org-dlc', '.dlc'),
        'org-onnx':  ('onnx', '.onnx'),
        'org-tflite':('org-tflite', '.tflite'),
        'org-dlc':   ('org-dlc', '.dlc'),
        'original':  ('original', '.tflite'),
    }
    from pathlib import Path
    import sys
    # --- ä¸€é–‹å§‹è‡ªå‹•æƒæ org ç›®éŒ„ ---
    org_dir = Path(__file__).parent.parent / 'models' / 'org'
    if not org_dir.exists():
        print(f"âŒ æ‰¾ä¸åˆ° org ç›®éŒ„: {org_dir}")
        return
    all_files = list(org_dir.glob('*'))
    print("\n[æ¨¡å‹æƒæ] org ç›®éŒ„ä¸‹æª”æ¡ˆï¼š")
    for f in all_files:
        print(f"  - {f.name}")
    tflite_files = [f for f in all_files if f.suffix == '.tflite']
    onnx_files = [f for f in all_files if f.suffix == '.onnx']
    dlc_files = [f for f in all_files if f.suffix == '.dlc']
    print(f"\nå…±åµæ¸¬åˆ°ï¼š{len(tflite_files)} å€‹ tflite, {len(onnx_files)} å€‹ onnx, {len(dlc_files)} å€‹ dlc æª”æ¡ˆ")
    # --- è‹¥æœ‰ tfliteï¼Œè©¢å•æ˜¯å¦è¦è½‰æ› ---
    from tflite2onnx_guard import convert_with_diagnostics
    if tflite_files:
        ans = input(f"\nåµæ¸¬åˆ° {len(tflite_files)} å€‹ tflite æª”æ¡ˆï¼Œæ˜¯å¦è¦æ‰¹æ¬¡è½‰æ›æˆ onnxï¼Ÿ(y/n)ï¼š").strip().lower()
        if ans == 'y':
            import struct
            import shutil
            import tempfile
            import tensorflow as tf
            failed_convert = []
            for tflite_path in tflite_files:
                onnx_dir = Path(__file__).parent.parent / 'models' / 'onnx'
                onnx_dir.mkdir(parents=True, exist_ok=True)
                # æª¢æŸ¥ç©ºæª”æ¡ˆ
                if tflite_path.stat().st_size == 0:
                    print(f"[Error] æª”æ¡ˆç‚ºç©ºï¼Œè·³é: {tflite_path.name}")
                    failed_convert.append(tflite_path.name)
                    continue
                # ä½¿ç”¨ guard é€²è¡Œè½‰æ›èˆ‡è¨ºæ–·
                result = convert_with_diagnostics(str(tflite_path), str(onnx_dir), timeout_sec=600)
                if result["status"] == "ok":
                    print(f"âœ… è½‰æ›æˆåŠŸ: {result['onnx_path']}")
                else:
                    print(f"âŒ è½‰æ›å¤±æ•—: {tflite_path.name}")
                    print(result["human_message"])
                    failed_convert.append(tflite_path.name)
                    continue
            print(f"\nâœ… æ‰¹æ¬¡è½‰æ›å®Œæˆï¼Œonnx æª”æ¡ˆå·²å­˜å…¥ {onnx_dir}")
            if failed_convert:
                print(f"\nâš ï¸ ä¸‹åˆ— tflite è½‰æ›å¤±æ•—ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥ï¼š")
                for f in failed_convert:
                    print(f"   - {f}")
            # è½‰æ›å®Œè‡ªå‹•åˆ‡æ› source ç‚º onnx
            source = 'onnx'
    # --- pipeline ç¹¼çºŒ ---
    if source not in source_map:
        print(f"âŒ ä¸æ”¯æ´çš„ source: {source}")
        return
    model_dir, ext = source_map[source]
    # --- pipeline ç¹¼çºŒ ---
    # è‹¥æ˜¯ onnxï¼Œå…ˆè‡ªå‹•ä¿®æ­£æ‰€æœ‰ onnx æª”æ¡ˆ
    if ext == '.onnx':
        from pathlib import Path
        onnx_dir = Path(__file__).parent.parent / 'models' / model_dir
        onnx_files = list(onnx_dir.glob('*.onnx'))
        for onnx_path in onnx_files:
            try:
                fix_onnx_value_info(str(onnx_path))
                print(f"[Auto] ä¿®æ­£ value_info: {onnx_path.name}")
            except Exception as e:
                print(f"[Warning] ä¿®æ­£ value_info å¤±æ•—: {onnx_path.name} | {e}")
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models(source=source, model_dir=model_dir, ext=ext)
    # ONNX æª¢æŸ¥
    if ext == '.onnx':
        invalid = system.check_onnx_models()
        if invalid:
            print("\nâŒ åµæ¸¬åˆ°ä¸‹åˆ— ONNX æ¨¡å‹æª”æ¡ˆæ ¼å¼ç•°å¸¸ï¼Œè«‹ä¿®æ­£å¾Œå†åŸ·è¡Œï¼š")
            for name, path, err in invalid:
                print(f"   - {name}: {path}\n     éŒ¯èª¤: {err}")
            print("\nè«‹åƒè€ƒ onnx.checker.check_model('your_model.onnx', full_check=True) é€²è¡Œæœ¬åœ°æª¢æŸ¥ã€‚\næµç¨‹ä¸­æ­¢ã€‚")
            return
    # åˆ—å‡ºè¦è™•ç†çš„æ¨¡å‹
    models_to_process = [k for k, v in system.qai_hub_models.items() if v.get('loaded')]
    print(f"\nğŸ” åµæ¸¬åˆ° {len(models_to_process)} å€‹æ¨¡å‹å°‡é€²è¡Œ QAI Hub æœ€ä½³åŒ–ï¼š")
    for m in models_to_process:
        print(f"   - {m}")
    # æç¤º QAI Hub ä½œæ¥­æµç¨‹
    print("""
ğŸ“‹ QAI Hub é›²ç«¯æœ€ä½³åŒ–ä½œæ¥­æµç¨‹ï¼š
1. ä¸Šå‚³æ¨¡å‹ï¼ˆUpload Modelï¼‰
2. æäº¤ç·¨è­¯ä»»å‹™ï¼ˆSubmit Compile Jobï¼‰
3. ç­‰å¾…é›²ç«¯å®Œæˆæœ€ä½³åŒ–ï¼ˆJob Queue & Compileï¼‰
4. ä¸‹è¼‰æœ€ä½³åŒ–æ¨¡å‹ï¼ˆDownloadï¼‰
5. å¯é€²è¡Œ Profileã€Inferã€Demo ç­‰å¾ŒçºŒæ¸¬è©¦
\nè©³ç´°èªªæ˜èˆ‡ API ç¯„ä¾‹è«‹åƒè€ƒï¼š
https://app.aihub.qualcomm.com/docs/hub/index.html#examples
""")
    # åƒ…ä¿ç•™ä¸Šå‚³èˆ‡ç·¨è­¯
    system.upload_models_to_qai_hub()
    system.submit_compilation_jobs()
    # ç­‰å¾…æ‰€æœ‰ compile job å®Œæˆ
    import time
    import sys
    print("\nâ³ ç­‰å¾…æ‰€æœ‰ QAI Hub Compile Job å®Œæˆ...")
    all_done = False
    poll_interval = 10  # ç§’
    max_wait = 60 * 30  # æœ€é•·ç­‰å¾… 30 åˆ†é˜
    waited = 0
    completed_status = ('COMPLETED', 'SUCCEEDED', 'SUCCESS', 'FINISHED', 'COMPLETED_SUCCESSFULLY', 'RESULTS_READY', 'RESULTS READY', 'results_ready', 'Results Ready')
    last_status = {}
    while not all_done and waited < max_wait:
        all_done = True
        status_lines = []
        for model_name, model_info in system.qai_hub_models.items():
            job = model_info.get('compile_job')
            if job and hasattr(job, 'refresh'):
                job.refresh()
                status = getattr(job, 'status', None) or getattr(job, 'state', None)
                model_info['compile_status'] = status
            elif job:
                status = getattr(job, 'status', None) or getattr(job, 'state', None)
                model_info['compile_status'] = status
            else:
                status = None
            job_id = getattr(job, 'job_id', '') if job else ''
            status_lines.append(f"  {model_name}: {job_id} ç‹€æ…‹: {status}")
            if status is None or str(status).upper() not in [s.upper() for s in completed_status]:
                all_done = False
        # æ¸…é™¤å‰ä¸€è¼ªé¡¯ç¤º
        if waited > 0:
            sys.stdout.write(f"\033[{len(last_status)}A")  # æ¸¸æ¨™ä¸Šç§» N è¡Œ
        for line in status_lines:
            print(line)
        last_status = status_lines
        if not all_done:
            print(f"  ...å°šæœ‰ Compile Job åŸ·è¡Œä¸­ï¼Œ{poll_interval} ç§’å¾Œå†æŸ¥è©¢...\n")
            time.sleep(poll_interval)
            waited += poll_interval
    if not all_done:
        print("âš ï¸ è¶…éæœ€å¤§ç­‰å¾…æ™‚é–“ï¼Œéƒ¨åˆ† Job å¯èƒ½å°šæœªå®Œæˆã€‚")
    print("\nğŸ“Š QAI Hub Compile Jobç‹€æ…‹:")
    for model_name, model_info in system.qai_hub_models.items():
        job = model_info.get('compile_job')
        job_id = job.job_id if job else ''
        status = model_info.get('compile_status', '')
        print(f"   {model_name}: Job {job_id} ç‹€æ…‹: {status}")
        if job_id:
            print(f"     Dashboard: https://aihub.qualcomm.com/jobs/{job_id}")
    # ç”¢ç”Ÿ HTML å ±å‘Š
    html_file = 'practical_qai_hub_compile_report.html'
    with open(html_file, 'w') as f:
        f.write('<html><head><meta charset="utf-8"><title>QAI Hub Compile Report</title></head><body>')
        f.write('<h1>QAI Hub Compile Report</h1>')
        f.write(f'<p><b>Timestamp:</b> {time.strftime("%Y-%m-%d %H:%M:%S")}</p>')
        f.write('<h2>Models & Compile Jobs</h2><table border="1" cellpadding="4"><tr><th>Model</th><th>Status</th><th>Job ID</th><th>Dashboard</th></tr>')
        for model_name, model_info in system.qai_hub_models.items():
            job = model_info.get('compile_job')
            job_id = job.job_id if job else ''
            dashboard = f'<a href="https://aihub.qualcomm.com/jobs/{job_id}">{job_id}</a>' if job_id else ''
            status = model_info.get('compile_status', '') or ('å·²æäº¤' if job_id else (model_info.get('error') or 'æœªæäº¤'))
            f.write(f'<tr><td>{model_name}</td><td>{status}</td><td>{job_id}</td><td>{dashboard}</td></tr>')
        f.write('</table>')
        f.write('</body></html>')
    print(f"\nâœ… Compile å®Œæˆï¼HTML å ±å‘Šå·²ç”¢ç”Ÿ {html_file}")

def run_profile():
    print("\n[Profile] QAI Hub Profile Pipeline (Practical)")
    from pathlib import Path
    import os
    from practical_qai_hub_onnx import PracticalQAIHubONNX
    org_dir = Path(__file__).parent.parent / 'models' / 'org'
    # æ”¯æ´å¤šæ ¼å¼è‡ªå‹•åµæ¸¬
    all_models = list(org_dir.glob('*.tflite')) + list(org_dir.glob('*.onnx')) + list(org_dir.glob('*.dlc'))
    if not all_models:
        print("âŒ org ç›®éŒ„ä¸‹æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆï¼Œç„¡æ³•é€²è¡Œ profileã€‚")
        return
    print(f"ğŸ” åµæ¸¬åˆ° {len(all_models)} å€‹æ¨¡å‹å°‡é€²è¡Œ QAI Hub profileï¼š")
    for f in all_models:
        print(f"   - {f.name}")
    # å•Ÿå‹•ç³»çµ±ï¼Œå…ˆå–å¾—è£ç½®æ”¯æ´æ ¼å¼
    system = PracticalQAIHubONNX()
    device = system.target_device
    if not device:
        print("âŒ ç„¡æ³•å–å¾—ç›®æ¨™è£ç½®ï¼Œç„¡æ³•é€²è¡Œ profileã€‚")
        return
    device_attrs = getattr(device, 'attributes', [])
    support_onnx = any('framework:onnx' in a for a in device_attrs)
    support_tflite = any('framework:tflite' in a for a in device_attrs)
    support_dlc = any('framework:dlc' in a for a in device_attrs)
    print(f"\n[è£ç½®æ”¯æ´æ ¼å¼] ONNX={support_onnx}, TFLite={support_tflite}, DLC={support_dlc}")
    # çµ±ä¸€è©¢å• tflite æ˜¯å¦è¦è½‰æ›
    tflite_models = [f for f in all_models if f.suffix.lower() == '.tflite']
    convert_tflite = False
    import threading
    def ask_convert():
        print(f"âš ï¸ ç›®æ¨™è£ç½®ä¸æ”¯æ´ TFLiteï¼Œåµæ¸¬åˆ° {len(tflite_models)} å€‹ tflite æ¨¡å‹ã€‚æ˜¯å¦è¦è‡ªå‹•å…¨éƒ¨è½‰æ›ç‚º ONNXï¼Ÿ(y/n, 30ç§’å¾Œè‡ªå‹•è½‰æ›)")
        ans = [None]
        def get_input():
            ans[0] = input().strip().lower()
        t = threading.Thread(target=get_input)
        t.daemon = True
        t.start()
        t.join(timeout=30)
        if ans[0] is None or ans[0] == '':
            print("â° é€¾æ™‚ï¼Œè‡ªå‹•åŸ·è¡Œè½‰æ› (y)")
            return True
        return ans[0] == 'y'
    if tflite_models and not support_tflite:
        if ask_convert():
            convert_tflite = True
    # é è™•ç†æ¨¡å‹æ¸…å–®
    models_to_profile = []
    for f in all_models:
        ext = f.suffix.lower()
        if ext == '.onnx' and not support_onnx:
            print(f"â­ï¸ è·³é {f.name}ï¼Œå› ç›®æ¨™è£ç½®ä¸æ”¯æ´ ONNX")
            continue
        if ext == '.dlc' and not support_dlc:
            print(f"â­ï¸ è·³é {f.name}ï¼Œå› ç›®æ¨™è£ç½®ä¸æ”¯æ´ DLC")
            continue
        if ext == '.tflite':
            if support_tflite:
                models_to_profile.append(f)
            elif convert_tflite:
                # å˜—è©¦è½‰æ›
                onnx_path = str(f.with_suffix('.onnx'))
                print(f"ğŸ”„ è‡ªå‹•è½‰æ› {f} â†’ {onnx_path} ...")
                import subprocess
                result = subprocess.run(['tflite2onnx', str(f), onnx_path], capture_output=True, text=True)
                if result.returncode == 0 and os.path.exists(onnx_path):
                    print(f"âœ… è½‰æ›æˆåŠŸ: {onnx_path}")
                    models_to_profile.append(Path(onnx_path))
                else:
                    print(f"âŒ è½‰æ›å¤±æ•—: {result.stderr}")
            else:
                print(f"â­ï¸ è·³é {f.name}ï¼Œå› ç›®æ¨™è£ç½®ä¸æ”¯æ´ TFLite ä¸”æœªé¸æ“‡è‡ªå‹•è½‰æ›")
        else:
            models_to_profile.append(f)
    if not models_to_profile:
        print("âŒ ç„¡å¯ profile çš„æ¨¡å‹ã€‚æµç¨‹çµæŸã€‚")
        return
    print(f"\nâœ… æœ€çµ‚å°‡é€²è¡Œ profile çš„æ¨¡å‹ï¼š")
    for f in models_to_profile:
        print(f"   - {f.name}")
    # è¼‰å…¥ã€ä¸Šå‚³ã€profile
    # åªè¼‰å…¥è¦ profile çš„æ¨¡å‹
    # ä¾å‰¯æª”åæ±ºå®š source
    for ext, source in [('.onnx', 'onnx'), ('.tflite', 'tflite'), ('.dlc', 'dlc')]:
        files = [f for f in models_to_profile if f.suffix.lower() == ext]
        if files:
            system.load_mediapipe_models(source=source, model_dir='org', ext=ext)
    system.upload_models_to_qai_hub()
    system.submit_profile_jobs()
    # ç­‰å¾…æ‰€æœ‰ profile job å®Œæˆ
    import time
    import sys
    import threading
    print("\nâ³ ç­‰å¾…æ‰€æœ‰ QAI Hub Profile Job å®Œæˆ... (æŒ‰ q + Enter å¯éš¨æ™‚è·³å‡º)\n")
    all_done = False
    poll_interval = 10  # ç§’
    max_wait = 60 * 30  # æœ€é•·ç­‰å¾… 30 åˆ†é˜
    waited = 0
    completed_status = ('COMPLETED', 'SUCCEEDED', 'SUCCESS', 'FINISHED', 'COMPLETED_SUCCESSFULLY', 'RESULTS_READY', 'RESULTS READY', 'results_ready', 'Results Ready')
    last_status = []
    user_quit = [False]
    def check_quit():
        while True:
            s = sys.stdin.readline().strip().lower()
            if s == 'q':
                user_quit[0] = True
                break
    quit_thread = threading.Thread(target=check_quit, daemon=True)
    quit_thread.start()
    while not all_done and waited < max_wait and not user_quit[0]:
        all_done = True
        status_lines = []
        for model_name, model_info in system.qai_hub_models.items():
            job = model_info.get('profile_job')
            # å¼·åˆ¶æ¯æ¬¡éƒ½é‡æ–°æŸ¥è©¢é›²ç«¯ï¼ˆå³ä½¿æ²’æœ‰ refresh ä¹Ÿé‡æ–°å–å¾—æœ€æ–°ç‹€æ…‹ï¼‰
            status = None
            api_raw_status = None
            if job and hasattr(job, 'refresh'):
                job.refresh()
                status = getattr(job, 'status', None) or getattr(job, 'state', None)
                # é¡å¤–é¡¯ç¤º API å›å‚³çš„åŸå§‹ç‹€æ…‹å­—ä¸²ï¼ˆå¦‚æœ‰ï¼‰
                api_raw_status = getattr(job, 'raw_status', None) or getattr(job, '_raw_status', None)
                model_info['profile_status'] = status
            elif job:
                # å˜—è©¦å¼·åˆ¶é‡æ–°å–å¾—ç‹€æ…‹ï¼ˆå¦‚ SDK æ”¯æ´ï¼‰
                try:
                    if hasattr(job, 'get_status'):
                        status = job.get_status()
                        api_raw_status = status
                    else:
                        status = getattr(job, 'status', None) or getattr(job, 'state', None)
                        api_raw_status = status
                except Exception as e:
                    status = None
                    api_raw_status = None
                model_info['profile_status'] = status
            else:
                status = None
                api_raw_status = None
            job_id = getattr(job, 'job_id', '') if job else ''
            status_str = f"{model_name:30} | Job: {job_id:12} | ç‹€æ…‹: {str(status) if status else 'æŸ¥è©¢ä¸­...'}"
            if api_raw_status and str(api_raw_status) != str(status):
                status_str += f" | APIåŸå§‹: {api_raw_status}"
            status_lines.append(status_str)
            if status is None or str(status).upper() not in [s.upper() for s in completed_status]:
                all_done = False
        # æ¸…é™¤å‰ä¸€è¼ªé¡¯ç¤º
        if waited > 0:
            sys.stdout.write(f"\033[{len(last_status)}A")
        for line in status_lines:
            print(line)
        last_status = status_lines
        if not all_done and not user_quit[0]:
            print(f"\n  ...å°šæœ‰ Profile Job åŸ·è¡Œä¸­ï¼Œ{poll_interval} ç§’å¾Œå†æŸ¥è©¢... (æŒ‰ q + Enter å¯è·³å‡º)\n")
            time.sleep(poll_interval)
            waited += poll_interval
    if user_quit[0]:
        print("\nâš ï¸ ä½¿ç”¨è€…å·²é¸æ“‡è·³å‡ºç­‰å¾…ï¼Œéƒ¨åˆ† Job å¯èƒ½å°šæœªå®Œæˆã€‚\n")
    elif not all_done:
        print("âš ï¸ è¶…éæœ€å¤§ç­‰å¾…æ™‚é–“ï¼Œéƒ¨åˆ† Job å¯èƒ½å°šæœªå®Œæˆã€‚\n")
    print("\nğŸ“Š QAI Hub Profile Jobç‹€æ…‹:")
    for model_name, model_info in system.qai_hub_models.items():
        job = model_info.get('profile_job')
        job_id = job.job_id if job else ''
        status = model_info.get('profile_status', '')
        print(f"   {model_name}: Job {job_id} ç‹€æ…‹: {status}")
        if job_id:
            print(f"     Dashboard: https://aihub.qualcomm.com/jobs/{job_id}")
    # ç”¢ç”Ÿ HTML å ±å‘Š
    html_file = 'practical_qai_hub_profile_report.html'
    with open(html_file, 'w') as f:
        f.write('<html><head><meta charset="utf-8"><title>QAI Hub Profile Report</title></head><body>')
        f.write('<h1>QAI Hub Profile Report</h1>')
        f.write(f'<p><b>Timestamp:</b> {time.strftime("%Y-%m-%d %H:%M:%S")}</p>')
        f.write('<h2>Models & Profile Jobs</h2><table border="1" cellpadding="4"><tr><th>Model</th><th>Status</th><th>Job ID</th><th>Dashboard</th></tr>')
        for model_name, model_info in system.qai_hub_models.items():
            job = model_info.get('profile_job')
            job_id = job.job_id if job else ''
            dashboard = f'<a href="https://aihub.qualcomm.com/jobs/{job_id}">{job_id}</a>' if job_id else ''
            status = model_info.get('profile_status', '') or ('å·²æäº¤' if job_id else (model_info.get('error') or 'æœªæäº¤'))
            f.write(f'<tr><td>{model_name}</td><td>{status}</td><td>{job_id}</td><td>{dashboard}</td></tr>')
        f.write('</table>')
        f.write('</body></html>')
    print(f"\nâœ… Profile å®Œæˆï¼HTML å ±å‘Šå·²ç”¢ç”Ÿ {html_file}")

def run_infer():
    print("\n[Infer] QAI Hub Inference Demo (Practical)")
    system = PracticalQAIHubONNX()
    system.load_mediapipe_models()
    # åƒ…ä¿ç•™ç¾æœ‰æ¨¡å‹æ¨è«–ï¼ˆä¸åšä»»ä½•è½‰æ›ï¼‰
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æ¨¡å‹æ¨è«–ï¼Œç„¡è‡ªå‹•è½‰æ›)")
    print("\nInfer å®Œæˆï¼")

def run_demo():
    print("\n[Demo] QAI Hub Unified Detection Demo")
    demo_qai_hub_detection()
    print("\nDemo å®Œæˆï¼")

def run_official():
    print("\n[Official] å®˜æ–¹ QAI Hub Detector Demo")
    demo_official_qai_hub_detection()
    print("\nOfficial Demo å®Œæˆï¼")

def run_test():
    print("\n[Test] QAI Hub Unified Detector æ¸¬è©¦ (Live)")
    test_live_detection()
    print("\nTest å®Œæˆï¼")

def run_compile_profile_jobs(source='dlc'):
    print(f"\n[Compile+Profile] QAI Hub Compile+Profile Pipeline (Full) | source={source}")
    # åƒ…ä¿ç•™ç¾æœ‰æª”æ¡ˆçš„ compile+profile æµç¨‹
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æª”æ¡ˆï¼Œä¸åšè‡ªå‹•è½‰æ›)")
    print("\nCompile+Profile Jobs å®Œæˆï¼")

def run_compile_profile(source='dlc'):
    print(f"\n[Compile+Profile] QAI Hub Compile+Profile Pipeline (Half) | source={source}")
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æª”æ¡ˆï¼Œä¸åšè‡ªå‹•è½‰æ›)")
    print("\nCompile+Profile å®Œæˆï¼")

def run_link(source='dlc'):
    print(f"\n[Link] QAI Hub Link Job Pipeline | source={source}")
    print("(ç›®å‰åƒ…æ”¯æ´ç¾æœ‰æª”æ¡ˆï¼Œä¸åšè‡ªå‹•è½‰æ›)")
    print("\nLink Job å®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description="QAI Hub Optimize Full - All-in-One æ•´åˆå·¥å…·")
    parser.add_argument('mode', choices=['compile', 'profile', 'infer', 'demo', 'official', 'test', 'compile_profile_jobs', 'compile_profile', 'link'],
                        help="åŸ·è¡Œæ¨¡å¼: compile | profile | infer | demo | official | test | compile_profile_jobs | compile_profile | link")
    parser.add_argument('--source', choices=['onnx', 'original', 'org-onnx', 'org-tflite', 'org-dlc', 'dlc'], default='dlc',
                        help="æ¨¡å‹ä¾†æº: onnxã€originalã€org-onnxã€org-tfliteã€org-dlcã€dlc (é è¨­ dlc)")
    args = parser.parse_args()

    if args.mode == 'compile':
        run_compile(source=args.source)
    elif args.mode == 'profile':
        run_profile()
    elif args.mode == 'infer':
        run_infer()
    elif args.mode == 'demo':
        run_demo()
    elif args.mode == 'official':
        run_official()
    elif args.mode == 'test':
        run_test()
    elif args.mode == 'compile_profile_jobs':
        run_compile_profile_jobs(source=args.source)
    elif args.mode == 'compile_profile':
        run_compile_profile(source=args.source)
    elif args.mode == 'link':
        run_link(source=args.source)
    else:
        print("æœªçŸ¥æ¨¡å¼ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main()
