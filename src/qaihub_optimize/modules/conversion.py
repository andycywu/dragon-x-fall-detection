"""
æ¨¡å‹è½‰æ›æ¨¡çµ„ - è² è²¬æ¨¡å‹æ ¼å¼è½‰æ›åŠŸèƒ½
"""
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Optional
import threading
import onnx
from onnx import ModelProto
import torch


def sanitize_onnx_inplace(model: ModelProto) -> dict:
    """
    ä¿®æ­£ 1) IO èˆ‡ value_info é‡è¤‡ã€2) initializer ä¹Ÿåˆ—åœ¨ input çš„å•é¡Œï¼Œ
    ä¸¦éè¿´è™•ç†å­åœ–ã€‚
    """
    report = {"removed_value_info": 0, "removed_input_initializers": 0}

    def _remove_vi_dups(graph):
        io_names = {t.name for t in list(graph.input) + list(graph.output)}
        keep = []
        removed = 0
        for vi in graph.value_info:
            if vi.name in io_names:
                removed += 1
            else:
                keep.append(vi)
        del graph.value_info[:]
        graph.value_info.extend(keep)
        return removed

    def _inputs_not_in_initializers(graph):
        init_names = {init.name for init in graph.initializer}
        keep = []
        removed = 0
        for in_tensor in graph.input:
            if in_tensor.name in init_names:
                removed += 1
            else:
                keep.append(in_tensor)
        del graph.input[:]
        graph.input.extend(keep)
        return removed

    def _walk_subgraphs(node):
        subgraphs = []
        for attr in node.attribute:
            if attr.g:
                subgraphs.append(attr.g)
            for g in attr.graphs:
                subgraphs.append(g)
        return subgraphs

    def _sanitize_graph(g):
        report["removed_value_info"] += _remove_vi_dups(g)
        report["removed_input_initializers"] += _inputs_not_in_initializers(g)
        for n in g.node:
            for sg in _walk_subgraphs(n):
                _sanitize_graph(sg)

    _sanitize_graph(model.graph)
    return report


def convert_pt_to_onnx(pt_model_path: str, onnx_output_path: str, input_shape: tuple):
    """
    å°‡ PyTorch (.pt) æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼ã€‚

    Args:
        pt_model_path (str): PyTorch æ¨¡å‹çš„è·¯å¾‘ã€‚
        onnx_output_path (str): è¼¸å‡ºçš„ ONNX æ¨¡å‹è·¯å¾‘ã€‚
        input_shape (tuple): æ¨¡å‹çš„è¼¸å…¥å½¢ç‹€ã€‚

    Returns:
        dict: åŒ…å«è½‰æ›ç‹€æ…‹å’Œè¨Šæ¯çš„å­—å…¸ã€‚
    """
    try:
        # åŠ è¼‰ PyTorch æ¨¡å‹
        model = torch.load(pt_model_path)
        model.eval()

        # å»ºç«‹è™›æ“¬è¼¸å…¥
        dummy_input = torch.randn(*input_shape)

        # å°‡æ¨¡å‹å°å‡ºç‚º ONNX
        torch.onnx.export(
            model,
            dummy_input,
            onnx_output_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output']
        )

        return {
            "status": "ok",
            "message": f"æˆåŠŸå°‡ {pt_model_path} è½‰æ›ç‚º {onnx_output_path}"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è½‰æ›å¤±æ•—: {str(e)}"
        }


class ModelConverter:
    """æ¨¡å‹æ ¼å¼è½‰æ›å™¨"""
    
    def __init__(self):
        self.conversion_timeout = 600  # è½‰æ›è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    
    def convert_tflite_to_onnx(self, tflite_path: Path, output_dir: Path) -> Dict:
        """
        å°‡ TFLite æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼
        
        Args:
            tflite_path: TFLite æ¨¡å‹è·¯å¾‘
            output_dir: è¼¸å‡ºç›®éŒ„
            
        Returns:
            è½‰æ›çµæœå­—å…¸
        """
        if not tflite_path.exists():
            return {
                "status": "error",
                "message": f"TFLite æª”æ¡ˆä¸å­˜åœ¨: {tflite_path}",
                "onnx_path": None
            }
        
        if tflite_path.stat().st_size == 0:
            return {
                "status": "error",
                "message": f"TFLite æª”æ¡ˆç‚ºç©º: {tflite_path.name}",
                "onnx_path": None
            }
        
        output_dir.mkdir(parents=True, exist_ok=True)
        onnx_path = output_dir / f"{tflite_path.stem}.onnx"
        
        try:
            # ä½¿ç”¨ tflite2onnx é€²è¡Œè½‰æ›
            result = subprocess.run(
                ['tflite2onnx', str(tflite_path), str(onnx_path)],
                capture_output=True,
                text=True,
                timeout=self.conversion_timeout
            )

            if result.returncode == 0 and onnx_path.exists():
                # åŠ å…¥ sanitize æ­¥é©Ÿ
                model = onnx.load(onnx_path)
                sanitize_report = sanitize_onnx_inplace(model)
                onnx.checker.check_model(model, full_check=True)
                onnx.save(model, onnx_path)

                return {
                    "status": "ok",
                    "message": "è½‰æ›æˆåŠŸä¸¦å·²æ¸…ç†",
                    "onnx_path": onnx_path,
                    "tflite_path": tflite_path,
                    "sanitize_report": sanitize_report,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
            else:
                return {
                    "status": "error",
                    "message": f"è½‰æ›å¤±æ•—: {result.stderr}",
                    "onnx_path": None,
                    "tflite_path": tflite_path,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }

        except subprocess.TimeoutExpired:
            return {
                "status": "error",
                "message": f"è½‰æ›è¶…æ™‚ ({self.conversion_timeout}ç§’)",
                "onnx_path": None,
                "tflite_path": tflite_path
            }
        except Exception as e:
            return {
                "status": "error",
                "message": f"è½‰æ›ç•°å¸¸: {str(e)}",
                "onnx_path": None,
                "tflite_path": tflite_path
            }
    
    def batch_convert_tflite(self, tflite_files: List[Path], output_dir: Path) -> List[Dict]:
        """
        æ‰¹æ¬¡è½‰æ›å¤šå€‹ TFLite æª”æ¡ˆ
        
        Args:
            tflite_files: TFLite æª”æ¡ˆåˆ—è¡¨
            output_dir: è¼¸å‡ºç›®éŒ„
            
        Returns:
            è½‰æ›çµæœåˆ—è¡¨
        """
        results = []
        failed_conversions = []
        
        print(f"\nğŸ”„ é–‹å§‹æ‰¹æ¬¡è½‰æ› {len(tflite_files)} å€‹ TFLite æª”æ¡ˆ...")
        
        for i, tflite_path in enumerate(tflite_files, 1):
            print(f"  [{i}/{len(tflite_files)}] è½‰æ› {tflite_path.name} ...")
            
            result = self.convert_tflite_to_onnx(tflite_path, output_dir)
            results.append(result)
            
            if result["status"] == "ok":
                print(f"    âœ… æˆåŠŸ: {result['onnx_path'].name}")
            else:
                print(f"    âŒ å¤±æ•—: {result['message']}")
                failed_conversions.append(tflite_path.name)
        
        return results, failed_conversions
    
    def ask_for_conversion(self, tflite_count: int, timeout: int = 30) -> bool:
        """
        è©¢å•ä½¿ç”¨è€…æ˜¯å¦è¦é€²è¡Œè½‰æ›
        
        Args:
            tflite_count: TFLite æª”æ¡ˆæ•¸é‡
            timeout: è©¢å•è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦é€²è¡Œè½‰æ›
        """
        print(f"âš ï¸  åµæ¸¬åˆ° {tflite_count} å€‹ TFLite æ¨¡å‹ã€‚æ˜¯å¦è¦è‡ªå‹•å…¨éƒ¨è½‰æ›ç‚º ONNXï¼Ÿ(y/n, {timeout}ç§’å¾Œè‡ªå‹•è½‰æ›)")
        
        user_response = [None]
        
        def get_user_input():
            try:
                user_response[0] = input().strip().lower()
            except:
                user_response[0] = None
        
        # å•Ÿå‹•è¼¸å…¥åŸ·è¡Œç·’
        input_thread = threading.Thread(target=get_user_input)
        input_thread.daemon = True
        input_thread.start()
        
        # ç­‰å¾…ä½¿ç”¨è€…è¼¸å…¥æˆ–è¶…æ™‚
        input_thread.join(timeout=timeout)
        
        if user_response[0] is None or user_response[0] == '':
            print("â° é€¾æ™‚ï¼Œè‡ªå‹•åŸ·è¡Œè½‰æ› (y)")
            return True
        
        return user_response[0] == 'y'
    
    def convert_pt_to_onnx(self, pt_model_path: Path, output_dir: Path, input_shape: tuple) -> Dict:
        """
        å°‡ PyTorch (.pt) æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼ã€‚

        Args:
            pt_model_path (Path): PyTorch æ¨¡å‹çš„è·¯å¾‘ã€‚
            output_dir (Path): è¼¸å‡ºç›®éŒ„ã€‚
            input_shape (tuple): æ¨¡å‹çš„è¼¸å…¥å½¢ç‹€ã€‚

        Returns:
            dict: åŒ…å«è½‰æ›ç‹€æ…‹å’Œè¨Šæ¯çš„å­—å…¸ã€‚
        """
        if not pt_model_path.exists():
            return {
                "status": "error",
                "message": f"PyTorch æ¨¡å‹ä¸å­˜åœ¨: {pt_model_path}"
            }

        output_dir.mkdir(parents=True, exist_ok=True)
        onnx_path = output_dir / f"{pt_model_path.stem}.onnx"

        return convert_pt_to_onnx(str(pt_model_path), str(onnx_path), input_shape)


# å–®ä¾‹å¯¦ä¾‹
model_converter = ModelConverter()


def convert_tflite_files(tflite_files: List[Path], output_dir: Path) -> List[Dict]:
    """
    å¿«é€Ÿè½‰æ› TFLite æª”æ¡ˆ
    
    Args:
        tflite_files: TFLite æª”æ¡ˆåˆ—è¡¨
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        è½‰æ›çµæœåˆ—è¡¨å’Œå¤±æ•—æ¸…å–®
    """
    return model_converter.batch_convert_tflite(tflite_files, output_dir)


def ask_for_tflite_conversion(tflite_count: int) -> bool:
    """
    å¿«é€Ÿè©¢å•æ˜¯å¦é€²è¡Œ TFLite è½‰æ›
    
    Args:
        tflite_count: TFLite æª”æ¡ˆæ•¸é‡
        
    Returns:
        æ˜¯å¦é€²è¡Œè½‰æ›
    """
    return model_converter.ask_for_conversion(tflite_count)


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼
    test_dir = Path(__file__).parent.parent.parent / 'models' / 'test'
    test_tflite = test_dir / "test.tflite"
    
    if test_tflite.exists():
        result = model_converter.convert_tflite_to_onnx(test_tflite, test_dir)
        print(f"è½‰æ›çµæœ: {result}")
    else:
        print("æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹å…ˆå»ºç«‹æ¸¬è©¦æª”æ¡ˆ")
