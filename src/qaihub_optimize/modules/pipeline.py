"""
å·¥ä½œæµç¨‹ç®¡é“æ¨¡çµ„
è² è²¬ç®¡ç†å®Œæ•´çš„ QAI Hub æœ€ä½³åŒ–å·¥ä½œæµç¨‹
"""
import time
import threading
from pathlib import Path
from typing import Dict, List, Optional, Callable, Any
import sys

from .scanner import ModelScanner
from .conversion import ModelConverter
from .format_check import FormatChecker
from .qaihub_client import QAIHubClient


class QAIHubPipeline:
    """QAI Hub å·¥ä½œæµç¨‹ç®¡é“ç®¡ç†é¡åˆ¥"""
    
    def __init__(self, base_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµç¨‹ç®¡é“
        
        Args:
            base_dir: åŸºç¤ç›®éŒ„è·¯å¾‘
        """
        self.base_dir = base_dir or Path.cwd()
        # ç¢ºä¿åŸºç¤ç›®éŒ„æŒ‡å‘æ­£ç¢ºçš„ models ç›®éŒ„
        models_base_dir = self.base_dir / 'models' if (self.base_dir / 'models').exists() else self.base_dir
        self.scanner = ModelScanner(models_base_dir)
        self.converter = ModelConverter()
        self.format_checker = FormatChecker(models_base_dir)
        self.qaihub_client = QAIHubClient(models_base_dir)
        self.current_models = {}
    
    def scan_models(self) -> Dict[str, List[Path]]:
        """
        æƒææ¨¡å‹æª”æ¡ˆ
        
        Returns:
            æƒæåˆ°çš„æ¨¡å‹æª”æ¡ˆå­—å…¸
        """
        print("ğŸ” é–‹å§‹æƒææ¨¡å‹æª”æ¡ˆ...")
        try:
            models = self.scanner.scan_org_directory()
            self.current_models = models
            return models
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return {}
        except Exception as e:
            print(f"âŒ æƒææ¨¡å‹å¤±æ•—: {e}")
            return {}
    
    def convert_tflite_models(self, ask_user: bool = True) -> bool:
        """
        è½‰æ› TFLite æ¨¡å‹åˆ° ONNX
        
        Args:
            ask_user: æ˜¯å¦è©¢å•ä½¿ç”¨è€…
            
        Returns:
            è½‰æ›æ˜¯å¦æˆåŠŸ
        """
        tflite_files = self.current_models.get('tflite', [])
        if not tflite_files:
            print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° TFLite æ¨¡å‹éœ€è¦è½‰æ›")
            return True
        
        print(f"ğŸ”„ ç™¼ç¾ {len(tflite_files)} å€‹ TFLite æ¨¡å‹éœ€è¦è½‰æ›")
        
        if ask_user:
            response = input(f"æ˜¯å¦è¦å°‡ {len(tflite_files)} å€‹ TFLite æ¨¡å‹è½‰æ›ç‚º ONNXï¼Ÿ(y/n): ").strip().lower()
            if response != 'y':
                print("â­ï¸ è·³é TFLite è½‰æ›")
                return True
        
        success_count = 0
        failed_files = []
        
        for tflite_path in tflite_files:
            try:
                result = self.converter.convert_tflite_to_onnx(tflite_path, self.base_dir / 'models' / 'onnx')
                if result['status'] == 'ok':
                    success_count += 1
                    print(f"âœ… è½‰æ›æˆåŠŸ: {tflite_path.name}")
                else:
                    failed_files.append(tflite_path.name)
                    print(f"âŒ è½‰æ›å¤±æ•—: {tflite_path.name} - {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            except Exception as e:
                failed_files.append(tflite_path.name)
                print(f"âŒ è½‰æ›ç•°å¸¸: {tflite_path.name} - {e}")
        
        print(f"\nğŸ“Š TFLite è½‰æ›çµæœ: {success_count} æˆåŠŸ, {len(failed_files)} å¤±æ•—")
        if failed_files:
            print("âŒ è½‰æ›å¤±æ•—çš„æª”æ¡ˆ:")
            for filename in failed_files:
                print(f"   - {filename}")
        
        return len(failed_files) == 0
    
    def check_and_fix_onnx_models(self) -> bool:
        """
        æª¢æŸ¥ä¸¦ä¿®å¾© ONNX æ¨¡å‹æ ¼å¼
        
        Returns:
            æª¢æŸ¥ä¿®å¾©æ˜¯å¦æˆåŠŸ
        """
        onnx_files = self.current_models.get('onnx', [])
        if not onnx_files:
            print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° ONNX æ¨¡å‹éœ€è¦æª¢æŸ¥")
            return True
        
        print(f"ğŸ”§ æª¢æŸ¥ {len(onnx_files)} å€‹ ONNX æ¨¡å‹æ ¼å¼...")
        
        invalid_models = []
        fixed_count = 0
        
        for onnx_path in onnx_files:
            try:
                # æª¢æŸ¥æ¨¡å‹æ ¼å¼
                is_valid, error = self.format_checker.check_onnx_model(onnx_path)
                if not is_valid:
                    print(f"âš ï¸ æ ¼å¼ç•°å¸¸: {onnx_path.name} - {error}")
                    invalid_models.append((onnx_path.name, error))
                    
                    # å˜—è©¦ä¿®å¾© value_info
                    if "value_info" in str(error).lower():
                        fixed = self.format_checker.fix_onnx_value_info(onnx_path)
                        if fixed:
                            fixed_count += 1
                            print(f"âœ… ä¿®å¾©æˆåŠŸ: {onnx_path.name}")
                        else:
                            print(f"âŒ ä¿®å¾©å¤±æ•—: {onnx_path.name}")
                else:
                    print(f"âœ… æ ¼å¼æ­£å¸¸: {onnx_path.name}")
                    
            except Exception as e:
                print(f"âŒ æª¢æŸ¥ç•°å¸¸: {onnx_path.name} - {e}")
                invalid_models.append((onnx_path.name, str(e)))
        
        print(f"\nğŸ“Š ONNX æª¢æŸ¥çµæœ: {fixed_count} å€‹ä¿®å¾©, {len(invalid_models)} å€‹ç•°å¸¸")
        
        if invalid_models:
            print("âŒ æ ¼å¼ç•°å¸¸çš„æ¨¡å‹:")
            for filename, error in invalid_models:
                print(f"   - {filename}: {error}")
            return False
        
        return True
    
    def run_compile_pipeline(self, source: str = 'onnx') -> bool:
        """
        åŸ·è¡Œç·¨è­¯å·¥ä½œæµç¨‹
        
        Args:
            source: æ¨¡å‹ä¾†æºé¡å‹
            
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹ç·¨è­¯å·¥ä½œæµç¨‹ (source: {source})")
        
        # æƒææ¨¡å‹
        models = self.scan_models()
        if not models:
            print("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æª”æ¡ˆ")
            return False
        
        # æ ¹æ“šä¾†æºé¡å‹è™•ç†æ¨¡å‹
        if source == 'tflite':
            if not self.convert_tflite_models(ask_user=True):
                print("âŒ TFLite è½‰æ›å¤±æ•—ï¼Œæµç¨‹ä¸­æ­¢")
                return False
            source = 'onnx'  # è½‰æ›å¾Œä½¿ç”¨ ONNX
        
        # æª¢æŸ¥ ONNX æ¨¡å‹æ ¼å¼
        if source == 'onnx':
            if not self.check_and_fix_onnx_models():
                print("âŒ ONNX æ ¼å¼æª¢æŸ¥å¤±æ•—ï¼Œæµç¨‹ä¸­æ­¢")
                return False
        
        # è¼‰å…¥æ¨¡å‹åˆ° QAI Hub å®¢æˆ¶ç«¯
        ext_map = {
            'onnx': '.onnx',
            'tflite': '.tflite', 
            'dlc': '.dlc'
        }
        
        if source not in ext_map:
            print(f"âŒ ä¸æ”¯æ´çš„ä¾†æºé¡å‹: {source}")
            return False
        
        loaded = self.qaihub_client.load_models(source, 'org', ext_map[source])
        if not loaded:
            print("âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—")
            return False
        
        # ä¸Šå‚³æ¨¡å‹åˆ° QAI Hub
        if not self.qaihub_client.upload_models():
            print("âŒ ä¸Šå‚³æ¨¡å‹å¤±æ•—")
            return False
        
        # æäº¤ç·¨è­¯ä»»å‹™
        if not self.qaihub_client.submit_compilation_jobs():
            print("âŒ æäº¤ç·¨è­¯ä»»å‹™å¤±æ•—")
            return False
        
        # ç­‰å¾…ç·¨è­¯å®Œæˆ
        final_statuses = self.qaihub_client.wait_for_jobs_completion('compile')
        
        # ç”¢ç”Ÿå ±å‘Š
        report_file = self.qaihub_client.generate_html_report('compile')
        
        # æª¢æŸ¥æœ€çµ‚ç‹€æ…‹
        success_count = sum(1 for status in final_statuses.values() 
                          if str(status).upper() in ['COMPLETED', 'SUCCEEDED', 'SUCCESS'])
        
        print(f"\nğŸ¯ ç·¨è­¯æµç¨‹å®Œæˆ: {success_count}/{len(final_statuses)} å€‹ä»»å‹™æˆåŠŸ")
        print(f"ğŸ“Š è©³ç´°å ±å‘Š: {report_file}")
        
        return success_count > 0
    
    def run_profile_pipeline(self) -> bool:
        """
        åŸ·è¡Œæ•ˆèƒ½åˆ†æå·¥ä½œæµç¨‹
        
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹æ•ˆèƒ½åˆ†æå·¥ä½œæµç¨‹")
        
        # æƒææ¨¡å‹
        models = self.scan_models()
        if not models:
            print("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æª”æ¡ˆ")
            return False
        
        # æª¢æŸ¥è£ç½®æ”¯æ´
        support_info = self.qaihub_client.check_device_support()
        
        # æ ¹æ“šè£ç½®æ”¯æ´éæ¿¾æ¨¡å‹
        models_to_process = []
        for ext, files in models.items():
            if ext == 'onnx' and support_info.get('onnx', False):
                models_to_process.extend(files)
            elif ext == 'tflite' and support_info.get('tflite', False):
                models_to_process.extend(files)
            elif ext == 'dlc' and support_info.get('dlc', False):
                models_to_process.extend(files)
            else:
                print(f"â­ï¸ è·³é {ext.upper()} æ ¼å¼ï¼Œè£ç½®ä¸æ”¯æ´")
        
        if not models_to_process:
            print("âŒ æ²’æœ‰è£ç½®æ”¯æ´çš„æ¨¡å‹æ ¼å¼")
            return False
        
        print(f"âœ… å°‡è™•ç† {len(models_to_process)} å€‹è£ç½®æ”¯æ´çš„æ¨¡å‹")
        
        # è¼‰å…¥æ‰€æœ‰æ”¯æ´çš„æ¨¡å‹
        loaded_count = 0
        for ext in ['onnx', 'tflite', 'dlc']:
            if support_info.get(ext, False):
                loaded = self.qaihub_client.load_models(ext, 'org', f'.{ext}')
                if loaded:
                    loaded_count += len(loaded)
        
        if loaded_count == 0:
            print("âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—")
            return False
        
        # ä¸Šå‚³æ¨¡å‹åˆ° QAI Hub
        if not self.qaihub_client.upload_models():
            print("âŒ ä¸Šå‚³æ¨¡å‹å¤±æ•—")
            return False
        
        # æäº¤åˆ†æä»»å‹™
        if not self.qaihub_client.submit_profile_jobs():
            print("âŒ æäº¤åˆ†æä»»å‹™å¤±æ•—")
            return False
        
        # ç­‰å¾…åˆ†æå®Œæˆ
        final_statuses = self.qaihub_client.wait_for_jobs_completion('profile')
        
        # ç”¢ç”Ÿå ±å‘Š
        report_file = self.qaihub_client.generate_html_report('profile')
        
        # æª¢æŸ¥æœ€çµ‚ç‹€æ…‹
        success_count = sum(1 for status in final_statuses.values() 
                          if str(status).upper() in ['COMPLETED', 'SUCCEEDED', 'SUCCESS'])
        
        print(f"\nğŸ¯ åˆ†ææµç¨‹å®Œæˆ: {success_count}/{len(final_statuses)} å€‹ä»»å‹™æˆåŠŸ")
        print(f"ğŸ“Š è©³ç´°å ±å‘Š: {report_file}")
        
        return success_count > 0
    
    def run_full_pipeline(self, do_profile: bool = True, do_infer: bool = False) -> bool:
        """
        åŸ·è¡Œå®Œæ•´å·¥ä½œæµç¨‹ (ç·¨è­¯ + åˆ†æ + æ¨è«–)
        
        Args:
            do_profile: æ˜¯å¦åŸ·è¡Œæ•ˆèƒ½åˆ†æ
            do_infer: æ˜¯å¦åŸ·è¡Œæ¨è«–æ¸¬è©¦
            
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹å®Œæ•´å·¥ä½œæµç¨‹ (Profile: {do_profile}, Infer: {do_infer})")
        
        # åŸ·è¡Œç·¨è­¯æµç¨‹
        compile_success = self.run_compile_pipeline('onnx')
        if not compile_success:
            print("âŒ ç·¨è­¯æµç¨‹å¤±æ•—ï¼Œå®Œæ•´æµç¨‹ä¸­æ­¢")
            return False
        
        # åŸ·è¡Œåˆ†ææµç¨‹
        profile_success = True
        if do_profile:
            profile_success = self.run_profile_pipeline()
            if not profile_success:
                print("âš ï¸ åˆ†ææµç¨‹å¤±æ•—ï¼Œç¹¼çºŒå¾ŒçºŒæµç¨‹")
        
        # åŸ·è¡Œæ¨è«–æ¸¬è©¦ (placeholder)
        if do_infer:
            print("\nğŸ¤– é–‹å§‹æ¨è«–æ¸¬è©¦...")
            # é€™è£¡å¯ä»¥æ•´åˆæ¨è«–æ¸¬è©¦åŠŸèƒ½
            print("âœ… æ¨è«–æ¸¬è©¦å®Œæˆ (placeholder)")
        
        print(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆ!")
        print(f"   - ç·¨è­¯: {'âœ…' if compile_success else 'âŒ'}")
        print(f"   - åˆ†æ: {'âœ…' if profile_success else 'âŒ'}")
        print(f"   - æ¨è«–: {'âœ…' if do_infer else 'â­ï¸'}")
        
        return compile_success and (not do_profile or profile_success)
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        å–å¾—ç•¶å‰æ¨¡å‹è³‡è¨Š
        
        Returns:
            æ¨¡å‹è³‡è¨Šå­—å…¸
        """
        return {
            'scanned_models': self.current_models,
            'qaihub_models': self.qaihub_client.qai_hub_models,
            'device': self.qaihub_client.target_device.name if self.qaihub_client.target_device else None
        }
    
    def run_compile_profile_pipeline(self, do_infer: bool = False) -> bool:
        """
        åŸ·è¡Œç·¨è­¯+åˆ†æå·¥ä½œæµç¨‹
        
        Args:
            do_infer: æ˜¯å¦åŸ·è¡Œæ¨è«–æ¸¬è©¦
            
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹ç·¨è­¯+åˆ†æå·¥ä½œæµç¨‹ (Infer: {do_infer})")
        
        # åŸ·è¡Œç·¨è­¯æµç¨‹
        compile_success = self.run_compile_pipeline('onnx')
        if not compile_success:
            print("âŒ ç·¨è­¯æµç¨‹å¤±æ•—ï¼Œå®Œæ•´æµç¨‹ä¸­æ­¢")
            return False
        
        # åŸ·è¡Œåˆ†ææµç¨‹
        profile_success = self.run_profile_pipeline()
        if not profile_success:
            print("âš ï¸ åˆ†ææµç¨‹å¤±æ•—ï¼Œç¹¼çºŒå¾ŒçºŒæµç¨‹")
        
        # åŸ·è¡Œæ¨è«–æ¸¬è©¦ (placeholder)
        if do_infer:
            print("\nğŸ¤– é–‹å§‹æ¨è«–æ¸¬è©¦...")
            # é€™è£¡å¯ä»¥æ•´åˆæ¨è«–æ¸¬è©¦åŠŸèƒ½
            print("âœ… æ¨è«–æ¸¬è©¦å®Œæˆ (placeholder)")
        
        print(f"\nğŸ‰ ç·¨è­¯+åˆ†æå·¥ä½œæµç¨‹å®Œæˆ!")
        print(f"   - ç·¨è­¯: {'âœ…' if compile_success else 'âŒ'}")
        print(f"   - åˆ†æ: {'âœ…' if profile_success else 'âŒ'}")
        print(f"   - æ¨è«–: {'âœ…' if do_infer else 'â­ï¸'}")
        
        return compile_success and profile_success


# å–®ä¾‹æ¨¡å¼å¯¦ä¾‹
_pipeline_instance = None

def get_pipeline(base_dir: Optional[Path] = None) -> QAIHubPipeline:
    """
    å–å¾—å·¥ä½œæµç¨‹ç®¡é“å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        base_dir: åŸºç¤ç›®éŒ„è·¯å¾‘
        
    Returns:
        QAIHubPipeline å¯¦ä¾‹
    """
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = QAIHubPipeline(base_dir)
    return _pipeline_instance
