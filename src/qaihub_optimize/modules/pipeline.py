"""
å·¥ä½œæµç¨‹ç®¡é“æ¨¡çµ„
è² è²¬ç®¡ç†å®Œæ•´çš„ QAI Hub æœ€ä½³åŒ–å·¥ä½œæµç¨‹
"""
import time
import threading
from pathlib import Path
from typing import Dict, List, Optional, Callable, Any
import sys

from .scanner import ModelScanner
from .conversion import ModelConverter
from .format_check import FormatChecker
from .qaihub_client import QAIHubClient
from .job_monitor import get_job_monitor
from .preprocessor import get_model_preprocessor, preprocess_models

# é…ç½®å¸¸æ•¸ - å¾ç’°å¢ƒè®Šæ•¸è®€å–
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

MODEL_SOURCE_DIR = os.getenv('MODEL_SOURCE_DIR', 'raw')  # åŸå§‹æ¨¡å‹ç›®éŒ„åç¨±
OPTIMIZED_MODEL_DIR = os.getenv('OPTIMIZED_MODEL_DIR', 'qaihub_optimized')  # å„ªåŒ–æ¨¡å‹ç›®éŒ„åç¨±
ONNX_MODEL_DIR = os.getenv('ONNX_MODEL_DIR', 'onnx')  # ONNX æ¨¡å‹ç›®éŒ„åç¨±


class QAIHubPipeline:
    """QAI Hub å·¥ä½œæµç¨‹ç®¡é“ç®¡ç†é¡åˆ¥"""
    
    def __init__(self, base_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµç¨‹ç®¡é“
        
        Args:
            base_dir: åŸºç¤ç›®éŒ„è·¯å¾‘
        """
        self.base_dir = base_dir or Path.cwd()
        
        # ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„è·¯å¾‘é…ç½®
        models_base_dir_env = os.getenv('MODELS_BASE_DIR')
        if models_base_dir_env:
            models_base_dir = Path(models_base_dir_env)
            print(f"ğŸ“ ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„æ¨¡å‹åŸºç¤ç›®éŒ„: {models_base_dir}")
        else:
            # ä½¿ç”¨å‹•æ…‹è·¯å¾‘ï¼Œå¾ç•¶å‰å·¥ä½œç›®éŒ„å‘ä¸Šæ‰¾åˆ°é …ç›®æ ¹ç›®éŒ„
            current_path = Path.cwd()
            project_root = current_path
            # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ src/qaihub_optimize/modulesï¼Œå‰‡å‘ä¸Šæ‰¾åˆ°é …ç›®æ ¹ç›®éŒ„
            if 'src' in current_path.parts and 'qaihub_optimize' in current_path.parts:
                project_root = current_path.parent.parent.parent
            models_base_dir = project_root / 'src' / 'models'
            print(f"ğŸ“ ä½¿ç”¨å‹•æ…‹è¨ˆç®—çš„æ¨¡å‹åŸºç¤ç›®éŒ„: {models_base_dir}")
        
        print(f"ğŸ“ é …ç›®æ ¹ç›®éŒ„: {project_root if 'project_root' in locals() else 'N/A'}")
        print(f"ğŸ“ models ç›®éŒ„: {models_base_dir}")
        print(f"ğŸ“ models ç›®éŒ„æ˜¯å¦å­˜åœ¨: {models_base_dir.exists()}")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        models_base_dir.mkdir(parents=True, exist_ok=True)
        
        self.scanner = ModelScanner(models_base_dir)
        self.converter = ModelConverter()
        self.format_checker = FormatChecker(models_base_dir)
        self.qaihub_client = QAIHubClient(models_base_dir)
        self.job_monitor = get_job_monitor(self.qaihub_client)
        self.current_models = {}
    
    def scan_models(self) -> Dict[str, List[Path]]:
        """
        æƒææ¨¡å‹æª”æ¡ˆ
        
        Returns:
            æƒæåˆ°çš„æ¨¡å‹æª”æ¡ˆå­—å…¸
        """
        print("ğŸ” é–‹å§‹æƒææ¨¡å‹æª”æ¡ˆ...")
        try:
            models = self.scanner.scan_model_directory(MODEL_SOURCE_DIR)
            self.current_models = models
            return models
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return {}
        except Exception as e:
            print(f"âŒ æƒææ¨¡å‹å¤±æ•—: {e}")
            return {}
    
    def convert_tflite_models(self, ask_user: bool = True) -> bool:
        """
        è½‰æ› TFLite æ¨¡å‹åˆ° ONNX
        
        Args:
            ask_user: æ˜¯å¦è©¢å•ä½¿ç”¨è€…
            
        Returns:
            è½‰æ›æ˜¯å¦æˆåŠŸ
        """
        tflite_files = self.current_models.get('tflite', [])
        if not tflite_files:
            print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° TFLite æ¨¡å‹éœ€è¦è½‰æ›")
            return True
        
        print(f"ğŸ”„ ç™¼ç¾ {len(tflite_files)} å€‹ TFLite æ¨¡å‹éœ€è¦è½‰æ›")
        
        if ask_user:
            response = input(f"æ˜¯å¦è¦å°‡ {len(tflite_files)} å€‹ TFLite æ¨¡å‹è½‰æ›ç‚º ONNXï¼Ÿ(y/n): ").strip().lower()
            if response != 'y':
                print("â­ï¸ è·³é TFLite è½‰æ›")
                return True
        
        success_count = 0
        failed_files = []
        
        for tflite_path in tflite_files:
            try:
                result = self.converter.convert_tflite_to_onnx(tflite_path, self.base_dir / 'models' / ONNX_MODEL_DIR)
                if result['status'] == 'ok':
                    success_count += 1
                    print(f"âœ… è½‰æ›æˆåŠŸ: {tflite_path.name}")
                else:
                    failed_files.append(tflite_path.name)
                    print(f"âŒ è½‰æ›å¤±æ•—: {tflite_path.name} - {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            except Exception as e:
                failed_files.append(tflite_path.name)
                print(f"âŒ è½‰æ›ç•°å¸¸: {tflite_path.name} - {e}")
        
        print(f"\nğŸ“Š TFLite è½‰æ›çµæœ: {success_count} æˆåŠŸ, {len(failed_files)} å¤±æ•—")
        if failed_files:
            print("âŒ è½‰æ›å¤±æ•—çš„æª”æ¡ˆ:")
            for filename in failed_files:
                print(f"   - {filename}")
        
        return len(failed_files) == 0
    
    def check_and_fix_onnx_models(self) -> bool:
        """
        æª¢æŸ¥ä¸¦ä¿®å¾© ONNX æ¨¡å‹æ ¼å¼
        
        Returns:
            æª¢æŸ¥ä¿®å¾©æ˜¯å¦æˆåŠŸ
        """
        onnx_files = self.current_models.get('onnx', [])
        if not onnx_files:
            print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° ONNX æ¨¡å‹éœ€è¦æª¢æŸ¥")
            return True
        
        print(f"ğŸ”§ æª¢æŸ¥ {len(onnx_files)} å€‹ ONNX æ¨¡å‹æ ¼å¼...")
        
        invalid_models = []
        fixed_count = 0
        
        for onnx_path in onnx_files:
            try:
                # æª¢æŸ¥æ¨¡å‹æ ¼å¼
                error = self.format_checker.check_onnx_model(onnx_path)
                if error:
                    print(f"âš ï¸ æ ¼å¼ç•°å¸¸: {onnx_path.name} - {error}")
                    invalid_models.append((onnx_path.name, error))
                    
                    # å˜—è©¦ä¿®å¾© value_info
                    if "value_info" in str(error).lower():
                        fixed = self.format_checker.fix_onnx_value_info(onnx_path)
                        if fixed:
                            fixed_count += 1
                            print(f"âœ… ä¿®å¾©æˆåŠŸ: {onnx_path.name}")
                        else:
                            print(f"âŒ ä¿®å¾©å¤±æ•—: {onnx_path.name}")
                else:
                    print(f"âœ… æ ¼å¼æ­£å¸¸: {onnx_path.name}")
                    
            except Exception as e:
                print(f"âŒ æª¢æŸ¥ç•°å¸¸: {onnx_path.name} - {e}")
                invalid_models.append((onnx_path.name, str(e)))
        
        print(f"\nğŸ“Š ONNX æª¢æŸ¥çµæœ: {fixed_count} å€‹ä¿®å¾©, {len(invalid_models)} å€‹ç•°å¸¸")
        
        if invalid_models:
            print("âŒ æ ¼å¼ç•°å¸¸çš„æ¨¡å‹:")
            for filename, error in invalid_models:
                print(f"   - {filename}: {error}")
            return False
        
        return True
    
    def run_compile_pipeline(self, source: str = 'onnx') -> bool:
        """
        åŸ·è¡Œç·¨è­¯å·¥ä½œæµç¨‹ï¼ˆè‡ªå‹•æ ¹æ“šç›®æ¨™è£ç½®æ”¯æ´æ±ºå®šä¾†æºé¡å‹ï¼‰
        
        Args:
            source: æ¨¡å‹ä¾†æºé¡å‹ ('onnx', 'tflite', 'dlc')
            
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹ç·¨è­¯å·¥ä½œæµç¨‹ (ä¾†æº: {source})")
        
        # åŸ·è¡Œæ¨¡å‹é è™•ç†ï¼ˆè‡ªå‹•è½‰æ›å’Œç§»å‹•åˆ° onnx ç›®éŒ„ï¼‰
        preprocess_result = preprocess_models()
        if not preprocess_result:
            print("âŒ æ¨¡å‹é è™•ç†å¤±æ•—")
            return False
        
        # æƒæ onnx ç›®éŒ„ä¸­çš„æ¨¡å‹
        onnx_models = self.scanner.scan_model_directory(ONNX_MODEL_DIR)
        if not onnx_models.get('onnx'):
            print("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„ ONNX æ¨¡å‹æª”æ¡ˆ")
            return False
        
        self.current_models = onnx_models
        print(f"âœ… æ‰¾åˆ° {len(onnx_models['onnx'])} å€‹ ONNX æ¨¡å‹æª”æ¡ˆ")
        
        # æª¢æŸ¥è£ç½®æ”¯æ´çš„æ ¼å¼
        support_info = self.qaihub_client.check_device_support()
        print(f"\nğŸ“‹ è£ç½®æ”¯æ´æ ¼å¼æª¢æŸ¥:")
        for framework, supported in support_info.items():
            status = "âœ…" if supported else "âŒ"
            print(f"   {status} {framework.upper()}: {'æ”¯æ´' if supported else 'ä¸æ”¯æ´'}")
        
        # ç¾åœ¨åªä½¿ç”¨ ONNX æ ¼å¼ï¼ˆç¶“éé è™•ç†å¾Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨ onnx ç›®éŒ„ä¸­ï¼‰
        source = 'onnx'
        print("âœ… ä½¿ç”¨ ONNX æ ¼å¼ï¼ˆç¶“éé è™•ç†å¾Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨ onnx ç›®éŒ„ä¸­ï¼‰")
        
        print(f"ğŸ“‹ æ¨¡å‹ä¾†æº: {source}")
        
        # æª¢æŸ¥ ONNX æ¨¡å‹æ ¼å¼ï¼ˆå¦‚æœä½¿ç”¨ ONNXï¼‰
        if source == 'onnx':
            if not self.check_and_fix_onnx_models():
                print("âŒ ONNX æ ¼å¼æª¢æŸ¥å¤±æ•—ï¼Œæµç¨‹ä¸­æ­¢")
                return False
        
        # åœ¨è¼‰å…¥æ¨¡å‹å‰å†æ¬¡æª¢æŸ¥ä¸¦ä¿®å¾© value_info å•é¡Œ
        print("ğŸ”§ è¼‰å…¥å‰å†æ¬¡æª¢æŸ¥ä¸¦ä¿®å¾© ONNX æ¨¡å‹ value_info å•é¡Œ...")
        onnx_files = self.current_models.get('onnx', [])
        for onnx_path in onnx_files:
            error = self.format_checker.check_onnx_model(onnx_path)
            if error and "value_info" in str(error).lower():
                print(f"âš ï¸  ç™¼ç¾ value_info å•é¡Œ: {onnx_path.name}")
                fixed = self.format_checker.fix_onnx_value_info(onnx_path)
                if fixed:
                    print(f"âœ… ä¿®å¾©æˆåŠŸ: {onnx_path.name}")
                else:
                    print(f"âŒ ä¿®å¾©å¤±æ•—: {onnx_path.name}")
        
        # è¼‰å…¥æ¨¡å‹åˆ° QAI Hub å®¢æˆ¶ç«¯
        ext_map = {
            'onnx': '.onnx',
            'tflite': '.tflite', 
            'dlc': '.dlc'
        }
        
        if source not in ext_map:
            print(f"âŒ ä¸æ”¯æ´çš„ä¾†æºé¡å‹: {source}")
            return False
        
        loaded = self.qaihub_client.load_models(source, ONNX_MODEL_DIR, ext_map[source])
        if not loaded:
            print("âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—")
            return False
        
        # ä¸Šå‚³æ¨¡å‹åˆ° QAI Hub
        if not self.qaihub_client.upload_models():
            print("âŒ ä¸Šå‚³æ¨¡å‹å¤±æ•—")
            return False
        
        # æäº¤ç·¨è­¯ä»»å‹™ï¼ˆä¸å‚³éé‡åŒ–é¸é …ï¼‰
        if not self.qaihub_client.submit_compilation_jobs():
            print("âŒ æäº¤ç·¨è­¯ä»»å‹™å¤±æ•—")
            return False
        
        # ä½¿ç”¨æ–°çš„ job monitor ç­‰å¾…ç·¨è­¯å®Œæˆï¼ˆå¯¦æ™‚é¡¯ç¤ºç‹€æ…‹ä¸¦è‡ªå‹•ä¸‹è¼‰å„ªåŒ–æ¨¡å‹ï¼‰
        print("\nğŸ” é–‹å§‹ç›£æ§ç·¨è­¯ä»»å‹™ç‹€æ…‹...")
        compile_success = self.job_monitor.wait_for_compile_jobs(
            self.qaihub_client.qai_hub_models, 
            timeout_minutes=30
        )
        
        # ç”¢ç”Ÿå ±å‘Š
        report_file = self.qaihub_client.generate_html_report('compile')
        
        # æª¢æŸ¥å„ªåŒ–æ¨¡å‹ä¸‹è¼‰ç‹€æ…‹
        downloaded_models = []
        for model_name, model_info in self.qaihub_client.qai_hub_models.items():
            if model_info.get('optimized_model_downloaded', False):
                downloaded_models.append(model_name)
        
        print(f"\nğŸ¯ ç·¨è­¯æµç¨‹å®Œæˆ: {'æˆåŠŸ' if compile_success else 'å¤±æ•—'}")
        print(f"ğŸ“Š è©³ç´°å ±å‘Š: {report_file}")
        
        if downloaded_models:
            print(f"ğŸ’¾ å·²ä¸‹è¼‰å„ªåŒ–æ¨¡å‹ ({len(downloaded_models)} å€‹):")
            for model_name in downloaded_models:
                model_path = self.qaihub_client.qai_hub_models[model_name].get('optimized_model_path', 'æœªçŸ¥è·¯å¾‘')
                quantize_info = f" (é‡åŒ–: {model_info.get('quantize', 'ç„¡')})" if model_info.get('quantize') else ""
                print(f"   - {model_name} -> {model_path}{quantize_info}")
        else:
            print("âš ï¸  æ²’æœ‰ä¸‹è¼‰å„ªåŒ–æ¨¡å‹")
        
        return compile_success
    
    def run_profile_pipeline(self) -> bool:
        """
        åŸ·è¡Œæ•ˆèƒ½åˆ†æå·¥ä½œæµç¨‹
        
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹æ•ˆèƒ½åˆ†æå·¥ä½œæµç¨‹")
        
        # åŸ·è¡Œæ¨¡å‹é è™•ç†ï¼ˆè‡ªå‹•è½‰æ›å’Œç§»å‹•åˆ° onnx ç›®éŒ„ï¼‰
        preprocess_result = preprocess_models()
        if not preprocess_result:
            print("âŒ æ¨¡å‹é è™•ç†å¤±æ•—")
            return False
        
        # æƒæ onnx ç›®éŒ„ä¸­çš„æ¨¡å‹
        onnx_models = self.scanner.scan_model_directory(ONNX_MODEL_DIR)
        if not onnx_models.get('onnx'):
            print("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„ ONNX æ¨¡å‹æª”æ¡ˆ")
            return False
        
        self.current_models = onnx_models
        print(f"âœ… æ‰¾åˆ° {len(onnx_models['onnx'])} å€‹ ONNX æ¨¡å‹æª”æ¡ˆ")
        
        # æª¢æŸ¥è£ç½®æ”¯æ´çš„æ ¼å¼
        support_info = self.qaihub_client.check_device_support()
        print(f"\nğŸ“‹ è£ç½®æ”¯æ´æ ¼å¼æª¢æŸ¥:")
        for framework, supported in support_info.items():
            status = "âœ…" if supported else "âŒ"
            print(f"   {status} {framework.upper()}: {'æ”¯æ´' if supported else 'ä¸æ”¯æ´'}")
        
        # ç¾åœ¨åªä½¿ç”¨ ONNX æ ¼å¼ï¼ˆç¶“éé è™•ç†å¾Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨ onnx ç›®éŒ„ä¸­ï¼‰
        source = 'onnx'
        print("âœ… ä½¿ç”¨ ONNX æ ¼å¼ï¼ˆç¶“éé è™•ç†å¾Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨ onnx ç›®éŒ„ä¸­ï¼‰")
        
        print(f"ğŸ“‹ æ¨¡å‹ä¾†æº: {source}")
        
        # æª¢æŸ¥ ONNX æ¨¡å‹æ ¼å¼
        if not self.check_and_fix_onnx_models():
            print("âŒ ONNX æ ¼å¼æª¢æŸ¥å¤±æ•—ï¼Œæµç¨‹ä¸­æ­¢")
            return False
        
        # åœ¨è¼‰å…¥æ¨¡å‹å‰å†æ¬¡æª¢æŸ¥ä¸¦ä¿®å¾© value_info å•é¡Œ
        print("ğŸ”§ è¼‰å…¥å‰å†æ¬¡æª¢æŸ¥ä¸¦ä¿®å¾© ONNX æ¨¡å‹ value_info å•é¡Œ...")
        onnx_files = self.current_models.get('onnx', [])
        for onnx_path in onnx_files:
            error = self.format_checker.check_onnx_model(onnx_path)
            if error and "value_info" in str(error).lower():
                print(f"âš ï¸  ç™¼ç¾ value_info å•é¡Œ: {onnx_path.name}")
                fixed = self.format_checker.fix_onnx_value_info(onnx_path)
                if fixed:
                    print(f"âœ… ä¿®å¾©æˆåŠŸ: {onnx_path.name}")
                else:
                    print(f"âŒ ä¿®å¾©å¤±æ•—: {onnx_path.name}")
        
        # è¼‰å…¥æ¨¡å‹åˆ° QAI Hub å®¢æˆ¶ç«¯
        loaded = self.qaihub_client.load_models(source, ONNX_MODEL_DIR, '.onnx')
        if not loaded:
            print("âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—")
            return False
        
        # ä¸Šå‚³æ¨¡å‹åˆ° QAI Hub
        if not self.qaihub_client.upload_models():
            print("âŒ ä¸Šå‚³æ¨¡å‹å¤±æ•—")
            return False
        
        # æäº¤åˆ†æä»»å‹™
        if not self.qaihub_client.submit_profile_jobs():
            print("âŒ æäº¤åˆ†æä»»å‹™å¤±æ•—")
            return False
        
        # ä½¿ç”¨æ–°çš„ job monitor ç­‰å¾…åˆ†æå®Œæˆï¼ˆå¯¦æ™‚é¡¯ç¤ºç‹€æ…‹ï¼‰
        print("\nğŸ” é–‹å§‹ç›£æ§åˆ†æä»»å‹™ç‹€æ…‹...")
        profile_success = self.job_monitor.wait_for_profile_jobs(
            self.qaihub_client.qai_hub_models, 
            timeout_minutes=30
        )
        
        # ç”¢ç”Ÿå ±å‘Š
        report_file = self.qaihub_client.generate_html_report('profile')
        
        print(f"\nğŸ¯ åˆ†ææµç¨‹å®Œæˆ: {'æˆåŠŸ' if profile_success else 'å¤±æ•—'}")
        print(f"ğŸ“Š è©³ç´°å ±å‘Š: {report_file}")
        
        return profile_success
    
    def run_full_pipeline(self, do_profile: bool = True, do_infer: bool = False) -> bool:
        """
        åŸ·è¡Œå®Œæ•´å·¥ä½œæµç¨‹ (ç·¨è­¯ + åˆ†æ + æ¨è«–)
        
        Args:
            do_profile: æ˜¯å¦åŸ·è¡Œæ•ˆèƒ½åˆ†æ
            do_infer: æ˜¯å¦åŸ·è¡Œæ¨è«–æ¸¬è©¦
            
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹å®Œæ•´å·¥ä½œæµç¨‹ (Profile: {do_profile}, Infer: {do_infer})")
        
        # åŸ·è¡Œç·¨è­¯æµç¨‹
        compile_success = self.run_compile_pipeline()
        if not compile_success:
            print("âŒ ç·¨è­¯æµç¨‹å¤±æ•—ï¼Œå®Œæ•´æµç¨‹ä¸­æ­¢")
            return False
        
        # åŸ·è¡Œåˆ†ææµç¨‹
        profile_success = True
        if do_profile:
            profile_success = self.run_profile_pipeline()
            if not profile_success:
                print("âš ï¸ åˆ†ææµç¨‹å¤±æ•—ï¼Œç¹¼çºŒå¾ŒçºŒæµç¨‹")
        
        # åŸ·è¡Œæ¨è«–æ¸¬è©¦ (placeholder)
        if do_infer:
            print("\nğŸ¤– é–‹å§‹æ¨è«–æ¸¬è©¦...")
            # é€™è£¡å¯ä»¥æ•´åˆæ¨è«–æ¸¬è©¦åŠŸèƒ½
            print("âœ… æ¨è«–æ¸¬è©¦å®Œæˆ (placeholder)")
        
        print(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆ!")
        print(f"   - ç·¨è­¯: {'âœ…' if compile_success else 'âŒ'}")
        print(f"   - åˆ†æ: {'âœ…' if profile_success else 'âŒ'}")
        print(f"   - æ¨è«–: {'âœ…' if do_infer else 'â­ï¸'}")
        
        return compile_success and (not do_profile or profile_success)
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        å–å¾—ç•¶å‰æ¨¡å‹è³‡è¨Š
        
        Returns:
            æ¨¡å‹è³‡è¨Šå­—å…¸
        """
        return {
            'scanned_models': self.current_models,
            'qaihub_models': self.qaihub_client.qai_hub_models,
            'device': self.qaihub_client.target_device.name if self.qaihub_client.target_device else None
        }
    
    def run_compile_profile_pipeline(self, do_infer: bool = False) -> bool:
        """
        åŸ·è¡Œç·¨è­¯+åˆ†æå·¥ä½œæµç¨‹
        
        Args:
            do_infer: æ˜¯å¦åŸ·è¡Œæ¨è«–æ¸¬è©¦
            
        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ é–‹å§‹ç·¨è­¯+åˆ†æå·¥ä½œæµç¨‹ (Infer: {do_infer})")
        
        # åŸ·è¡Œç·¨è­¯æµç¨‹
        compile_success = self.run_compile_pipeline()
        if not compile_success:
            print("âŒ ç·¨è­¯æµç¨‹å¤±æ•—ï¼Œå®Œæ•´æµç¨‹ä¸­æ­¢")
            return False
        
        # åŸ·è¡Œåˆ†ææµç¨‹
        profile_success = self.run_profile_pipeline()
        if not profile_success:
            print("âš ï¸ åˆ†ææµç¨‹å¤±æ•—ï¼Œç¹¼çºŒå¾ŒçºŒæµç¨‹")
        
        # åŸ·è¡Œæ¨è«–æ¸¬è©¦ (placeholder)
        if do_infer:
            print("\nğŸ¤– é–‹å§‹æ¨è«–æ¸¬è©¦...")
            # é€™è£¡å¯ä»¥æ•´åˆæ¨è«–æ¸¬è©¦åŠŸèƒ½
            print("âœ… æ¨è«–æ¸¬è©¦å®Œæˆ (placeholder)")
        
        print(f"\nğŸ‰ ç·¨è­¯+åˆ†æå·¥ä½œæµç¨‹å®Œæˆ!")
        print(f"   - ç·¨è­¯: {'âœ…' if compile_success else 'âŒ'}")
        print(f"   - åˆ†æ: {'âœ…' if profile_success else 'âŒ'}")
        print(f"   - æ¨è«–: {'âœ…' if do_infer else 'â­ï¸'}")
        
        return compile_success and profile_success


# å–®ä¾‹æ¨¡å¼å¯¦ä¾‹
_pipeline_instance = None

def get_pipeline(base_dir: Optional[Path] = None) -> QAIHubPipeline:
    """
    å–å¾—å·¥ä½œæµç¨‹ç®¡é“å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        base_dir: åŸºç¤ç›®éŒ„è·¯å¾‘
        
    Returns:
        QAIHubPipeline å¯¦ä¾‹
    """
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = QAIHubPipeline(base_dir)
    return _pipeline_instance
