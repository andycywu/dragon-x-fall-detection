#!/usr/bin/env python3
"""
ğŸ¯ å¯¦ç”¨QAI Hub + ONNX Runtimeå¯¦éš›æ‡‰ç”¨ç³»çµ±
å°ˆæ³¨æ–¼å¯¦éš›å¯è¡Œçš„MediaPipeæ¨¡å‹é›†æˆ
"""

import os
import qai_hub as hub
import numpy as np
import cv2
import onnxruntime as ort
import torch
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional
import time
import json
import tempfile

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PracticalQAIHubONNX:
    """å¯¦ç”¨çš„QAI Hub + ONNX Runtimeç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»çµ±"""
        self.api_token = os.getenv('QAI_HUB_API_TOKEN')
        if not self.api_token:
            raise ValueError("âŒ è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®QAI_HUB_API_TOKEN")
        
        # ONNX Runtimeé…ç½®
        self.onnx_providers = self._setup_onnx_providers()
        self.onnx_sessions = {}
        
        # QAI Hubç›¸é—œ
        self.qai_hub_models = {}
        self.upload_jobs = {}
        
        logger.info("ğŸš€ åˆå§‹åŒ–å¯¦ç”¨QAI Hub + ONNXç³»çµ±...")
        self._verify_qai_hub_connection()
        
    def _verify_qai_hub_connection(self):
        """é©—è­‰QAI Hubé€£æ¥"""
        try:
            devices = hub.get_devices()
            logger.info(f"âœ… QAI Hubé€£æ¥æˆåŠŸï¼Œå¯ç”¨è¨­å‚™: {len(devices)}")
            
            # é¸æ“‡Snapdragonè¨­å‚™ä½œç‚ºç›®æ¨™
            snapdragon_devices = [d for d in devices if 'Snapdragon' in d.name or 'Samsung' in d.name]
            if snapdragon_devices:
                self.target_device = snapdragon_devices[0]
                logger.info(f"ğŸ¯ é¸æ“‡ç›®æ¨™è¨­å‚™: {self.target_device.name}")
            else:
                self.target_device = devices[0] if devices else None
                logger.info(f"ğŸ¯ ä½¿ç”¨è¨­å‚™: {self.target_device.name if self.target_device else 'None'}")
                
        except Exception as e:
            logger.error(f"âŒ QAI Hubé€£æ¥å¤±æ•—: {e}")
            raise
    
    def _setup_onnx_providers(self):
        """è¨­ç½®ONNX Runtimeæä¾›å•†"""
        providers = []
        available = ort.get_available_providers()
        
        logger.info(f"ğŸ“‹ å¯ç”¨ONNXåŸ·è¡Œæä¾›å•†: {available}")
        
        # å„ªå…ˆç´š: CUDA > DirectML > CPU
        if 'CUDAExecutionProvider' in available:
            providers.append('CUDAExecutionProvider')
            logger.info("âœ… å•Ÿç”¨CUDA GPUåŠ é€Ÿ")
        elif 'DmlExecutionProvider' in available:
            providers.append('DmlExecutionProvider')
            logger.info("âœ… å•Ÿç”¨DirectML GPUåŠ é€Ÿ")
        
        providers.append('CPUExecutionProvider')
        logger.info("âœ… CPUåŸ·è¡Œæä¾›å•†å·²æ·»åŠ ")
        
        return providers
    
    def load_mediapipe_models(self):
        """è¼‰å…¥MediaPipeæ¨¡å‹"""
        logger.info("ğŸ“¥ è¼‰å…¥QAI Hub MediaPipeæ¨¡å‹...")
        
        models_to_load = {
            'face': {
                'module': 'qai_hub_models.models.mediapipe_face',
                'class': 'Model',
                'description': 'MediaPipe Face Detection',
                'input_size': (192, 192)
            },
            'pose': {
                'module': 'qai_hub_models.models.mediapipe_pose',
                'class': 'Model',
                'description': 'MediaPipe Pose Estimation',
                'input_size': (256, 256)
            },
            'hand': {
                'module': 'qai_hub_models.models.mediapipe_hand',
                'class': 'Model',
                'description': 'MediaPipe Hand Detection',
                'input_size': (224, 224)
            }
        }
        
        for model_name, config in models_to_load.items():
            try:
                logger.info(f"ğŸ“± è¼‰å…¥ {config['description']}...")
                
                # å‹•æ…‹å°å…¥æ¨¡å‹
                module = __import__(config['module'], fromlist=[config['class']])
                ModelClass = getattr(module, config['class'])
                
                # å‰µå»ºé è¨“ç·´æ¨¡å‹
                model = ModelClass.from_pretrained()
                
                self.qai_hub_models[model_name] = {
                    'model': model,
                    'config': config,
                    'loaded': True
                }
                
                logger.info(f"âœ… {config['description']} è¼‰å…¥æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} è¼‰å…¥å¤±æ•—: {e}")
                self.qai_hub_models[model_name] = {
                    'model': None,
                    'config': config,
                    'loaded': False,
                    'error': str(e)
                }
    
    def export_models_to_torchscript(self):
        """å°‡æ¨¡å‹å°å‡ºç‚ºTorchScriptæ ¼å¼ï¼ˆQAI Hubæ”¯æ´ï¼‰"""
        logger.info("ğŸ“¤ å°å‡ºæ¨¡å‹ç‚ºTorchScriptæ ¼å¼...")
        
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info['loaded']:
                continue
                
            try:
                model = model_info['model']
                config = model_info['config']
                
                logger.info(f"ğŸ”„ å°å‡º {config['description']} ç‚ºTorchScript...")
                
                # æº–å‚™ç¤ºä¾‹è¼¸å…¥
                input_size = config['input_size']
                sample_input = torch.randn(1, 3, input_size[1], input_size[0])
                
                # è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
                model.eval()
                
                # å°å‡ºç‚ºTorchScript
                with torch.no_grad():
                    traced_model = torch.jit.trace(model, sample_input)
                
                # ä¿å­˜TorchScriptæ¨¡å‹
                torchscript_path = f"qai_hub_{model_name}_model.pt"
                traced_model.save(torchscript_path)
                
                logger.info(f"âœ… {config['description']} TorchScriptå·²ä¿å­˜: {torchscript_path}")
                
                # æ›´æ–°æ¨¡å‹ä¿¡æ¯
                model_info['torchscript_path'] = torchscript_path
                model_info['sample_input_shape'] = sample_input.shape
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} TorchScriptå°å‡ºå¤±æ•—: {e}")
                model_info['export_error'] = str(e)
    
    def upload_models_to_qai_hub(self):
        """ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub"""
        logger.info("â˜ï¸ ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub...")
        
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info.get('torchscript_path'):
                logger.warning(f"âš ï¸ {model_name} æ²’æœ‰TorchScriptæ–‡ä»¶ï¼Œè·³éä¸Šå‚³")
                continue
                
            try:
                torchscript_path = model_info['torchscript_path']
                config = model_info['config']
                
                logger.info(f"ğŸ“¤ ä¸Šå‚³ {config['description']} åˆ°QAI Hub...")
                
                # ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub
                uploaded_model = hub.upload_model(torchscript_path)
                
                logger.info(f"âœ… {config['description']} ä¸Šå‚³æˆåŠŸ")
                logger.info(f"   æ¨¡å‹ID: {uploaded_model.model_id}")
                
                # ä¿å­˜ä¸Šå‚³çš„æ¨¡å‹å¼•ç”¨
                model_info['qai_hub_model'] = uploaded_model
                model_info['model_id'] = uploaded_model.model_id
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ä¸Šå‚³å¤±æ•—: {e}")
                model_info['upload_error'] = str(e)
    
    def submit_compilation_jobs(self):
        """æäº¤ç·¨è­¯Jobs"""
        logger.info("ğŸ”„ æäº¤æ¨¡å‹ç·¨è­¯Jobsåˆ°QAI Hub...")
        
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info.get('qai_hub_model'):
                logger.warning(f"âš ï¸ {model_name} æ²’æœ‰ä¸Šå‚³åˆ°QAI Hubï¼Œè·³éç·¨è­¯")
                continue
                
            try:
                qai_model = model_info['qai_hub_model']
                config = model_info['config']
                input_size = config['input_size']
                
                logger.info(f"ğŸš€ æäº¤ {config['description']} ç·¨è­¯Job...")
                
                # è¨­ç½®è¼¸å…¥è¦æ ¼
                input_specs = {
                    "image": ((1, 3, input_size[1], input_size[0]), "float32")
                }
                
                # æäº¤ç·¨è­¯Job
                compile_job = hub.submit_compile_job(
                    model=qai_model,
                    input_specs=input_specs,
                    device=self.target_device
                )
                
                logger.info(f"âœ… {config['description']} ç·¨è­¯Jobå·²æäº¤")
                logger.info(f"   Job ID: {compile_job.job_id}")
                logger.info(f"   Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                
                # ä¿å­˜Jobå¼•ç”¨
                model_info['compile_job'] = compile_job
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ç·¨è­¯Jobæäº¤å¤±æ•—: {e}")
                model_info['compile_error'] = str(e)
    
    def convert_torchscript_to_onnx(self):
        """å°‡TorchScriptæ¨¡å‹è½‰æ›ç‚ºONNX"""
        logger.info("ğŸ”„ è½‰æ›TorchScriptæ¨¡å‹ç‚ºONNX...")
        
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info.get('torchscript_path'):
                continue
                
            try:
                config = model_info['config']
                input_size = config['input_size']
                
                logger.info(f"ğŸ”„ è½‰æ› {config['description']} ç‚ºONNX...")
                
                # è¼‰å…¥TorchScriptæ¨¡å‹
                torchscript_model = torch.jit.load(model_info['torchscript_path'])
                
                # æº–å‚™ç¤ºä¾‹è¼¸å…¥
                sample_input = torch.randn(1, 3, input_size[1], input_size[0])
                
                # å°å‡ºç‚ºONNX
                onnx_path = f"qai_hub_{model_name}_model.onnx"
                torch.onnx.export(
                    torchscript_model,
                    sample_input,
                    onnx_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['image'],
                    output_names=['output'],
                    dynamic_axes={
                        'image': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                logger.info(f"âœ… {config['description']} ONNXå·²ä¿å­˜: {onnx_path}")
                
                # è¼‰å…¥ONNX Runtimeæœƒè©±
                self._load_onnx_session(model_name, onnx_path, model_info)
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ONNXè½‰æ›å¤±æ•—: {e}")
                model_info['onnx_error'] = str(e)
    
    def _load_onnx_session(self, model_name: str, onnx_path: str, model_info: Dict):
        """è¼‰å…¥ONNX Runtimeæœƒè©±"""
        try:
            config = model_info['config']
            logger.info(f"ğŸ”„ è¼‰å…¥ {model_name} ONNX Runtimeæœƒè©±...")
            
            # è¨­ç½®æœƒè©±é¸é …
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # å‰µå»ºæ¨ç†æœƒè©±
            session = ort.InferenceSession(
                onnx_path,
                sess_options=sess_options,
                providers=self.onnx_providers
            )
            
            self.onnx_sessions[model_name] = {
                'session': session,
                'config': config,
                'onnx_path': onnx_path
            }
            
            # è¨˜éŒ„æœƒè©±ä¿¡æ¯
            input_info = session.get_inputs()[0]
            output_info = session.get_outputs()[0]
            
            logger.info(f"âœ… {model_name} ONNXæœƒè©±è¼‰å…¥æˆåŠŸ")
            logger.info(f"   è¼¸å…¥: {input_info.name} {input_info.shape} {input_info.type}")
            logger.info(f"   è¼¸å‡º: {output_info.name} {output_info.shape} {output_info.type}")
            
        except Exception as e:
            logger.error(f"âŒ {model_name} ONNXæœƒè©±è¼‰å…¥å¤±æ•—: {e}")
    
    def detect_with_onnx(self, image: np.ndarray, model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨ONNX RuntimeåŸ·è¡Œæª¢æ¸¬"""
        if model_name not in self.onnx_sessions:
            return {"error": f"ONNXæœƒè©± {model_name} æœªè¼‰å…¥"}
        
        try:
            session_info = self.onnx_sessions[model_name]
            session = session_info['session']
            config = session_info['config']
            
            # é è™•ç†åœ–åƒ
            processed_image = self._preprocess_image(image, config['input_size'])
            
            # åŸ·è¡Œæ¨ç†
            input_name = session.get_inputs()[0].name
            start_time = time.time()
            outputs = session.run(None, {input_name: processed_image})
            inference_time = (time.time() - start_time) * 1000
            
            # å¾Œè™•ç†çµæœ
            results = self._postprocess_detection(outputs, model_name, config)
            results.update({
                'inference_time_ms': round(inference_time, 2),
                'model_type': f"QAI_Hub_{model_name}_ONNX",
                'description': config['description']
            })
            
            logger.info(f"âš¡ {model_name}æª¢æ¸¬å®Œæˆ: {inference_time:.2f}ms")
            return results
            
        except Exception as e:
            logger.error(f"âŒ {model_name}æª¢æ¸¬å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _preprocess_image(self, image: np.ndarray, input_size: tuple) -> np.ndarray:
        """é è™•ç†åœ–åƒ"""
        # èª¿æ•´åœ–åƒå¤§å°
        resized = cv2.resize(image, input_size)
        
        # è½‰æ›é¡è‰²ç©ºé–“ BGR -> RGB
        if len(resized.shape) == 3:
            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # æ­£è¦åŒ–åˆ° [0, 1]
        normalized = resized.astype(np.float32) / 255.0
        
        # èª¿æ•´ç¶­åº¦é †åº HWC -> CHW ä¸¦æ·»åŠ batchç¶­åº¦
        preprocessed = np.transpose(normalized, (2, 0, 1))
        preprocessed = np.expand_dims(preprocessed, axis=0)
        
        return preprocessed
    
    def _postprocess_detection(self, outputs: list, model_name: str, config: Dict) -> Dict[str, Any]:
        """å¾Œè™•ç†æª¢æ¸¬çµæœ"""
        results = {
            "success": True,
            "detections": [],
            "raw_output_shapes": [output.shape for output in outputs]
        }
        
        try:
            # æ ¹æ“šæ¨¡å‹é¡å‹è™•ç†è¼¸å‡º
            if model_name == 'face':
                results = self._process_face_outputs(outputs, results)
            elif model_name == 'pose':
                results = self._process_pose_outputs(outputs, results)
            elif model_name == 'hand':
                results = self._process_hand_outputs(outputs, results)
                
        except Exception as e:
            logger.error(f"âŒ {model_name}å¾Œè™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def _process_face_outputs(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†äººè‡‰æª¢æ¸¬è¼¸å‡º"""
        # ç°¡åŒ–çš„äººè‡‰æª¢æ¸¬çµæœè™•ç†
        if len(outputs) > 0:
            output = outputs[0]
            results["detections"] = [{
                "type": "face",
                "output_shape": output.shape,
                "detected": True
            }]
            results["total_faces"] = 1
        return results
    
    def _process_pose_outputs(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†å§¿æ…‹æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) > 0:
            output = outputs[0]
            results["detections"] = [{
                "type": "pose",
                "output_shape": output.shape,
                "detected": True
            }]
            results["total_poses"] = 1
        return results
    
    def _process_hand_outputs(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†æ‰‹éƒ¨æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) > 0:
            output = outputs[0]
            results["detections"] = [{
                "type": "hand",
                "output_shape": output.shape,
                "detected": True
            }]
            results["total_hands"] = 1
        return results
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„QAI Hub + ONNXæµæ°´ç·š"""
        pipeline_results = {
            "timestamp": time.time(),
            "steps": {},
            "final_status": {}
        }
        
        try:
            # Step 1: è¼‰å…¥æ¨¡å‹
            logger.info("ğŸ“¥ Step 1: è¼‰å…¥MediaPipeæ¨¡å‹")
            self.load_mediapipe_models()
            pipeline_results["steps"]["load_models"] = "completed"
            
            # Step 2: å°å‡ºTorchScript
            logger.info("ğŸ“¤ Step 2: å°å‡ºTorchScriptæ¨¡å‹")
            self.export_models_to_torchscript()
            pipeline_results["steps"]["export_torchscript"] = "completed"
            
            # Step 3: ä¸Šå‚³åˆ°QAI Hub
            logger.info("â˜ï¸ Step 3: ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub")
            self.upload_models_to_qai_hub()
            pipeline_results["steps"]["upload_qai_hub"] = "completed"
            
            # Step 4: æäº¤ç·¨è­¯Jobs
            logger.info("ğŸš€ Step 4: æäº¤ç·¨è­¯Jobs")
            self.submit_compilation_jobs()
            pipeline_results["steps"]["submit_compile_jobs"] = "completed"
            
            # Step 5: è½‰æ›ONNX
            logger.info("ğŸ”„ Step 5: è½‰æ›ç‚ºONNX")
            self.convert_torchscript_to_onnx()
            pipeline_results["steps"]["convert_onnx"] = "completed"
            
            # ç”Ÿæˆæœ€çµ‚ç‹€æ…‹å ±å‘Š
            pipeline_results["final_status"] = {
                "loaded_models": list(self.qai_hub_models.keys()),
                "onnx_sessions": list(self.onnx_sessions.keys()),
                "qai_hub_uploads": len([m for m in self.qai_hub_models.values() if m.get('model_id')]),
                "compile_jobs": len([m for m in self.qai_hub_models.values() if m.get('compile_job')])
            }
            
            logger.info("âœ… å®Œæ•´æµæ°´ç·šåŸ·è¡ŒæˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´ç·šåŸ·è¡Œå¤±æ•—: {e}")
            pipeline_results["error"] = str(e)
        
        return pipeline_results

def main():
    """ä¸»å‡½æ•¸ï¼šæ¼”ç¤ºå¯¦ç”¨QAI Hub + ONNXç³»çµ±"""
    print("ğŸ¯ å¯¦ç”¨QAI Hub + ONNX Runtimeç³»çµ±æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–ç³»çµ±
        system = PracticalQAIHubONNX()
        
        # é‹è¡Œå®Œæ•´æµæ°´ç·š
        results = system.run_full_pipeline()
        
        # æ¸¬è©¦æª¢æ¸¬ï¼ˆå¦‚æœONNXæœƒè©±å¯ç”¨ï¼‰
        if system.onnx_sessions:
            print("\nğŸ§ª æ¸¬è©¦ONNXæª¢æ¸¬...")
            
            test_images = ['andy.jpg', 'official_test_image.jpg']
            for img_path in test_images:
                if os.path.exists(img_path):
                    print(f"\nğŸ“· æ¸¬è©¦åœ–åƒ: {img_path}")
                    image = cv2.imread(img_path)
                    
                    if image is not None:
                        for model_name in system.onnx_sessions.keys():
                            detection_result = system.detect_with_onnx(image, model_name)
                            print(f"   {model_name}: {detection_result.get('inference_time_ms', 'N/A')}ms")
        
        # æª¢æŸ¥QAI Hub Jobç‹€æ…‹
        print("\nğŸ“Š QAI Hub Jobç‹€æ…‹:")
        for model_name, model_info in system.qai_hub_models.items():
            if 'compile_job' in model_info:
                job = model_info['compile_job']
                print(f"   {model_name}: Job {job.job_id}")
                print(f"     Dashboard: https://aihub.qualcomm.com/jobs/{job.job_id}")
        
        # ä¿å­˜çµæœ
        results_file = 'practical_qai_hub_onnx_results.json'
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼çµæœå·²ä¿å­˜åˆ° {results_file}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
