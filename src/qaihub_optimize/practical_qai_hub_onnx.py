#!/usr/bin/env python3
"""
ğŸ¯ å¯¦ç”¨QAI Hub + ONNX Runtimeå¯¦éš›æ‡‰ç”¨ç³»çµ±
å°ˆæ³¨æ–¼å¯¦éš›å¯è¡Œçš„MediaPipeæ¨¡å‹é›†æˆ
"""

import os
import qai_hub as hub
import numpy as np
import cv2
import onnxruntime as ort
import torch
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional
import time
import json
import tempfile
import onnx

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PracticalQAIHubONNX:
    def __init__(self, target_device_name: str = "Snapdragon X Elite CRD"):
        self.qai_hub_models = {}
        self.onnx_sessions = {}
        self.target_device = None
        self.target_device_name = target_device_name
        # ä¾åç¨±è‡ªå‹•é¸æ“‡ç›®æ¨™è¨­å‚™
        try:
            devices = hub.get_devices()
            if not devices:
                logger.warning("âŒ ç„¡å¯ç”¨QAI Hubè¨­å‚™ï¼Œè«‹ç¢ºèªå¸³è™Ÿæˆ–API Key")
                self.target_device = None
            else:
                # å…ˆç²¾ç¢ºæ¯”å°åç¨±
                matched = [d for d in devices if d.name == self.target_device_name]
                if matched:
                    self.target_device = matched[0]
                else:
                    # å†ç”¨é—œéµå­—æ¨¡ç³Šæ¯”å°
                    target_devices = [d for d in devices if any(kw in d.name for kw in ["Snapdragon", "X Elite", "CPU"])]
                    self.target_device = target_devices[0] if target_devices else devices[0]
                logger.info(f"ğŸ¯ ç›®æ¨™è¨­å‚™: {self.target_device.name}")
        except Exception as e:
            logger.error(f"âŒ å–å¾—QAI Hubè¨­å‚™å¤±æ•—: {e}")
            self.target_device = None
    """å¯¦ç”¨çš„QAI Hub + ONNX Runtimeç³»çµ±"""
    
    def load_mediapipe_models(self, source: str = 'onnx', model_dir: str = None, ext: str = None):
        """
        è‡ªå‹•è¼‰å…¥æŒ‡å®šç›®éŒ„ä¸‹æ‰€æœ‰å‰¯æª”åæ­£ç¢ºçš„æ¨¡å‹ï¼ˆonnx/tflite/dlcï¼‰ï¼Œä¸å†åªä¾ key å°æ‡‰ã€‚
        source: æ¨™è¨˜ä¾†æºé¡å‹
        model_dir: æŒ‡å®šç›®éŒ„ï¼ˆå¦‚ org-onnxã€org-tfliteã€org-dlc...ï¼‰
        ext: æŒ‡å®šå‰¯æª”åï¼ˆ.onnx/.tflite/.dlcï¼‰
        """
        logger.info(f"ğŸ“¥ è¼‰å…¥MediaPipeæ¨¡å‹ä¾†æº: {source}")
        base_dir = Path(__file__).parent.parent / 'models'
        if model_dir is not None and ext is not None:
            model_dir = base_dir / model_dir
        else:
            if source == 'onnx':
                model_dir = base_dir / 'onnx'
                ext = '.onnx'
            elif source == 'original':
                model_dir = base_dir / 'original'
                ext = '.tflite'
            else:
                raise ValueError(f"æœªçŸ¥æ¨¡å‹ä¾†æº: {source}")

        if not model_dir.exists() or not model_dir.is_dir():
            logger.warning(f"âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {model_dir}")
            return

        found_models = list(model_dir.glob(f"*{ext}"))
        if not found_models:
            logger.warning(f"âŒ ç›®éŒ„ {model_dir} ä¸‹æ‰¾ä¸åˆ°ä»»ä½• {ext} æ¨¡å‹æª”æ¡ˆ")
        # å¸¸è¦‹ MediaPipe æ¨¡å‹ input_size å°æ‡‰è¡¨
        input_size_map = {
            'facedetector': (192, 192),
            'facelandmark': (192, 192),
            'facelandmarkdetector': (192, 192),
            'facelandmark_with_attention': (192, 192),
            'handdetector': (224, 224),
            'handlandmark': (224, 224),
            'handlandmarkdetector': (224, 224),
            'handrecrop': (224, 224),
            'irislandmark': (64, 64),
            'posedetector': (256, 256),
            'poselandmark': (256, 256),
            'poselandmarkdetector': (256, 256),
            'poselandmark_full': (256, 256),
            'poselandmark_heavy': (256, 256),
            'poselandmark_lite': (256, 256),
        }
        default_input_size = (224, 224)
        for model_path in found_models:
            model_name = model_path.stem
            # å˜—è©¦è‡ªå‹•å°æ‡‰ input_size
            key = model_name.lower().replace('mediapipe-', '').replace('_w8a8', '').replace('_with_attention', '').replace('_full', '').replace('_heavy', '').replace('_lite', '').replace('_detector', '').replace('_landmark', 'landmark')
            # ä¾‹å¦‚ MediaPipe-FaceDetector -> facedetector
            input_size = None
            for k, v in input_size_map.items():
                if k in key:
                    input_size = v
                    break
            if input_size is None:
                input_size = default_input_size
                logger.warning(f"âš ï¸ {model_name} æœªçŸ¥input_sizeï¼Œè‡ªå‹•è¨­ç‚º {default_input_size}")
            self.qai_hub_models[model_name] = {
                'model_path': str(model_path),
                'config': {'description': model_name, 'input_size': input_size},
                'loaded': True,
                'format': source
            }
            logger.info(f"âœ… {model_name} è¼‰å…¥æˆåŠŸ: {model_path} | input_size={input_size}")
        logger.info("âœ… CPUåŸ·è¡Œæä¾›å•†å·²æ·»åŠ ")
    
    
    def export_models_to_torchscript(self):
        """å°‡æ¨¡å‹å°å‡ºç‚ºTorchScriptæ ¼å¼ï¼ˆQAI Hubæ”¯æ´ï¼‰"""
        logger.info("ğŸ“¤ å°å‡ºæ¨¡å‹ç‚ºTorchScriptæ ¼å¼...")
        
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info.get('loaded'):
                continue
            if 'model' not in model_info:
                logger.info(f"âš ï¸ {model_name} ç„¡ PyTorch modelï¼Œè·³é TorchScript å°å‡ºã€‚")
                continue
            try:
                model = model_info['model']
                config = model_info.get('config', {'description': model_name})
                logger.info(f"ğŸ”„ å°å‡º {config['description']} ç‚ºTorchScript...")
                input_size = config['input_size']
                sample_input = torch.randn(1, 3, input_size[1], input_size[0])
                model.eval()
                with torch.no_grad():
                    traced_model = torch.jit.trace(model, sample_input)
                torchscript_path = f"qai_hub_{model_name}_model.pt"
                traced_model.save(torchscript_path)
                logger.info(f"âœ… {config['description']} TorchScriptå·²ä¿å­˜: {torchscript_path}")
                model_info['torchscript_path'] = torchscript_path
                model_info['sample_input_shape'] = sample_input.shape
            except Exception as e:
                logger.error(f"âŒ {config['description']} TorchScriptå°å‡ºå¤±æ•—: {e}")
                model_info['export_error'] = str(e)
    
    def upload_models_to_qai_hub(self):
        """ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub (æ”¯æ´ onnx/tflite/torchscript)"""
        logger.info("â˜ï¸ ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub...")
        for model_name, model_info in self.qai_hub_models.items():
            # å„ªå…ˆé †åºï¼štorchscript > onnx/tflite
            model_path = model_info.get('torchscript_path') or model_info.get('model_path')
            if not model_info.get('loaded') or not model_path or not os.path.exists(model_path):
                logger.warning(f"âš ï¸ {model_name} æ²’æœ‰å¯ç”¨æ¨¡å‹æª”æ¡ˆï¼Œè·³éä¸Šå‚³")
                continue
            try:
                config = model_info['config']
                logger.info(f"ğŸ“¤ ä¸Šå‚³ {config['description']} ({model_path}) åˆ°QAI Hub...")
                uploaded_model = hub.upload_model(model_path)
                logger.info(f"âœ… {config['description']} ä¸Šå‚³æˆåŠŸ")
                logger.info(f"   æ¨¡å‹ID: {uploaded_model.model_id}")
                model_info['qai_hub_model'] = uploaded_model
                model_info['model_id'] = uploaded_model.model_id
            except Exception as e:
                logger.error(f"âŒ {config['description']} ä¸Šå‚³å¤±æ•—: {e}")
                model_info['upload_error'] = str(e)
    
    def submit_compilation_jobs(self):
        """æäº¤ç·¨è­¯Jobs (åªè¦æœ‰æˆåŠŸä¸Šå‚³çš„æ¨¡å‹éƒ½èƒ½æäº¤)"""
        logger.info("ğŸ”„ æäº¤æ¨¡å‹ç·¨è­¯Jobsåˆ°QAI Hub...")
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info.get('qai_hub_model'):
                logger.warning(f"âš ï¸ {model_name} æ²’æœ‰ä¸Šå‚³åˆ°QAI Hubï¼Œè·³éç·¨è­¯")
                continue
            try:
                qai_model = model_info['qai_hub_model']
                config = model_info['config']
                logger.info(f"ğŸš€ æäº¤ {config['description']} ç·¨è­¯Job...")
                # input_specs è½‰æ›: {'input': (1, 3, H, W)}
                input_size = config.get('input_size', (224, 224))
                # é è¨­ input åç¨±ç‚º 'input'ï¼Œå¯ä¾éœ€æ±‚æ“´å……
                input_specs = {'input': (1, 3, input_size[1], input_size[0])}
                logger.info(f"   input_specs: {input_specs}")
                compile_job = hub.submit_compile_job(
                    model=qai_model,
                    input_specs=input_specs,
                    device=self.target_device
                )
                logger.info(f"âœ… {config['description']} ç·¨è­¯Jobå·²æäº¤")
                logger.info(f"   Job ID: {compile_job.job_id}")
                logger.info(f"   Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                model_info['compile_job'] = compile_job
            except Exception as e:
                logger.error(f"âŒ {config['description']} ç·¨è­¯Jobæäº¤å¤±æ•—: {e}")
                model_info['compile_error'] = str(e)
    
    def convert_torchscript_to_onnx(self):
        """å°‡TorchScriptæ¨¡å‹è½‰æ›ç‚ºONNX"""
        logger.info("ğŸ”„ è½‰æ›TorchScriptæ¨¡å‹ç‚ºONNX...")
        
        for model_name, model_info in self.qai_hub_models.items():
            if not model_info.get('torchscript_path'):
                continue
                
            try:
                config = model_info['config']
                input_size = config['input_size']
                
                logger.info(f"ğŸ”„ è½‰æ› {config['description']} ç‚ºONNX...")
                
                # è¼‰å…¥TorchScriptæ¨¡å‹
                torchscript_model = torch.jit.load(model_info['torchscript_path'])
                
                # æº–å‚™ç¤ºä¾‹è¼¸å…¥
                sample_input = torch.randn(1, 3, input_size[1], input_size[0])
                
                # å°å‡ºç‚ºONNX
                onnx_path = f"qai_hub_{model_name}_model.onnx"
                torch.onnx.export(
                    torchscript_model,
                    sample_input,
                    onnx_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['image'],
                    output_names=['output'],
                    dynamic_axes={
                        'image': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                logger.info(f"âœ… {config['description']} ONNXå·²ä¿å­˜: {onnx_path}")
                
                # è¼‰å…¥ONNX Runtimeæœƒè©±
                self._load_onnx_session(model_name, onnx_path, model_info)
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ONNXè½‰æ›å¤±æ•—: {e}")
                model_info['onnx_error'] = str(e)
    
    def _load_onnx_session(self, model_name: str, onnx_path: str, model_info: Dict):
        """è¼‰å…¥ONNX Runtimeæœƒè©±"""
        try:
            config = model_info['config']
            logger.info(f"ğŸ”„ è¼‰å…¥ {model_name} ONNX Runtimeæœƒè©±...")
            
            # è¨­ç½®æœƒè©±é¸é …
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # å‰µå»ºæ¨ç†æœƒè©±
            session = ort.InferenceSession(
                onnx_path,
                sess_options=sess_options,
                providers=self.onnx_providers
            )
            
            self.onnx_sessions[model_name] = {
                'session': session,
                'config': config,
                'onnx_path': onnx_path
            }
            
            # è¨˜éŒ„æœƒè©±ä¿¡æ¯
            input_info = session.get_inputs()[0]
            output_info = session.get_outputs()[0]
            
            logger.info(f"âœ… {model_name} ONNXæœƒè©±è¼‰å…¥æˆåŠŸ")
            logger.info(f"   è¼¸å…¥: {input_info.name} {input_info.shape} {input_info.type}")
            logger.info(f"   è¼¸å‡º: {output_info.name} {output_info.shape} {output_info.type}")
            
        except Exception as e:
            logger.error(f"âŒ {model_name} ONNXæœƒè©±è¼‰å…¥å¤±æ•—: {e}")
    
    def detect_with_onnx(self, image: np.ndarray, model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨ONNX RuntimeåŸ·è¡Œæª¢æ¸¬"""
        if model_name not in self.onnx_sessions:
            return {"error": f"ONNXæœƒè©± {model_name} æœªè¼‰å…¥"}
        
        try:
            session_info = self.onnx_sessions[model_name]
            session = session_info['session']
            config = session_info['config']
            
            # é è™•ç†åœ–åƒ
            processed_image = self._preprocess_image(image, config['input_size'])
            
            # åŸ·è¡Œæ¨ç†
            input_name = session.get_inputs()[0].name
            start_time = time.time()
            outputs = session.run(None, {input_name: processed_image})
            inference_time = (time.time() - start_time) * 1000
            
            # å¾Œè™•ç†çµæœ
            results = self._postprocess_detection(outputs, model_name, config)
            results.update({
                'inference_time_ms': round(inference_time, 2),
                'model_type': f"QAI_Hub_{model_name}_ONNX",
                'description': config['description']
            })
            
            logger.info(f"âš¡ {model_name}æª¢æ¸¬å®Œæˆ: {inference_time:.2f}ms")
            return results
            
        except Exception as e:
            logger.error(f"âŒ {model_name}æª¢æ¸¬å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _preprocess_image(self, image: np.ndarray, input_size: tuple) -> np.ndarray:
        """é è™•ç†åœ–åƒ"""
        # èª¿æ•´åœ–åƒå¤§å°
        resized = cv2.resize(image, input_size)
        
        # è½‰æ›é¡è‰²ç©ºé–“ BGR -> RGB
        if len(resized.shape) == 3:
            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # æ­£è¦åŒ–åˆ° [0, 1]
        normalized = resized.astype(np.float32) / 255.0
        
        # èª¿æ•´ç¶­åº¦é †åº HWC -> CHW ä¸¦æ·»åŠ batchç¶­åº¦
        preprocessed = np.transpose(normalized, (2, 0, 1))
        preprocessed = np.expand_dims(preprocessed, axis=0)
        
        return preprocessed
    
    def _postprocess_detection(self, outputs: list, model_name: str, config: Dict) -> Dict[str, Any]:
        """å¾Œè™•ç†æª¢æ¸¬çµæœ"""
        results = {
            "success": True,
            "detections": [],
            "raw_output_shapes": [output.shape for output in outputs]
        }
        
        try:
            # æ ¹æ“šæ¨¡å‹é¡å‹è™•ç†è¼¸å‡º
            if model_name == 'face':
                results = self._process_face_outputs(outputs, results)
            elif model_name == 'pose':
                results = self._process_pose_outputs(outputs, results)
            elif model_name == 'hand':
                results = self._process_hand_outputs(outputs, results)
                
        except Exception as e:
            logger.error(f"âŒ {model_name}å¾Œè™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
        
        return results
    
    def _process_face_outputs(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†äººè‡‰æª¢æ¸¬è¼¸å‡º"""
        # ç°¡åŒ–çš„äººè‡‰æª¢æ¸¬çµæœè™•ç†
        if len(outputs) > 0:
            output = outputs[0]
            results["detections"] = [{
                "type": "face",
                "output_shape": output.shape,
                "detected": True
            }]
            results["total_faces"] = 1
        return results
    
    def _process_pose_outputs(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†å§¿æ…‹æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) > 0:
            output = outputs[0]
            results["detections"] = [{
                "type": "pose",
                "output_shape": output.shape,
                "detected": True
            }]
            results["total_poses"] = 1
        return results
    
    def _process_hand_outputs(self, outputs: list, results: Dict) -> Dict:
        """è™•ç†æ‰‹éƒ¨æª¢æ¸¬è¼¸å‡º"""
        if len(outputs) > 0:
            output = outputs[0]
            results["detections"] = [{
                "type": "hand",
                "output_shape": output.shape,
                "detected": True
            }]
            results["total_hands"] = 1
        return results
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„QAI Hub + ONNXæµæ°´ç·š"""
        pipeline_results = {
            "timestamp": time.time(),
            "steps": {},
            "final_status": {}
        }
        
        try:
            # Step 1: è¼‰å…¥æ¨¡å‹
            logger.info("ğŸ“¥ Step 1: è¼‰å…¥MediaPipeæ¨¡å‹")
            self.load_mediapipe_models()
            pipeline_results["steps"]["load_models"] = "completed"
            
            # Step 2: å°å‡ºTorchScript
            logger.info("ğŸ“¤ Step 2: å°å‡ºTorchScriptæ¨¡å‹")
            self.export_models_to_torchscript()
            pipeline_results["steps"]["export_torchscript"] = "completed"
            
            # Step 3: ä¸Šå‚³åˆ°QAI Hub
            logger.info("â˜ï¸ Step 3: ä¸Šå‚³æ¨¡å‹åˆ°QAI Hub")
            self.upload_models_to_qai_hub()
            pipeline_results["steps"]["upload_qai_hub"] = "completed"
            
            # Step 4: æäº¤ç·¨è­¯Jobs
            logger.info("ğŸš€ Step 4: æäº¤ç·¨è­¯Jobs")
            self.submit_compilation_jobs()
            pipeline_results["steps"]["submit_compile_jobs"] = "completed"
            
            # Step 5: è½‰æ›ONNX
            logger.info("ğŸ”„ Step 5: è½‰æ›ç‚ºONNX")
            self.convert_torchscript_to_onnx()
            pipeline_results["steps"]["convert_onnx"] = "completed"
            
            # ç”Ÿæˆæœ€çµ‚ç‹€æ…‹å ±å‘Š
            pipeline_results["final_status"] = {
                "loaded_models": list(self.qai_hub_models.keys()),
                "onnx_sessions": list(self.onnx_sessions.keys()),
                "qai_hub_uploads": len([m for m in self.qai_hub_models.values() if m.get('model_id')]),
                "compile_jobs": len([m for m in self.qai_hub_models.values() if m.get('compile_job')])
            }
            
            logger.info("âœ… å®Œæ•´æµæ°´ç·šåŸ·è¡ŒæˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´ç·šåŸ·è¡Œå¤±æ•—: {e}")
            pipeline_results["error"] = str(e)
        
        return pipeline_results

    def check_onnx_models(self):
        """æª¢æŸ¥æ‰€æœ‰ onnx æª”æ¡ˆæ˜¯å¦åˆæ³•ï¼Œå›å‚³ç•°å¸¸æ¸…å–®"""
        invalid = []
        for model_name, model_info in self.qai_hub_models.items():
            if model_info.get('format') == 'onnx' and model_info.get('loaded'):
                path = model_info['model_path']
                try:
                    onnx.checker.check_model(path, full_check=True)
                except Exception as e:
                    invalid.append((model_name, path, str(e)))
        return invalid

def main():
    """ä¸»å‡½æ•¸ï¼šæ¼”ç¤ºå¯¦ç”¨QAI Hub + ONNXç³»çµ±"""
    print("ğŸ¯ å¯¦ç”¨QAI Hub + ONNX Runtimeç³»çµ±æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–ç³»çµ±
        system = PracticalQAIHubONNX()
        
        # é‹è¡Œå®Œæ•´æµæ°´ç·š
        results = system.run_full_pipeline()
        
        # é¡¯ç¤ºå°‡é€²è¡Œ QAI Hub æœ€ä½³åŒ–çš„æ¨¡å‹æ•¸é‡èˆ‡æ¸…å–®
        models_to_optimize = [k for k, v in system.qai_hub_models.items() if v.get('loaded')]
        print(f"\nğŸ” åµæ¸¬åˆ° {len(models_to_optimize)} å€‹æ¨¡å‹å°‡é€²è¡Œ QAI Hub æœ€ä½³åŒ–ï¼š")
        for m in models_to_optimize:
            print(f"   - {m}")

        # æ¸¬è©¦æª¢æ¸¬ï¼ˆå¦‚æœONNXæœƒè©±å¯ç”¨ï¼‰
        if system.onnx_sessions:
            print("\nğŸ§ª æ¸¬è©¦ONNXæª¢æ¸¬...")
            test_images = ['andy.jpg', 'official_test_image.jpg']
            for img_path in test_images:
                if os.path.exists(img_path):
                    print(f"\nğŸ“· æ¸¬è©¦åœ–åƒ: {img_path}")
                    image = cv2.imread(img_path)
                    if image is not None:
                        for model_name in system.onnx_sessions.keys():
                            detection_result = system.detect_with_onnx(image, model_name)
                            print(f"   {model_name}: {detection_result.get('inference_time_ms', 'N/A')}ms")
        
        # è‡ªå‹•æŸ¥è©¢ QAI Hub Job ç‹€æ…‹ï¼Œç­‰å¾…æ‰€æœ‰ Job å®Œæˆ
        import time
        print("\nâ³ ç­‰å¾…æ‰€æœ‰ QAI Hub Job å®Œæˆ...")
        all_done = False
        poll_interval = 10  # ç§’
        max_wait = 60 * 30  # æœ€é•·ç­‰å¾… 30 åˆ†é˜
        waited = 0
        while not all_done and waited < max_wait:
            all_done = True
            for model_name, model_info in system.qai_hub_models.items():
                job = model_info.get('compile_job')
                if job:
                    job.refresh()  # é‡æ–°æŸ¥è©¢ç‹€æ…‹
                    status = getattr(job, 'status', None) or getattr(job, 'state', None)
                    model_info['job_status'] = status
                    if status not in ('COMPLETED', 'SUCCEEDED', 'SUCCESS', 'FINISHED', 'COMPLETED_SUCCESSFULLY'):
                        all_done = False
            if not all_done:
                print(f"  ...å°šæœ‰ Job åŸ·è¡Œä¸­ï¼Œ{poll_interval} ç§’å¾Œå†æŸ¥è©¢...")
                time.sleep(poll_interval)
                waited += poll_interval
        print("\nğŸ“Š QAI Hub Jobç‹€æ…‹:")
        for model_name, model_info in system.qai_hub_models.items():
            job = model_info.get('compile_job')
            job_id = job.job_id if job else ''
            status = model_info.get('job_status', '')
            print(f"   {model_name}: Job {job_id} ç‹€æ…‹: {status}")
            if job_id:
                print(f"     Dashboard: https://aihub.qualcomm.com/jobs/{job_id}")

        # ä¿å­˜çµæœ (JSON)
        results_file = 'practical_qai_hub_onnx_results.json'
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)

        # ç”¢ç”Ÿ HTML å ±å‘Š
        html_file = 'practical_qai_hub_onnx_report.html'
        with open(html_file, 'w') as f:
            f.write('<html><head><meta charset="utf-8"><title>QAI Hub Pipeline Report</title></head><body>')
            f.write('<h1>QAI Hub Pipeline Report</h1>')
            f.write(f'<p><b>Timestamp:</b> {results.get("timestamp", "")}</p>')
            f.write('<h2>Models & Jobs</h2><table border="1" cellpadding="4"><tr><th>Model</th><th>Status</th><th>Job ID</th><th>Dashboard</th></tr>')
            for model_name, model_info in system.qai_hub_models.items():
                job = model_info.get('compile_job')
                job_id = job.job_id if job else ''
                dashboard = f'<a href="https://aihub.qualcomm.com/jobs/{job_id}">{job_id}</a>' if job_id else ''
                status = model_info.get('job_status', '') or ('å·²æäº¤' if job_id else (model_info.get('error') or 'æœªæäº¤'))
                f.write(f'<tr><td>{model_name}</td><td>{status}</td><td>{job_id}</td><td>{dashboard}</td></tr>')
            f.write('</table>')
            # pipeline summary
            f.write('<h2>Pipeline Summary</h2><ul>')
            for step, stat in results.get('steps', {}).items():
                f.write(f'<li>{step}: {stat}</li>')
            f.write('</ul>')
            if 'error' in results:
                f.write(f'<p style="color:red"><b>Pipeline Error:</b> {results["error"]}</p>')
            f.write('</body></html>')

        print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼çµæœå·²ä¿å­˜åˆ° {results_file}ï¼ŒHTML å ±å‘Šå·²ç”¢ç”Ÿ {html_file}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
