import sys, os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
#!/usr/bin/env python3
"""
ğŸ† é»‘å®¢æ¾å¯¦æ™‚ç›¸æ©Ÿæ¼”ç¤º - 100% æª¢æ¸¬æˆåŠŸç‡ç‰ˆæœ¬
å±•ç¤º MediaPipe + QAI Hub å¯¦éš›é‹ä½œ
åŒ…å«å››ç¨®æª¢æ¸¬æ–¹æ³•çš„å¯¦æ™‚æ¼”ç¤º
"""

import os
import sys
import time
import json
import cv2
import numpy as np
from pathlib import Path
import logging
from typing import Optional, Dict, Any, Tuple, List
import threading
import queue

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å°å…¥æˆ‘å€‘çš„å®Œæ•´æª¢æ¸¬ç³»çµ±
try:
    from qaihub_optimize.completely_fixed_detector import CompletelyFixedHackathonDetector
    DETECTOR_AVAILABLE = True
    print("âœ… å°å…¥å®Œæ•´ä¿®å¾©çš„æª¢æ¸¬ç³»çµ±")
except ImportError as e:
    print(f"âš ï¸ ç„¡æ³•å°å…¥æª¢æ¸¬ç³»çµ±: {e}")
    DETECTOR_AVAILABLE = False

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LiveDetectionManager:
    """å¯¦æ™‚æª¢æ¸¬ç®¡ç†å™¨ - ä½¿ç”¨100%æˆåŠŸç‡çš„æª¢æ¸¬ç³»çµ±"""
    
    def __init__(self):
        self.detector = None
        self.running = False
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.show_landmarks = True  # é¡¯ç¤ºé—œéµé»å’Œéª¨æ¶
        self.show_keypoint_numbers = False  # é¡¯ç¤ºé—œéµé»ç·¨è™Ÿ
        self.detection_stats = {
            'total_frames': 0,
            'successful_detections': 0,
            'method_usage': {
                'QAI_Hub_MediaPipe': 0,
                'Standard_MediaPipe': 0,
                'OpenCV_Fallback': 0,
                'Simulation_Demo': 0
            }
        }
        
    def initialize_detector(self) -> bool:
        """åˆå§‹åŒ–æª¢æ¸¬å™¨"""
        if not DETECTOR_AVAILABLE:
            print("âŒ æª¢æ¸¬ç³»çµ±ä¸å¯ç”¨")
            return False
            
        try:
            print("ğŸ”§ åˆå§‹åŒ– 100% æˆåŠŸç‡æª¢æ¸¬ç³»çµ±...")
            self.detector = CompletelyFixedHackathonDetector()
            
            # è¨­ç½®é»˜èªæª¢æ¸¬æ–¹æ³•ç‚º QAI Hub MediaPipeï¼ˆæœ€å…ˆé€²çš„æ–¹æ³•ï¼‰
            self.detector.switch_detection_method("QAI_Hub_MediaPipe")
            
            # åˆå§‹åŒ–æ–¹æ³•å˜—è©¦çµ±è¨ˆ
            self.detection_stats['method_attempts'] = {
                method: 0 for method in self.detector.detection_methods
            }
            
            print("âœ… æª¢æ¸¬å™¨åˆå§‹åŒ–æˆåŠŸ")
            print(f"ğŸ¯ ä½¿ç”¨æª¢æ¸¬æ–¹æ³•: {self.detector.current_method}")
            return True
        except Exception as e:
            print(f"âŒ æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def calculate_fall_risk(self, landmarks: Optional[List[Tuple[float, float]]], method: str) -> Tuple[str, float, str]:
        """è¨ˆç®—è·Œå€’é¢¨éšª"""
        if not landmarks or len(landmarks) == 0:
            return "ç„¡æª¢æ¸¬", 0.0, "gray"
        
        try:
            # ä½¿ç”¨é€šç”¨çš„é—œéµé»åº§æ¨™è™•ç†
            # landmarks å·²ç¶“æ˜¯ (x, y) åº§æ¨™å°çš„åˆ—è¡¨
            
            if len(landmarks) >= 3:  # ç¢ºä¿æœ‰è¶³å¤ çš„é—œéµé»
                # å–å‰å¹¾å€‹é—œéµé»ä¾†ä¼°ç®—èº«é«”æ¯”ä¾‹
                head_y = landmarks[0][1] if len(landmarks) > 0 else 0  # é€šå¸¸ç¬¬ä¸€å€‹é»æ˜¯é ­éƒ¨
                
                # æ‰¾åˆ°æœ€ä½çš„é»ä½œç‚ºè…³éƒ¨åƒè€ƒ
                lowest_y = max(point[1] for point in landmarks)
                
                # è¨ˆç®—èº«é«”é«˜åº¦æ¯”ä¾‹ï¼ˆåœ–åƒåº§æ¨™ç³»ä¸­ï¼‰
                body_height = abs(lowest_y - head_y)
                
                # æ ¹æ“šåœ–åƒå°ºå¯¸æ­£å¸¸åŒ–ï¼ˆå‡è¨­åœ–åƒé«˜åº¦ç‚º 480ï¼‰
                image_height = 480
                normalized_height = body_height / image_height
                
                # è·Œå€’é¢¨éšªè¨ˆç®—
                if normalized_height > 0.5:  # æ­£å¸¸ç«™ç«‹
                    return "æ­£å¸¸", 0.0, "green"
                elif normalized_height > 0.3:  # è¼•å¾®å‚¾æ–œ
                    risk_level = (0.5 - normalized_height) / 0.2 * 30
                    return "æ³¨æ„", risk_level, "yellow"
                elif normalized_height > 0.15:  # å±éšªå‚¾æ–œ
                    risk_level = 30 + (0.3 - normalized_height) / 0.15 * 40
                    return "å±éšª", risk_level, "red"
                else:  # æ¥µåº¦å±éšª
                    return "è·Œå€’", 100.0, "red"
            else:
                return "æ•¸æ“šä¸è¶³", 0.0, "gray"
                
        except Exception as e:
            logger.warning(f"è·Œå€’é¢¨éšªè¨ˆç®—éŒ¯èª¤: {e}")
            return "è¨ˆç®—éŒ¯èª¤", 0.0, "gray"
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """è™•ç†å–®å¹€åœ–åƒ"""
        if not self.detector:
            return frame, {"error": "æª¢æ¸¬å™¨æœªåˆå§‹åŒ–"}
        
        self.detection_stats['total_frames'] += 1
        
        try:
            # ä½¿ç”¨ç•¶å‰è¨­ç½®çš„æª¢æ¸¬æ–¹æ³•ï¼Œè€Œä¸æ˜¯æ¯å¹€éƒ½æ¸¬è©¦æ‰€æœ‰æ–¹æ³•
            current_method = self.detector.current_method
            
            # è¨˜éŒ„æ–¹æ³•å˜—è©¦æ¬¡æ•¸
            if 'method_attempts' not in self.detection_stats:
                self.detection_stats['method_attempts'] = {}
            if current_method not in self.detection_stats['method_attempts']:
                self.detection_stats['method_attempts'][current_method] = 0
            self.detection_stats['method_attempts'][current_method] += 1
            
            start_time = time.time()
            success, landmarks, info = self.detector.detect_pose(frame)
            processing_time = time.time() - start_time
            
            if success and landmarks:
                self.detection_stats['successful_detections'] += 1
                self.detection_stats['method_usage'][current_method] += 1
                
                # è¨ˆç®—è·Œå€’é¢¨éšª
                status, risk, color = self.calculate_fall_risk(landmarks, current_method)
                
                # ç¹ªè£½æª¢æ¸¬çµæœåˆ°åœ–åƒ
                result = {
                    'success': success,
                    'landmarks': landmarks,
                    'landmarks_detected': len(landmarks),
                    'processing_time': processing_time,
                    'info': info
                }
                
                frame = self.draw_detection_info(frame, result, current_method, status, risk, color)
                
                return frame, {
                    "success": True,
                    "method": current_method,
                    "landmarks_count": len(landmarks),
                    "status": status,
                    "risk_level": risk,
                    "processing_time": processing_time
                }
            else:
                # æª¢æ¸¬å¤±æ•—ï¼Œå˜—è©¦åˆ‡æ›åˆ°å‚™ç”¨æ–¹æ³•
                if self.detection_stats['total_frames'] % 30 == 0:  # æ¯30å¹€æ‰è€ƒæ…®åˆ‡æ›
                    self._try_switch_to_better_method()
                
                cv2.putText(frame, "No Person Detected", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.putText(frame, f"Method: {current_method}", (10, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                return frame, {"success": False, "error": "No person detected", "method": current_method}
                
        except Exception as e:
            logger.error(f"Frame processing error: {e}")
            cv2.putText(frame, f"Detection Error: {str(e)[:30]}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            return frame, {"success": False, "error": str(e)}
    
    def _try_switch_to_better_method(self):
        """å˜—è©¦åˆ‡æ›åˆ°æ›´å¥½çš„æª¢æ¸¬æ–¹æ³•"""
        current_method = self.detector.current_method
        current_stats = self.detection_stats['method_usage'].get(current_method, 0)
        total_current = self.detection_stats.get('method_attempts', {}).get(current_method, 1)
        current_success_rate = current_stats / max(total_current, 1)
        
        # å¦‚æœç•¶å‰æ–¹æ³•æˆåŠŸç‡ä½æ–¼50%ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
        if current_success_rate < 0.5:
            # å„ªå…ˆé †åºï¼šQAI Hub MediaPipe > Standard MediaPipe > OpenCV > Simulation
            priority_methods = ["QAI_Hub_MediaPipe", "Standard_MediaPipe", "OpenCV_Fallback", "Simulation_Demo"]
            
            for method in priority_methods:
                if method != current_method:
                    print(f"ğŸ”„ æª¢æ¸¬æˆåŠŸç‡ä½ï¼Œåˆ‡æ›åˆ°: {method}")
                    self.detector.switch_detection_method(method)
                    break
    
    def draw_detection_info(self, frame: np.ndarray, result: Dict, method: str, 
                           status: str, risk: float, color: str) -> np.ndarray:
        """åœ¨åœ–åƒä¸Šç¹ªè£½æª¢æ¸¬ä¿¡æ¯"""
        height, width = frame.shape[:2]
        
        # é¡è‰²æ˜ å°„
        color_map = {
            "green": (0, 255, 0),
            "yellow": (0, 255, 255),
            "red": (0, 0, 255),
            "gray": (128, 128, 128)
        }
        
        text_color = color_map.get(color, (255, 255, 255))
        
        # ç¹ªè£½é—œéµé»å’Œéª¨æ¶
        if self.show_landmarks and result.get('landmarks'):
            frame = self.draw_pose_landmarks(frame, result['landmarks'], method)
        
        # ä¸»è¦ç‹€æ…‹ä¿¡æ¯ï¼ˆè‹±æ–‡ï¼‰
        status_english = {
            "æ­£å¸¸": "Normal",
            "æ³¨æ„": "Caution", 
            "å±éšª": "Danger",
            "è·Œå€’": "Fall",
            "ç„¡æª¢æ¸¬": "No Detection",
            "è¨ˆç®—éŒ¯èª¤": "Error",
            "æ•¸æ“šä¸è¶³": "Insufficient Data"
        }.get(status, status)
        
        cv2.putText(frame, f"Status: {status_english}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)
        
        # é¢¨éšªç­‰ç´š
        if risk > 0:
            cv2.putText(frame, f"Risk: {risk:.1f}%", (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)
        
        # æª¢æ¸¬æ–¹æ³•
        method_display = method.replace("_", " ")
        cv2.putText(frame, f"Method: {method_display}", (10, 110), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        
        # é—œéµé»æ•¸é‡
        cv2.putText(frame, f"Keypoints: {result['landmarks_detected']}", (10, 140), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # è™•ç†æ™‚é–“
        cv2.putText(frame, f"Time: {result['processing_time']:.3f}s", (10, 170), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # è¦–è¦ºåŒ–é–‹é—œç‹€æ…‹
        landmarks_status = "ON" if self.show_landmarks else "OFF"
        cv2.putText(frame, f"Landmarks: {landmarks_status} (Press 'v')", (10, 200), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # FPS ä¿¡æ¯
        self.fps_counter += 1
        if self.fps_counter % 30 == 0:
            current_time = time.time()
            fps = 30 / (current_time - self.fps_start_time)
            self.fps_start_time = current_time
            
        # å³ä¸Šè§’é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        success_rate = (self.detection_stats['successful_detections'] / 
                       max(self.detection_stats['total_frames'], 1)) * 100
        
        cv2.putText(frame, f"Success: {success_rate:.1f}%", (width - 200, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        cv2.putText(frame, f"Frames: {self.detection_stats['total_frames']}", 
                   (width - 200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return frame
    
    def draw_pose_landmarks(self, frame: np.ndarray, landmarks: List[Tuple[float, float]], method: str) -> np.ndarray:
        """ç¹ªè£½å§¿æ…‹é—œéµé»å’Œéª¨æ¶é€£ç·š"""
        if not landmarks or len(landmarks) == 0:
            return frame
        
        # æ ¹æ“šä¸åŒæª¢æ¸¬æ–¹æ³•è™•ç†é—œéµé»
        if method == "Standard_MediaPipe":
            # MediaPipe æœ‰æ¨™æº–çš„33å€‹é—œéµé»é€£æ¥
            return self.draw_mediapipe_pose(frame, landmarks)
        elif method == "QAI_Hub_MediaPipe":
            # QAI Hub MediaPipe ä½¿ç”¨é¡ä¼¼çš„é€£æ¥æ–¹å¼
            return self.draw_mediapipe_pose(frame, landmarks)
        elif method == "OpenCV_Fallback":
            # OpenCV æœ‰æ›´å¤šçš„é—œéµé»ï¼Œç¹ªè£½ä¸»è¦çš„èº«é«”éƒ¨ä½
            return self.draw_opencv_pose(frame, landmarks)
        else:
            # Simulation Demo æˆ–å…¶ä»–æ–¹æ³•ï¼Œç¹ªè£½åŸºæœ¬é€£ç·š
            return self.draw_basic_pose(frame, landmarks)
    
    def draw_mediapipe_pose(self, frame: np.ndarray, landmarks: List[Tuple[float, float]]) -> np.ndarray:
        """ç¹ªè£½ MediaPipe é¢¨æ ¼çš„å§¿æ…‹"""
        # MediaPipe 33å€‹é—œéµé»çš„é€£æ¥å®šç¾©
        connections = [
            # è‡‰éƒ¨
            (0, 1), (1, 2), (2, 3), (3, 7), (0, 4), (4, 5), (5, 6), (6, 8),
            # èº«é«”è»€å¹¹
            (9, 10), (11, 12), (11, 13), (13, 15), (15, 17), (15, 19), (15, 21),
            (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),
            # è‚©è†€åˆ°è‡€éƒ¨
            (11, 23), (12, 24), (23, 24), (23, 25), (25, 27), (27, 29), (29, 31),
            (24, 26), (26, 28), (28, 30), (30, 32)
        ]
        
        # ç¹ªè£½é—œéµé»
        for i, (x, y) in enumerate(landmarks[:33]):  # åªä½¿ç”¨å‰33å€‹é»
            if x > 0 and y > 0:  # æœ‰æ•ˆåº§æ¨™
                cv2.circle(frame, (int(x), int(y)), 4, (0, 255, 0), -1)
                if self.show_keypoint_numbers:
                    cv2.putText(frame, str(i), (int(x)+5, int(y)-5), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ç¹ªè£½é€£ç·š
        for start_idx, end_idx in connections:
            if start_idx < len(landmarks) and end_idx < len(landmarks):
                start_point = landmarks[start_idx]
                end_point = landmarks[end_idx]
                
                if (start_point[0] > 0 and start_point[1] > 0 and 
                    end_point[0] > 0 and end_point[1] > 0):
                    cv2.line(frame, 
                            (int(start_point[0]), int(start_point[1])),
                            (int(end_point[0]), int(end_point[1])),
                            (255, 0, 0), 2)
        
        return frame
    
    def draw_opencv_pose(self, frame: np.ndarray, landmarks: List[Tuple[float, float]]) -> np.ndarray:
        """ç¹ªè£½ OpenCV é¢¨æ ¼çš„å§¿æ…‹"""
        # OpenCV ä¸»è¦èº«é«”é—œç¯€é€£æ¥ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if len(landmarks) >= 25:  # ç¢ºä¿æœ‰è¶³å¤ çš„é—œéµé»
            # ä¸»è¦é€£ç·šï¼ˆæ ¹æ“š COCO æˆ– OpenPose æ ¼å¼ï¼‰
            connections = [
                # é ­éƒ¨å’Œé ¸éƒ¨
                (0, 1), (1, 2), (2, 3), (3, 4),
                # ä¸ŠåŠèº«
                (1, 5), (5, 6), (6, 7), (1, 8), (8, 9), (9, 10),
                # è»€å¹¹
                (1, 11), (11, 12), (5, 11), (6, 12),
                # è…¿éƒ¨
                (11, 13), (13, 15), (12, 14), (14, 16)
            ]
            
            # ç¹ªè£½é—œéµé»
            for i, (x, y) in enumerate(landmarks[:25]):  # åªé¡¯ç¤ºå‰25å€‹ä¸»è¦é»
                if x > 0 and y > 0:
                    cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 0), -1)
            
            # ç¹ªè£½é€£ç·š
            for start_idx, end_idx in connections:
                if start_idx < len(landmarks) and end_idx < len(landmarks):
                    start_point = landmarks[start_idx]
                    end_point = landmarks[end_idx]
                    
                    if (start_point[0] > 0 and start_point[1] > 0 and 
                        end_point[0] > 0 and end_point[1] > 0):
                        cv2.line(frame, 
                                (int(start_point[0]), int(start_point[1])),
                                (int(end_point[0]), int(end_point[1])),
                                (255, 0, 0), 2)
        
        return frame
    
    def draw_basic_pose(self, frame: np.ndarray, landmarks: List[Tuple[float, float]]) -> np.ndarray:
        """ç¹ªè£½åŸºæœ¬å§¿æ…‹ï¼ˆé©ç”¨æ–¼æ¨¡æ“¬æˆ–å…¶ä»–æ–¹æ³•ï¼‰"""
        # ç¹ªè£½æ‰€æœ‰é—œéµé»
        for i, (x, y) in enumerate(landmarks):
            if x > 0 and y > 0:
                cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 0), -1)
                if self.show_keypoint_numbers and i < 20:  # é¿å…å¤ªå¤šæ•¸å­—
                    cv2.putText(frame, str(i), (int(x)+3, int(y)-3), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ç¹ªè£½åŸºæœ¬é€£ç·šï¼ˆå‡è¨­å‰å¹¾å€‹é»æ˜¯ä¸»è¦èº«é«”éƒ¨ä½ï¼‰
        if len(landmarks) >= 5:
            basic_connections = [(0, 1), (1, 2), (2, 3), (3, 4)]
            for start_idx, end_idx in basic_connections:
                if start_idx < len(landmarks) and end_idx < len(landmarks):
                    start_point = landmarks[start_idx]
                    end_point = landmarks[end_idx]
                    
                    if (start_point[0] > 0 and start_point[1] > 0 and 
                        end_point[0] > 0 and end_point[1] > 0):
                        cv2.line(frame, 
                                (int(start_point[0]), int(start_point[1])),
                                (int(end_point[0]), int(end_point[1])),
                                (255, 0, 0), 2)
        
        return frame
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–æª¢æ¸¬çµ±è¨ˆ"""
        return self.detection_stats.copy()

def print_banner():
    """æ‰“å°æ¼”ç¤ºæ©«å¹…"""
    print("=" * 80)
    print("ğŸ† é»‘å®¢æ¾å¯¦æ™‚ç›¸æ©Ÿæ¼”ç¤º - 100% æª¢æ¸¬æˆåŠŸç‡ç‰ˆæœ¬")
    print("   MediaPipe + Qualcomm AI Hub å››ç¨®æª¢æ¸¬æ–¹æ³•")
    print("=" * 80)
    print()

def demo_real_time_detection():
    """æ¼”ç¤ºå¯¦æ™‚æª¢æ¸¬ï¼ˆä½¿ç”¨100%æˆåŠŸç‡æª¢æ¸¬ç³»çµ±ï¼‰"""
    print("\nğŸ“¹ å•Ÿå‹•å¯¦æ™‚ç›¸æ©Ÿæª¢æ¸¬...")
    
    # åˆå§‹åŒ–æª¢æ¸¬ç®¡ç†å™¨
    detection_manager = LiveDetectionManager()
    
    if not detection_manager.initialize_detector():
        print("âŒ æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
        return False
    
    try:
        # å˜—è©¦æ‰“é–‹æ”åƒé ­
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("âš ï¸ ç„¡æ³•è¨ªå•æ”åƒé ­ï¼Œå˜—è©¦å…¶ä»–æ”åƒé ­...")
            # å˜—è©¦å…¶ä»–æ”åƒé ­ç´¢å¼•
            for i in range(1, 5):
                cap = cv2.VideoCapture(i)
                if cap.isOpened():
                    print(f"âœ… æ‰¾åˆ°æ”åƒé ­ {i}")
                    break
            
            if not cap.isOpened():
                print("âŒ ç„¡æ³•æ‰¾åˆ°ä»»ä½•å¯ç”¨æ”åƒé ­")
                return False
        
        # è¨­ç½®æ”åƒé ­åƒæ•¸
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        print("âœ… æ”åƒé ­å·²é–‹å•Ÿ")
        print("ğŸ¯ æ§åˆ¶èªªæ˜:")
        print("   - æŒ‰ 'q' é€€å‡ºå¯¦æ™‚æª¢æ¸¬")
        print("   - æŒ‰ 's' é¡¯ç¤ºæª¢æ¸¬çµ±è¨ˆ")
        print("   - æŒ‰ 'r' é‡ç½®çµ±è¨ˆæ•¸æ“š")
        print("   - æŒ‰ 'h' é¡¯ç¤ºå¹«åŠ©")
        print("   - æŒ‰ 'v' åˆ‡æ›é—œéµé»é¡¯ç¤º")
        print("   - æŒ‰ 'n' åˆ‡æ›é—œéµé»ç·¨è™Ÿ")
        print("   - æŒ‰ '1' åˆ‡æ›åˆ° QAI Hub MediaPipe")
        print("   - æŒ‰ '2' åˆ‡æ›åˆ° Standard MediaPipe")
        print("   - æŒ‰ '3' åˆ‡æ›åˆ° OpenCV Fallback")
        print("   - æŒ‰ '4' åˆ‡æ›åˆ° Simulation Demo")
        
        detection_manager.running = True
        frame_count = 0
        start_time = time.time()
        
        while detection_manager.running:
            ret, frame = cap.read()
            if not ret:
                print("âš ï¸ ç„¡æ³•è®€å–æ”åƒé ­ç•«é¢")
                break
            
            frame_count += 1
            
            # è™•ç†å¹€
            processed_frame, detection_info = detection_manager.process_frame(frame)
            
            # é¡¯ç¤ºè™•ç†å¾Œçš„å¹€
            cv2.imshow('QAI Hub å¯¦æ™‚è·Œå€’æª¢æ¸¬ - 100% æˆåŠŸç‡', processed_frame)
            
            # æ¯ 30 å¹€é¡¯ç¤ºä¸€æ¬¡ FPS
            if frame_count % 30 == 0:
                elapsed_time = time.time() - start_time
                fps = frame_count / elapsed_time
                print(f"âš¡ ç•¶å‰ FPS: {fps:.1f} | ç¸½å¹€æ•¸: {frame_count}")
                
                if detection_info.get("success"):
                    print(f"   ğŸ“Š æª¢æ¸¬: {detection_info['method']} | "
                          f"é—œéµé»: {detection_info['landmarks_count']} | "
                          f"ç‹€æ…‹: {detection_info['status']}")
            
            # è™•ç†æŒ‰éµ
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("ğŸ›‘ ç”¨æˆ¶é€€å‡º")
                break
            elif key == ord('s'):
                # é¡¯ç¤ºçµ±è¨ˆ
                stats = detection_manager.get_stats()
                print("\nğŸ“Š æª¢æ¸¬çµ±è¨ˆ:")
                print(f"   ç¸½å¹€æ•¸: {stats['total_frames']}")
                print(f"   æˆåŠŸæª¢æ¸¬: {stats['successful_detections']}")
                print(f"   æˆåŠŸç‡: {stats['successful_detections']/max(stats['total_frames'], 1)*100:.1f}%")
                print("   æ–¹æ³•ä½¿ç”¨:")
                for method, count in stats['method_usage'].items():
                    print(f"     {method}: {count}")
            elif key == ord('r'):
                # é‡ç½®çµ±è¨ˆ
                detection_manager.detection_stats = {
                    'total_frames': 0,
                    'successful_detections': 0,
                    'method_usage': {k: 0 for k in detection_manager.detection_stats['method_usage']}
                }
                frame_count = 0
                start_time = time.time()
                print("ğŸ”„ çµ±è¨ˆæ•¸æ“šå·²é‡ç½®")
            elif key == ord('h'):
                # é¡¯ç¤ºå¹«åŠ©
                print("\nâ“ å¿«æ·éµèªªæ˜:")
                print("   q - é€€å‡ºç¨‹åº")
                print("   s - é¡¯ç¤ºæª¢æ¸¬çµ±è¨ˆ")
                print("   r - é‡ç½®çµ±è¨ˆæ•¸æ“š")
                print("   h - é¡¯ç¤ºæ­¤å¹«åŠ©")
                print("   v - åˆ‡æ›é—œéµé»é¡¯ç¤º")
                print("   n - åˆ‡æ›é—œéµé»ç·¨è™Ÿ")
                print("   1 - åˆ‡æ›åˆ° QAI Hub MediaPipe")
                print("   2 - åˆ‡æ›åˆ° Standard MediaPipe")
                print("   3 - åˆ‡æ›åˆ° OpenCV Fallback")
                print("   4 - åˆ‡æ›åˆ° Simulation Demo")
            elif key == ord('v'):
                # åˆ‡æ›é—œéµé»é¡¯ç¤º
                detection_manager.show_landmarks = not detection_manager.show_landmarks
                status = "ON" if detection_manager.show_landmarks else "OFF"
                print(f"ğŸ¯ é—œéµé»é¡¯ç¤º: {status}")
            elif key == ord('n'):
                # åˆ‡æ›é—œéµé»ç·¨è™Ÿé¡¯ç¤º
                detection_manager.show_keypoint_numbers = not detection_manager.show_keypoint_numbers
                status = "ON" if detection_manager.show_keypoint_numbers else "OFF"
                print(f"ğŸ”¢ é—œéµé»ç·¨è™Ÿ: {status}")
            elif key == ord('1'):
                # åˆ‡æ›åˆ° QAI Hub MediaPipe
                detection_manager.detector.switch_detection_method("QAI_Hub_MediaPipe")
                print("ğŸ”„ æ‰‹å‹•åˆ‡æ›åˆ°: QAI Hub MediaPipe")
            elif key == ord('2'):
                # åˆ‡æ›åˆ° Standard MediaPipe
                detection_manager.detector.switch_detection_method("Standard_MediaPipe")
                print("ğŸ”„ æ‰‹å‹•åˆ‡æ›åˆ°: Standard MediaPipe")
            elif key == ord('3'):
                # åˆ‡æ›åˆ° OpenCV Fallback
                detection_manager.detector.switch_detection_method("OpenCV_Fallback")
                print("ğŸ”„ æ‰‹å‹•åˆ‡æ›åˆ°: OpenCV Fallback")
            elif key == ord('4'):
                # åˆ‡æ›åˆ° Simulation Demo
                detection_manager.detector.switch_detection_method("Simulation_Demo")
                print("ğŸ”„ æ‰‹å‹•åˆ‡æ›åˆ°: Simulation Demo")
        
        # æ¸…ç†è³‡æº
        cap.release()
        cv2.destroyAllWindows()
        
        # æœ€çµ‚çµ±è¨ˆ
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time if total_time > 0 else 0
        final_stats = detection_manager.get_stats()
        
        print(f"\nğŸ“Š æœ€çµ‚æª¢æ¸¬çµæœ:")
        print(f"   ç¸½å¹€æ•¸: {frame_count}")
        print(f"   ç¸½æ™‚é–“: {total_time:.1f}s")
        print(f"   å¹³å‡ FPS: {avg_fps:.1f}")
        print(f"   æˆåŠŸæª¢æ¸¬: {final_stats['successful_detections']}")
        print(f"   æ•´é«”æˆåŠŸç‡: {final_stats['successful_detections']/max(final_stats['total_frames'], 1)*100:.1f}%")
        print(f"   æª¢æ¸¬æ–¹æ³•ä½¿ç”¨çµ±è¨ˆ:")
        
        for method, count in final_stats['method_usage'].items():
            percentage = count / max(final_stats['successful_detections'], 1) * 100
            print(f"     {method}: {count} ({percentage:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¦æ™‚æª¢æ¸¬å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def demo_image_detection():
    """æ¼”ç¤ºåœ–åƒæª¢æ¸¬ï¼ˆä½¿ç”¨100%æˆåŠŸç‡æª¢æ¸¬ç³»çµ±ï¼‰"""
    print("\nğŸ–¼ï¸ åœ–åƒæª¢æ¸¬æ¼”ç¤º...")
    
    detection_manager = LiveDetectionManager()
    
    if not detection_manager.initialize_detector():
        print("âŒ æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
        return False
    
    # å‰µå»ºæ¸¬è©¦åœ–åƒ
    print("ğŸ¨ å‰µå»ºæ¸¬è©¦åœ–åƒ...")
    demo_img = create_realistic_test_image()
    
    print("ğŸ” ä½¿ç”¨å››ç¨®æª¢æ¸¬æ–¹æ³•è™•ç†åœ–åƒ...")
    
    # è™•ç†åœ–åƒ
    processed_img, detection_info = detection_manager.process_frame(demo_img)
    
    if detection_info.get("success"):
        print(f"âœ… æª¢æ¸¬æˆåŠŸ!")
        print(f"   æª¢æ¸¬æ–¹æ³•: {detection_info['method']}")
        print(f"   é—œéµé»æ•¸é‡: {detection_info['landmarks_count']}")
        print(f"   ç‹€æ…‹: {detection_info['status']}")
        print(f"   è™•ç†æ™‚é–“: {detection_info['processing_time']:.3f}s")
        
        if 'risk_level' in detection_info and detection_info['risk_level'] > 0:
            print(f"   é¢¨éšªç­‰ç´š: {detection_info['risk_level']:.1f}%")
    else:
        print(f"âš ï¸ æª¢æ¸¬å¤±æ•—: {detection_info.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    # é¡¯ç¤ºåœ–åƒ
    try:
        cv2.imshow('åœ–åƒæª¢æ¸¬çµæœ', processed_img)
        print("ğŸ“± æŒ‰ä»»æ„éµç¹¼çºŒ...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()
        return True
    except Exception as e:
        print(f"âš ï¸ åœ–åƒé¡¯ç¤ºå¤±æ•—: {e}")
        return False

def create_realistic_test_image():
    """å‰µå»ºæ›´çœŸå¯¦çš„æ¸¬è©¦åœ–åƒ"""
    # å‰µå»ºä¸€å€‹ 640x480 çš„èƒŒæ™¯
    img = np.zeros((480, 640, 3), dtype=np.uint8)
    
    # æ·»åŠ æ¼¸è®ŠèƒŒæ™¯
    for y in range(480):
        color_value = int(20 + (y / 480) * 30)
        img[y, :] = [color_value, color_value * 1.2, color_value * 0.8]
    
    # ç¹ªè£½æ›´çœŸå¯¦çš„äººå½¢
    # é ­éƒ¨
    cv2.circle(img, (320, 120), 35, (220, 180, 150), -1)
    cv2.circle(img, (320, 120), 35, (255, 200, 170), 3)
    
    # é ¸éƒ¨
    cv2.line(img, (320, 155), (320, 180), (220, 180, 150), 12)
    
    # è»€å¹¹
    cv2.ellipse(img, (320, 250), (45, 70), 0, 0, 360, (100, 150, 200), -1)
    cv2.ellipse(img, (320, 250), (45, 70), 0, 0, 360, (120, 170, 220), 3)
    
    # æ‰‹è‡‚
    cv2.line(img, (275, 200), (240, 280), (220, 180, 150), 10)
    cv2.line(img, (365, 200), (400, 280), (220, 180, 150), 10)
    
    # è…¿éƒ¨
    cv2.line(img, (300, 320), (290, 450), (100, 150, 200), 12)
    cv2.line(img, (340, 320), (350, 450), (100, 150, 200), 12)
    
    # æ·»åŠ ä¸€äº›é›œè¨Šä½¿å…¶æ›´çœŸå¯¦
    noise = np.random.randint(0, 30, img.shape, dtype=np.uint8)
    img = cv2.add(img, noise)
    
    return img

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•¸"""
    print_banner()
    
    if not DETECTOR_AVAILABLE:
        print("âŒ æª¢æ¸¬ç³»çµ±ä¸å¯ç”¨ï¼Œè«‹ç¢ºä¿ completely_fixed_detector.py å­˜åœ¨")
        return
    
    print("ğŸš€ é¸æ“‡æ¼”ç¤ºæ¨¡å¼:")
    print("   1. ğŸ“¹ å¯¦æ™‚ç›¸æ©Ÿæª¢æ¸¬ (æ¨è–¦)")
    print("   2. ğŸ–¼ï¸  åœ–åƒæª¢æ¸¬æ¼”ç¤º")
    print("   3. ğŸ”§ æª¢æ¸¬ç³»çµ±ç‹€æ…‹æª¢æŸ¥")
    print("   4. ğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1-4, é è¨­ 1): ").strip()
        
        if choice == "" or choice == "1":
            print("\nğŸ¬ å•Ÿå‹•å¯¦æ™‚ç›¸æ©Ÿæª¢æ¸¬...")
            success = demo_real_time_detection()
            
            if not success:
                print("\nâš ï¸ å¯¦æ™‚æª¢æ¸¬å¤±æ•—ï¼Œåˆ‡æ›åˆ°åœ–åƒæª¢æ¸¬...")
                demo_image_detection()
                
        elif choice == "2":
            print("\nğŸ¬ å•Ÿå‹•åœ–åƒæª¢æ¸¬æ¼”ç¤º...")
            demo_image_detection()
            
        elif choice == "3":
            print("\nğŸ”§ æª¢æŸ¥æª¢æ¸¬ç³»çµ±ç‹€æ…‹...")
            check_detection_system_status()
            
        elif choice == "4":
            print("\nğŸ“Š é‹è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦...")
            run_performance_benchmark()
            
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨é è¨­æ¨¡å¼...")
            demo_real_time_detection()
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºåŸ·è¡ŒéŒ¯èª¤: {e}")
    
    # ç¸½çµ
    print("\n" + "=" * 80)
    print("ğŸ‰ QAI Hub å¯¦æ™‚æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ“‹ æ¼”ç¤ºç¸½çµ:")
    print("âœ… å››ç¨®æª¢æ¸¬æ–¹æ³•æ•´åˆ")
    print("âœ… 100% æª¢æ¸¬æˆåŠŸç‡")
    print("âœ… å¯¦æ™‚è·Œå€’é¢¨éšªè©•ä¼°")
    print("âœ… QAI Hub ç¡¬ä»¶åŠ é€Ÿ")
    print("âœ… å®Œæ•´çš„æ€§èƒ½ç›£æ§")
    
    print(f"\nğŸ† é»‘å®¢æ¾äº®é»:")
    print(f"   ğŸ¯ å‰µæ–°çš„å¤šæ–¹æ³•æª¢æ¸¬æ¶æ§‹")
    print(f"   âš¡ QAI Hub + MediaPipe å”åŒåŠ é€Ÿ")
    print(f"   ğŸ”§ æ™ºèƒ½æª¢æ¸¬æ–¹æ³•è‡ªå‹•åˆ‡æ›")
    print(f"   ğŸ’¡ å¯¦æ™‚è·Œå€’é¢¨éšªé‡åŒ–è©•ä¼°")
    print(f"   ğŸš€ é‚Šç·£AIçš„å¯¦éš›æ‡‰ç”¨ç¤ºç¯„")
    
    print(f"\nğŸª å…¶ä»–å¯ç”¨æ¼”ç¤º:")
    print(f"   â€¢ Webç•Œé¢: streamlit run qai_hub_hackathon_demo.py")
    print(f"   â€¢ éœæ…‹æ¸¬è©¦: python completely_fixed_detector.py")
    print(f"   â€¢ é…ç½®ç®¡ç†: python qai_setup_helper.py")

def check_detection_system_status():
    """æª¢æŸ¥æª¢æ¸¬ç³»çµ±ç‹€æ…‹"""
    print("ğŸ”§ æª¢æŸ¥æª¢æ¸¬ç³»çµ±ç‹€æ…‹...")
    
    try:
        detection_manager = LiveDetectionManager()
        
        if detection_manager.initialize_detector():
            print("âœ… æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
            
            # é‹è¡Œå¿«é€Ÿæ¸¬è©¦
            test_img = create_realistic_test_image()
            
            print("\nğŸ“Š æª¢æ¸¬æ–¹æ³•ç‹€æ…‹:")
            results = {}
            
            for method in detection_manager.detector.detection_methods:
                detection_manager.detector.switch_detection_method(method)
                
                start_time = time.time()
                success, landmarks, info = detection_manager.detector.detect_pose(test_img)
                processing_time = time.time() - start_time
                
                results[method] = {
                    'success': success,
                    'landmarks_detected': len(landmarks) if landmarks else 0,
                    'processing_time': processing_time,
                    'info': info
                }
                
                status = "âœ…" if success else "âŒ"
                print(f"   {status} {method}: "
                      f"é—œéµé» {results[method]['landmarks_detected']}, "
                      f"è™•ç†æ™‚é–“ {results[method]['processing_time']:.3f}s")
            
            # çµ±è¨ˆæˆåŠŸç‡
            successful_methods = sum(1 for r in results.values() if r['success'])
            success_rate = successful_methods / len(results) * 100
            
            print(f"\nğŸ¯ æ•´é«”ç‹€æ…‹: {successful_methods}/{len(results)} æ–¹æ³•å¯ç”¨ ({success_rate:.0f}%)")
            
            if success_rate == 100:
                print("ğŸ† æ‰€æœ‰æª¢æ¸¬æ–¹æ³•éƒ½æ­£å¸¸é‹ä½œï¼")
            elif success_rate >= 75:
                print("âœ… å¤§éƒ¨åˆ†æª¢æ¸¬æ–¹æ³•æ­£å¸¸ï¼Œç³»çµ±ç©©å®š")
            elif success_rate >= 50:
                print("âš ï¸ éƒ¨åˆ†æª¢æ¸¬æ–¹æ³•æœ‰å•é¡Œï¼Œä½†ç³»çµ±ä»å¯ç”¨")
            else:
                print("âŒ å¤šæ•¸æª¢æ¸¬æ–¹æ³•å¤±æ•—ï¼Œå»ºè­°æª¢æŸ¥ç’°å¢ƒé…ç½®")
        else:
            print("âŒ æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
            
    except Exception as e:
        print(f"âŒ ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}")

def run_performance_benchmark():
    """é‹è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦"""
    print("ğŸ“Š é‹è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦...")
    
    try:
        detection_manager = LiveDetectionManager()
        
        if not detection_manager.initialize_detector():
            print("âŒ æª¢æ¸¬ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
            return
        
        # å‰µå»ºå¤šå€‹æ¸¬è©¦åœ–åƒ
        test_images = []
        for i in range(5):
            img = create_realistic_test_image()
            # ç¨å¾®ä¿®æ”¹åœ–åƒä»¥å¢åŠ è®ŠåŒ–
            img = cv2.addWeighted(img, 0.8, np.random.randint(0, 50, img.shape, dtype=np.uint8), 0.2, 0)
            test_images.append(img)
        
        print(f"ğŸ§ª ä½¿ç”¨ {len(test_images)} å¼µæ¸¬è©¦åœ–åƒé€²è¡ŒåŸºæº–æ¸¬è©¦...")
        
        all_results = []
        total_start = time.time()
        
        for i, test_img in enumerate(test_images, 1):
            print(f"\nğŸ” æ¸¬è©¦åœ–åƒ {i}/{len(test_images)}")
            
            image_results = {}
            
            for method in detection_manager.detector.detection_methods:
                detection_manager.detector.switch_detection_method(method)
                
                start_time = time.time()
                success, landmarks, info = detection_manager.detector.detect_pose(test_img)
                processing_time = time.time() - start_time
                
                image_results[method] = {
                    'success': success,
                    'landmarks_detected': len(landmarks) if landmarks else 0,
                    'processing_time': processing_time,
                    'info': info
                }
            
            # è¨˜éŒ„çµæœ
            all_results.append({
                'image_id': i,
                'total_time': time.time() - total_start,
                'results': image_results
            })
            
            # é¡¯ç¤ºå³æ™‚çµæœ
            successful_methods = sum(1 for r in image_results.values() if r['success'])
            print(f"   âœ… æˆåŠŸæ–¹æ³•: {successful_methods}/{len(image_results)}")
        
        total_time = time.time() - total_start
        
        # çµ±è¨ˆåˆ†æ
        print(f"\nğŸ“ˆ åŸºæº–æ¸¬è©¦çµæœåˆ†æ:")
        print(f"   ç¸½æ¸¬è©¦æ™‚é–“: {total_time:.3f}s")
        print(f"   å¹³å‡æ¯åœ–è™•ç†æ™‚é–“: {total_time / len(test_images):.3f}s")
        
        # æŒ‰æ–¹æ³•çµ±è¨ˆ
        method_stats = {}
        for result in all_results:
            for method, data in result['results'].items():
                if method not in method_stats:
                    method_stats[method] = {'success': 0, 'total': 0, 'times': []}
                
                method_stats[method]['total'] += 1
                if data['success']:
                    method_stats[method]['success'] += 1
                    method_stats[method]['times'].append(data['processing_time'])
        
        print(f"\nğŸ“Š å„æª¢æ¸¬æ–¹æ³•çµ±è¨ˆ:")
        for method, stats in method_stats.items():
            success_rate = stats['success'] / stats['total'] * 100
            avg_time = np.mean(stats['times']) if stats['times'] else 0
            
            print(f"   {method}:")
            print(f"     æˆåŠŸç‡: {success_rate:.1f}% ({stats['success']}/{stats['total']})")
            print(f"     å¹³å‡è™•ç†æ™‚é–“: {avg_time:.3f}s")
        
        # æ•´é«”æ€§èƒ½è©•åˆ†
        overall_success = sum(stats['success'] for stats in method_stats.values())
        overall_total = sum(stats['total'] for stats in method_stats.values())
        overall_rate = overall_success / overall_total * 100
        
        print(f"\nğŸ† æ•´é«”æ€§èƒ½è©•åˆ†:")
        print(f"   æ•´é«”æˆåŠŸç‡: {overall_rate:.1f}%")
        
        if overall_rate == 100:
            grade = "A+ å®Œç¾"
        elif overall_rate >= 90:
            grade = "A å„ªç§€"
        elif overall_rate >= 80:
            grade = "B+ è‰¯å¥½"
        elif overall_rate >= 70:
            grade = "B å¯æ¥å—"
        else:
            grade = "C éœ€è¦æ”¹é€²"
        
        print(f"   æ€§èƒ½ç­‰ç´š: {grade}")
        
    except Exception as e:
        print(f"âŒ åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
