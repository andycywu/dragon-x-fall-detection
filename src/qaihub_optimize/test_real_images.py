#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨çœŸå¯¦åœ–åƒæ¸¬è©¦æª¢æ¸¬æ–¹æ³•
è§£æ±º QAI Hub MediaPipe å’Œæ¨™æº– MediaPipe çš„å•é¡Œ
"""

import cv2
import numpy as np
import time
import logging
from PIL import Image
from typing import List, Tuple, Optional, Dict, Any
import os
import sys

# ç’°å¢ƒé…ç½®
os.environ['PYTHONPATH'] = '/Users/andycyw/mvp_fall_detection_starter/.venv_mediapipe/lib/python3.11/site-packages'

# æ—¥èªŒé…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealImageFallDetector:
    def __init__(self):
        self.current_method = "QAI_Hub_MediaPipe"
        self.detection_methods = [
            "QAI_Hub_MediaPipe",
            "Standard_MediaPipe", 
            "OpenCV_Fallback",
            "Simulation_Demo"
        ]
        
        # æ€§èƒ½çµ±è¨ˆ
        self.performance_stats = {method: {'success': 0, 'total': 0, 'times': []} 
                                for method in self.detection_methods}
        
        # åˆå§‹åŒ–å„ç¨®æª¢æ¸¬å™¨
        self.qai_hub_models = None
        self.mediapipe_pose = None
        self.opencv_detector = None
        
        self._initialize_detectors()
    
    def _initialize_detectors(self):
        """åˆå§‹åŒ–æ‰€æœ‰æª¢æ¸¬å™¨"""
        print("ğŸš€ åˆå§‹åŒ–æª¢æ¸¬å™¨...")
        
        # 1. QAI Hub MediaPipe
        try:
            from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
            from qai_hub_models.models.mediapipe_pose.model import MediaPipePose
            
            pose_model = MediaPipePose.from_pretrained()
            self.qai_hub_models = MediaPipePoseApp.from_pretrained(pose_model)
            print("âœ… QAI Hub MediaPipe åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ QAI Hub MediaPipe åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # 2. æ¨™æº– MediaPipe
        try:
            import mediapipe as mp
            self.mp_pose = mp.solutions.pose
            self.mp_drawing = mp.solutions.drawing_utils
            
            # å„ªåŒ–é…ç½®ç”¨æ–¼æ›´å¥½çš„æª¢æ¸¬
            self.mediapipe_pose = self.mp_pose.Pose(
                static_image_mode=True,
                model_complexity=2,  # æœ€é«˜è¤‡é›œåº¦
                smooth_landmarks=True,
                enable_segmentation=True,  # å•Ÿç”¨åˆ†å‰²
                min_detection_confidence=0.1,  # éå¸¸ä½çš„æª¢æ¸¬é–¾å€¼
                min_tracking_confidence=0.1
            )
            print("âœ… æ¨™æº– MediaPipe åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨™æº– MediaPipe åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def load_official_test_image(self):
        """è¼‰å…¥å®˜æ–¹æ¸¬è©¦åœ–åƒ"""
        try:
            from qai_hub_models.models.mediapipe_pose.model import MODEL_ID, MODEL_ASSET_VERSION
            from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
            
            print("ğŸ“¥ è¼‰å…¥å®˜æ–¹æ¸¬è©¦åœ–åƒ...")
            official_image_asset = CachedWebModelAsset.from_asset_store(
                MODEL_ID, MODEL_ASSET_VERSION, "pose.jpeg"
            )
            official_image = load_image(official_image_asset)
            
            if isinstance(official_image, Image.Image):
                official_image = np.array(official_image)
                # è½‰æ› RGB åˆ° BGR (OpenCV æ ¼å¼)
                official_image = cv2.cvtColor(official_image, cv2.COLOR_RGB2BGR)
            
            print(f"âœ… å®˜æ–¹åœ–åƒå°ºå¯¸: {official_image.shape}")
            return official_image
            
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥å®˜æ–¹æ¸¬è©¦åœ–åƒ: {e}")
            return None
    
    def _detect_qai_hub_mediapipe(self, image: np.ndarray) -> Tuple[bool, List[Tuple[float, float]], str]:
        """æ”¹é€²çš„ QAI Hub MediaPipe æª¢æ¸¬"""
        try:
            if self.qai_hub_models is None:
                return False, [], "QAI Hub æ¨¡å‹æœªåˆå§‹åŒ–"
            
            # è½‰æ›ç‚º RGB PIL åœ–åƒ
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
            
            # ä½¿ç”¨ raw_output=True ç²å–è©³ç´°çµæœ
            start_time = time.time()
            result = self.qai_hub_models.predict_landmarks_from_image(pil_image, raw_output=True)
            detection_time = time.time() - start_time
            
            print(f"  QAI Hub çµæœé¡å‹: {type(result)}")
            print(f"  QAI Hub çµæœé•·åº¦: {len(result) if hasattr(result, '__len__') else 'N/A'}")
            
            if not isinstance(result, tuple) or len(result) < 4:
                return False, [], f"ç„¡æ•ˆçš„çµæœæ ¼å¼: {type(result)}"
            
            batched_selected_boxes, batched_selected_keypoints, batched_roi_4corners, landmarks_out = result
            
            print(f"  Boxes æ•¸é‡: {len(batched_selected_boxes)}")
            print(f"  Keypoints æ•¸é‡: {len(batched_selected_keypoints)}")
            print(f"  Landmarks æ•¸é‡: {len(landmarks_out) if landmarks_out else 0}")
            
            # æª¢æŸ¥ç¬¬ä¸€å€‹ batch
            if len(batched_selected_boxes) > 0:
                boxes = batched_selected_boxes[0]
                print(f"  ç¬¬ä¸€å€‹ Boxes å½¢ç‹€: {boxes.shape if hasattr(boxes, 'shape') else type(boxes)}")
                print(f"  ç¬¬ä¸€å€‹ Boxes å…ƒç´ æ•¸é‡: {boxes.numel() if hasattr(boxes, 'numel') else 'N/A'}")
                
                if hasattr(boxes, 'numel') and boxes.numel() > 0:
                    print(f"  âœ… æª¢æ¸¬åˆ°é‚Šç•Œæ¡†!")
                    
                    # è§£æé—œéµé»
                    pose_landmarks = []
                    
                    if landmarks_out and len(landmarks_out) > 0:
                        landmarks = landmarks_out[0]
                        print(f"  Landmarks é¡å‹: {type(landmarks)}")
                        
                        if isinstance(landmarks, list) and len(landmarks) > 0:
                            for i, landmark_tensor in enumerate(landmarks):
                                print(f"    Landmark {i}: {landmark_tensor.shape if hasattr(landmark_tensor, 'shape') else type(landmark_tensor)}")
                                
                                if hasattr(landmark_tensor, 'shape') and landmark_tensor.numel() > 0:
                                    # landmark_tensor å½¢ç‹€æ‡‰è©²æ˜¯ [1, N, 4] (batch, landmarks, coordinates)
                                    if len(landmark_tensor.shape) >= 2:
                                        if len(landmark_tensor.shape) == 3:
                                            landmark_data = landmark_tensor[0]  # å–ç¬¬ä¸€å€‹ batch
                                        else:
                                            landmark_data = landmark_tensor
                                        
                                        print(f"      è™•ç†å½¢ç‹€: {landmark_data.shape}")
                                        
                                        for j in range(min(landmark_data.shape[0], 33)):  # æœ€å¤š33å€‹é—œéµé»
                                            if landmark_data.shape[1] >= 2:  # è‡³å°‘æœ‰ x, y
                                                x, y = float(landmark_data[j, 0]), float(landmark_data[j, 1])
                                                
                                                # æª¢æŸ¥æ˜¯å¦ç‚ºæ­¸ä¸€åŒ–åº§æ¨™ (0-1) æˆ–åœ–åƒåº§æ¨™
                                                if 0 <= x <= 1 and 0 <= y <= 1:
                                                    # æ­¸ä¸€åŒ–åº§æ¨™ï¼Œè½‰æ›ç‚ºåœ–åƒåº§æ¨™
                                                    img_x = x * image.shape[1]
                                                    img_y = y * image.shape[0]
                                                else:
                                                    # å‡è¨­å·²ç¶“æ˜¯åœ–åƒåº§æ¨™
                                                    img_x, img_y = x, y
                                                
                                                if 0 <= img_x <= image.shape[1] and 0 <= img_y <= image.shape[0]:
                                                    pose_landmarks.append((img_x, img_y))
                    
                    if pose_landmarks:
                        self.performance_stats['QAI_Hub_MediaPipe']['times'].append(detection_time)
                        return True, pose_landmarks, f"æª¢æ¸¬åˆ° {len(pose_landmarks)} å€‹é—œéµé»"
                    else:
                        return False, [], "æœ‰é‚Šç•Œæ¡†ä½†ç„¡æ³•è§£æé—œéµé»"
                else:
                    return False, [], "æœªæª¢æ¸¬åˆ°é‚Šç•Œæ¡†"
            
            return False, [], "ç„¡æª¢æ¸¬çµæœ"
                
        except Exception as e:
            logger.error(f"QAI Hub MediaPipe æª¢æ¸¬éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False, [], f"æª¢æ¸¬éŒ¯èª¤: {str(e)}"
    
    def _detect_standard_mediapipe(self, image: np.ndarray) -> Tuple[bool, List[Tuple[float, float]], str]:
        """æ”¹é€²çš„æ¨™æº– MediaPipe æª¢æ¸¬"""
        try:
            if self.mediapipe_pose is None:
                return False, [], "MediaPipe æ¨¡å‹æœªåˆå§‹åŒ–"
            
            # è½‰æ›ç‚º RGB
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            start_time = time.time()
            results = self.mediapipe_pose.process(rgb_image)
            detection_time = time.time() - start_time
            
            print(f"  MediaPipe çµæœ: {results.pose_landmarks is not None}")
            
            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                print(f"  é—œéµé»æ•¸é‡: {len(landmarks)}")
                
                pose_landmarks = []
                
                # è½‰æ›é—œéµé»åº§æ¨™
                height, width = image.shape[:2]
                visible_count = 0
                
                for i, landmark in enumerate(landmarks):
                    x = landmark.x * width
                    y = landmark.y * height
                    visibility = landmark.visibility
                    
                    if visibility > 0.1:  # é™ä½å¯è¦‹æ€§é–¾å€¼
                        pose_landmarks.append((x, y))
                        if visibility > 0.5:
                            visible_count += 1
                
                if pose_landmarks:
                    self.performance_stats['Standard_MediaPipe']['times'].append(detection_time)
                    return True, pose_landmarks, f"æª¢æ¸¬åˆ° {len(pose_landmarks)} å€‹é—œéµé» (é«˜å¯è¦‹æ€§: {visible_count})"
            
            return False, [], "æœªæª¢æ¸¬åˆ°å§¿æ…‹é—œéµé»"
            
        except Exception as e:
            logger.error(f"æ¨™æº– MediaPipe æª¢æ¸¬éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False, [], f"æª¢æ¸¬éŒ¯èª¤: {str(e)}"
    
    def _detect_simulation_demo(self, image: np.ndarray) -> Tuple[bool, List[Tuple[float, float]], str]:
        """æ™ºèƒ½æ¨¡æ“¬æª¢æ¸¬ï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰"""
        try:
            start_time = time.time()
            
            height, width = image.shape[:2]
            center_x, center_y = width // 2, height // 2
            
            # ç”Ÿæˆæ¨™æº–çš„ 33 å€‹ MediaPipe é—œéµé»
            pose_landmarks = [
                # è‡‰éƒ¨é—œéµé» (0-10)
                (center_x, center_y - height//3),  # 0: é¼»å­
                (center_x - 10, center_y - height//3 - 5),  # 1: å·¦çœ¼å…§è§’
                (center_x - 20, center_y - height//3),  # 2: å·¦çœ¼
                (center_x - 30, center_y - height//3 - 5),  # 3: å·¦çœ¼å¤–è§’
                (center_x + 10, center_y - height//3 - 5),  # 4: å³çœ¼å…§è§’
                (center_x + 20, center_y - height//3),  # 5: å³çœ¼
                (center_x + 30, center_y - height//3 - 5),  # 6: å³çœ¼å¤–è§’
                (center_x - 25, center_y - height//3 + 10),  # 7: å·¦è€³
                (center_x + 25, center_y - height//3 + 10),  # 8: å³è€³
                (center_x - 10, center_y - height//3 + 15),  # 9: å˜´å·¦
                (center_x + 10, center_y - height//3 + 15),  # 10: å˜´å³
                
                # ä¸Šèº«é—œéµé» (11-22)
                (center_x - 40, center_y - height//6),  # 11: å·¦è‚©
                (center_x + 40, center_y - height//6),  # 12: å³è‚©
                (center_x - 80, center_y),  # 13: å·¦è‚˜
                (center_x + 80, center_y),  # 14: å³è‚˜
                (center_x - 100, center_y + height//8),  # 15: å·¦æ‰‹è…•
                (center_x + 100, center_y + height//8),  # 16: å³æ‰‹è…•
                (center_x - 110, center_y + height//6),  # 17: å·¦æ‰‹æŒ‡
                (center_x + 110, center_y + height//6),  # 18: å³æ‰‹æŒ‡
                (center_x - 15, center_y + height//8),  # 19: å·¦è‡€
                (center_x + 15, center_y + height//8),  # 20: å³è‡€
                (center_x - 120, center_y + height//6),  # 21: å·¦å°æŒ‡
                (center_x + 120, center_y + height//6),  # 22: å³å°æŒ‡
                
                # ä¸‹èº«é—œéµé» (23-32)
                (center_x - 20, center_y + height//4),  # 23: å·¦è†
                (center_x + 20, center_y + height//4),  # 24: å³è†
                (center_x - 25, center_y + height//2.5),  # 25: å·¦è…³è¸
                (center_x + 25, center_y + height//2.5),  # 26: å³è…³è¸
                (center_x - 30, center_y + height//2.2),  # 27: å·¦è…³è·Ÿ
                (center_x + 30, center_y + height//2.2),  # 28: å³è…³è·Ÿ
                (center_x - 35, center_y + height//2),  # 29: å·¦è…³è¶¾
                (center_x + 35, center_y + height//2),  # 30: å³è…³è¶¾
                (center_x - 40, center_y + height//2),  # 31: å·¦è…³å¤–å´
                (center_x + 40, center_y + height//2),  # 32: å³è…³å¤–å´
            ]
            
            # ç¢ºä¿åº§æ¨™åœ¨åœ–åƒç¯„åœå…§
            valid_landmarks = []
            for x, y in pose_landmarks:
                x = max(0, min(width, x))
                y = max(0, min(height, y))
                valid_landmarks.append((x, y))
            
            detection_time = time.time() - start_time
            self.performance_stats['Simulation_Demo']['times'].append(detection_time)
            
            return True, valid_landmarks, f"æ¨¡æ“¬æª¢æ¸¬ç”Ÿæˆ {len(valid_landmarks)} å€‹é—œéµé»ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰"
            
        except Exception as e:
            logger.error(f"æ¨¡æ“¬æª¢æ¸¬éŒ¯èª¤: {e}")
            return False, [], f"æ¨¡æ“¬éŒ¯èª¤: {str(e)}"
    
    def detect_pose(self, image: np.ndarray) -> Tuple[bool, List[Tuple[float, float]], str]:
        """çµ±ä¸€çš„å§¿æ…‹æª¢æ¸¬æ¥å£"""
        method = self.current_method
        self.performance_stats[method]['total'] += 1
        
        # æ ¹æ“šç•¶å‰æ–¹æ³•é€²è¡Œæª¢æ¸¬
        if method == "QAI_Hub_MediaPipe":
            success, landmarks, info = self._detect_qai_hub_mediapipe(image)
        elif method == "Standard_MediaPipe":
            success, landmarks, info = self._detect_standard_mediapipe(image)
        elif method == "Simulation_Demo":
            success, landmarks, info = self._detect_simulation_demo(image)
        else:
            return False, [], f"æœªçŸ¥æª¢æ¸¬æ–¹æ³•: {method}"
        
        # æ›´æ–°çµ±è¨ˆ
        if success:
            self.performance_stats[method]['success'] += 1
        
        return success, landmarks, info
    
    def switch_detection_method(self, method: str):
        """åˆ‡æ›æª¢æ¸¬æ–¹æ³•"""
        if method in self.detection_methods:
            old_method = self.current_method
            self.current_method = method
            print(f"ğŸ”„ åˆ‡æ›æª¢æ¸¬æ–¹æ³•: {old_method} â†’ {method}")
        else:
            print(f"âŒ ç„¡æ•ˆçš„æª¢æ¸¬æ–¹æ³•: {method}")
    
    def get_performance_summary(self) -> str:
        """ç²å–æ€§èƒ½çµ±è¨ˆæ‘˜è¦"""
        summary = "\nğŸ“Š æª¢æ¸¬æ€§èƒ½çµ±è¨ˆ:\n" + "="*50 + "\n"
        
        for method, stats in self.performance_stats.items():
            total = stats['total']
            success = stats['success']
            success_rate = (success / total * 100) if total > 0 else 0
            
            avg_time = 0
            if stats['times']:
                avg_time = sum(stats['times']) / len(stats['times'])
            
            status = "âœ…" if success_rate > 80 else "âš ï¸" if success_rate > 50 else "âŒ"
            
            summary += f"{status} {method}:\n"
            summary += f"   æˆåŠŸç‡: {success_rate:.1f}% ({success}/{total})\n"
            summary += f"   å¹³å‡è€—æ™‚: {avg_time:.3f}ç§’\n"
            if method == self.current_method:
                summary += f"   ğŸ“ ç•¶å‰ä½¿ç”¨ä¸­\n"
            summary += "\n"
        
        return summary

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ çœŸå¯¦åœ–åƒè·Œå€’æª¢æ¸¬æ¸¬è©¦")
    print("=" * 50)
    
    detector = RealImageFallDetector()
    
    # è¼‰å…¥å®˜æ–¹æ¸¬è©¦åœ–åƒ
    test_image = detector.load_official_test_image()
    
    if test_image is None:
        print("âŒ ç„¡æ³•è¼‰å…¥æ¸¬è©¦åœ–åƒï¼Œé€€å‡º")
        return
    
    # ä¿å­˜æ¸¬è©¦åœ–åƒ
    cv2.imwrite("official_test_image.jpg", test_image)
    print("ğŸ’¾ ä¿å­˜å®˜æ–¹æ¸¬è©¦åœ–åƒ: official_test_image.jpg")
    
    # æ¸¬è©¦æ¯å€‹æª¢æ¸¬æ–¹æ³•
    test_methods = ["QAI_Hub_MediaPipe", "Standard_MediaPipe", "Simulation_Demo"]
    
    for method in test_methods:
        print(f"\nğŸ§ª æ¸¬è©¦æ–¹æ³•: {method}")
        print("-" * 40)
        
        detector.switch_detection_method(method)
        success, landmarks, info = detector.detect_pose(test_image)
        
        status = "âœ…" if success else "âŒ"
        print(f"{status} çµæœ: {info}")
        
        if success and landmarks:
            # ç¹ªè£½æª¢æ¸¬çµæœ
            result_image = test_image.copy()
            for i, (x, y) in enumerate(landmarks):
                cv2.circle(result_image, (int(x), int(y)), 5, (0, 255, 0), -1)
                cv2.putText(result_image, str(i), (int(x)+8, int(y)-8), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            cv2.imwrite(f"result_{method.lower()}.jpg", result_image)
            print(f"ğŸ’¾ ä¿å­˜çµæœåœ–åƒ: result_{method.lower()}.jpg")
    
    # é¡¯ç¤ºæ€§èƒ½çµ±è¨ˆ
    print(detector.get_performance_summary())

if __name__ == "__main__":
    main()
