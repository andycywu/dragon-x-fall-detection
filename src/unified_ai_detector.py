#!/usr/bin/env python3
"""
ğŸ§  çµ±ä¸€AIæª¢æ¸¬æ¨¡å¡Š
æ”¯æŒMacå’ŒSnapdragonå¹³å°çš„ç„¡ç¸«åˆ‡æ›
"""

import os
import cv2
import numpy as np
import logging
from typing import Dict, Any, List, Tuple, Optional
import json
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedAIDetector:
    """çµ±ä¸€AIæª¢æ¸¬å™¨ - è·¨å¹³å°å…¼å®¹"""
    
    def __init__(self, config_path: str = "cross_platform_config.json"):
        """åˆå§‹åŒ–çµ±ä¸€æª¢æ¸¬å™¨"""
        self.config = self._load_config(config_path)
        self.platform_info = self._detect_platform()
        self.platform_config = self._get_platform_config()
        
        # æª¢æ¸¬èƒ½åŠ›
        self.onnx_available = False
        self.qai_hub_available = False
        self.mediapipe_available = False
        
        # æ¨¡å‹å’Œæœƒè©±
        self.models = {}
        self.sessions = {}
        
        # åˆå§‹åŒ–ç³»çµ±
        self._initialize_system()
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ é…ç½®è¼‰å…¥å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é è¨­é…ç½®"""
        return {
            "platform_configs": {
                "fallback": {
                    "batch_size": 1,
                    "num_threads": 2,
                    "providers": ["CPUExecutionProvider"]
                }
            }
        }
    
    def _detect_platform(self) -> Dict[str, str]:
        """æª¢æ¸¬ç•¶å‰å¹³å°"""
        import platform
        
        system = platform.system().lower()
        machine = platform.machine().lower()
        processor = platform.processor().lower()
        
        platform_info = {
            "system": system,
            "machine": machine,
            "processor": processor
        }
        
        # åˆ¤æ–·å¹³å°é¡å‹
        if system == "darwin" and ("arm" in machine or "arm64" in machine):
            platform_info["type"] = "mac_apple_silicon"
            platform_info["ai_accelerator"] = "apple_neural_engine"
        elif "snapdragon" in processor.lower() or "qualcomm" in processor.lower():
            platform_info["type"] = "snapdragon_x_elite"
            platform_info["ai_accelerator"] = "hexagon_npu"
        elif system == "windows" and ("arm" in machine or "aarch64" in machine):
            platform_info["type"] = "snapdragon_x_elite"  # Windows ARMå¯èƒ½æ˜¯Snapdragon
            platform_info["ai_accelerator"] = "hexagon_npu"
        else:
            platform_info["type"] = "fallback"
            platform_info["ai_accelerator"] = "cpu_generic"
        
        return platform_info
    
    def _get_platform_config(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰å¹³å°é…ç½®"""
        platform_type = self.platform_info["type"]
        configs = self.config.get("platform_configs", {})
        return configs.get(platform_type, configs.get("fallback", {}))
    
    def _initialize_system(self):
        """åˆå§‹åŒ–æª¢æ¸¬ç³»çµ±"""
        logger.info("ğŸš€ åˆå§‹åŒ–çµ±ä¸€AIæª¢æ¸¬ç³»çµ±...")
        logger.info(f"ğŸ–¥ï¸ æª¢æ¸¬å¹³å°: {self.platform_info['type']}")
        logger.info(f"ğŸ§  AIåŠ é€Ÿå™¨: {self.platform_info['ai_accelerator']}")
        
        # æª¢æŸ¥ä¾è³´é …
        self._check_dependencies()
        
        # åˆå§‹åŒ–ONNX Runtime
        if self.onnx_available:
            self._initialize_onnx_runtime()
        
        # åˆå§‹åŒ–QAI Hub
        if self.qai_hub_available:
            self._initialize_qai_hub()
        
        # åˆå§‹åŒ–MediaPipeå¾Œå‚™
        if self.mediapipe_available:
            self._initialize_mediapipe()
    
    def _check_dependencies(self):
        """æª¢æŸ¥ä¾è³´é …å¯ç”¨æ€§"""
        # æª¢æŸ¥ONNX Runtime
        try:
            import onnxruntime as ort
            self.onnx_available = True
            logger.info("âœ… ONNX Runtimeå¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ ONNX Runtimeä¸å¯ç”¨")
        
        # æª¢æŸ¥QAI Hub
        try:
            import qai_hub as hub
            api_token = os.getenv('QAI_HUB_API_TOKEN')
            if api_token:
                self.qai_hub_available = True
                logger.info("âœ… QAI Hubå¯ç”¨")
            else:
                logger.warning("âš ï¸ QAI Hub API Tokenæœªè¨­ç½®")
        except ImportError:
            logger.warning("âš ï¸ QAI Hub SDKä¸å¯ç”¨")
        
        # æª¢æŸ¥MediaPipe
        try:
            import mediapipe as mp
            self.mediapipe_available = True
            logger.info("âœ… MediaPipeå¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ MediaPipeä¸å¯ç”¨")
    
    def _initialize_onnx_runtime(self):
        """åˆå§‹åŒ–ONNX Runtime"""
        try:
            import onnxruntime as ort
            
            # ç²å–å¯ç”¨æä¾›å•†
            available_providers = ort.get_available_providers()
            platform_providers = self.platform_config.get("providers", ["CPUExecutionProvider"])
            
            # é¸æ“‡æä¾›å•†
            self.providers = [p for p in platform_providers if p in available_providers]
            if not self.providers:
                self.providers = ["CPUExecutionProvider"]
            
            logger.info(f"ğŸ“‹ ONNXæä¾›å•†: {self.providers}")
            
            # å‰µå»ºæœƒè©±é¸é …
            self.session_options = ort.SessionOptions()
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            self.session_options.enable_cpu_mem_arena = True
            self.session_options.enable_mem_pattern = True
            
            num_threads = self.platform_config.get("num_threads", 4)
            self.session_options.intra_op_num_threads = num_threads
            
            logger.info("âœ… ONNX Runtimeåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ONNX Runtimeåˆå§‹åŒ–å¤±æ•—: {e}")
            self.onnx_available = False
    
    def _initialize_qai_hub(self):
        """åˆå§‹åŒ–QAI Hub"""
        try:
            import qai_hub as hub
            
            # ç²å–è¨­å‚™åˆ—è¡¨
            self.devices = hub.get_devices()
            logger.info(f"â˜ï¸ QAI Hubè¨­å‚™æ•¸é‡: {len(self.devices)}")
            
            # é¸æ“‡ç›®æ¨™è¨­å‚™
            preferred_devices = self.config.get("qai_hub_config", {}).get("preferred_devices", [])
            self.target_device = None
            
            for preferred in preferred_devices:
                for device in self.devices:
                    if preferred.lower() in device.name.lower():
                        self.target_device = device
                        logger.info(f"ğŸ¯ é¸æ“‡è¨­å‚™: {device.name}")
                        break
                if self.target_device:
                    break
            
            if not self.target_device and self.devices:
                self.target_device = self.devices[0]
                logger.info(f"ğŸ¯ ä½¿ç”¨é è¨­è¨­å‚™: {self.target_device.name}")
            
            logger.info("âœ… QAI Hubåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ QAI Hubåˆå§‹åŒ–å¤±æ•—: {e}")
            self.qai_hub_available = False
    
    def _initialize_mediapipe(self):
        """åˆå§‹åŒ–MediaPipeå¾Œå‚™ç³»çµ±"""
        try:
            import mediapipe as mp
            
            # åˆå§‹åŒ–MediaPipeçµ„ä»¶
            self.mp_face_detection = mp.solutions.face_detection.FaceDetection(
                model_selection=0, min_detection_confidence=0.5)
            
            self.mp_pose = mp.solutions.pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                smooth_landmarks=True,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5)
            
            self.mp_hands = mp.solutions.hands.Hands(
                static_image_mode=False,
                max_num_hands=2,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5)
            
            logger.info("âœ… MediaPipeå¾Œå‚™ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ MediaPipeåˆå§‹åŒ–å¤±æ•—: {e}")
            self.mediapipe_available = False
    
    def load_onnx_model(self, model_name: str, model_path: str) -> bool:
        """è¼‰å…¥ONNXæ¨¡å‹"""
        if not self.onnx_available:
            logger.warning("âš ï¸ ONNX Runtimeä¸å¯ç”¨ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹")
            return False
        
        try:
            import onnxruntime as ort
            
            if not os.path.exists(model_path):
                logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return False
            
            session = ort.InferenceSession(
                model_path,
                sess_options=self.session_options,
                providers=self.providers
            )
            
            self.sessions[model_name] = session
            
            # è¨˜éŒ„æ¨¡å‹ä¿¡æ¯
            input_info = session.get_inputs()[0]
            output_info = session.get_outputs()[0]
            
            logger.info(f"âœ… è¼‰å…¥æ¨¡å‹ {model_name}")
            logger.info(f"   è¼¸å…¥: {input_info.name} {input_info.shape}")
            logger.info(f"   è¼¸å‡º: {output_info.name} {output_info.shape}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•— {model_name}: {e}")
            return False
    
    def detect_faces(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """äººè‡‰æª¢æ¸¬"""
        results = []
        
        # å˜—è©¦ONNXæ¨¡å‹
        if "face_detection" in self.sessions:
            try:
                results = self._onnx_face_detection(image)
                logger.debug("ğŸ¯ ä½¿ç”¨ONNXäººè‡‰æª¢æ¸¬")
            except Exception as e:
                logger.warning(f"âš ï¸ ONNXäººè‡‰æª¢æ¸¬å¤±æ•—: {e}")
        
        # å¾Œå‚™MediaPipe
        if not results and self.mediapipe_available:
            try:
                results = self._mediapipe_face_detection(image)
                logger.debug("ğŸ¯ ä½¿ç”¨MediaPipeäººè‡‰æª¢æ¸¬")
            except Exception as e:
                logger.warning(f"âš ï¸ MediaPipeäººè‡‰æª¢æ¸¬å¤±æ•—: {e}")
        
        return results
    
    def detect_pose(self, image: np.ndarray) -> Dict[str, Any]:
        """å§¿æ…‹æª¢æ¸¬"""
        result = None
        
        # å˜—è©¦ONNXæ¨¡å‹
        if "pose_estimation" in self.sessions:
            try:
                result = self._onnx_pose_detection(image)
                logger.debug("ğŸ¯ ä½¿ç”¨ONNXå§¿æ…‹æª¢æ¸¬")
            except Exception as e:
                logger.warning(f"âš ï¸ ONNXå§¿æ…‹æª¢æ¸¬å¤±æ•—: {e}")
        
        # å¾Œå‚™MediaPipe
        if not result and self.mediapipe_available:
            try:
                result = self._mediapipe_pose_detection(image)
                logger.debug("ğŸ¯ ä½¿ç”¨MediaPipeå§¿æ…‹æª¢æ¸¬")
            except Exception as e:
                logger.warning(f"âš ï¸ MediaPipeå§¿æ…‹æª¢æ¸¬å¤±æ•—: {e}")
        
        return result or {}
    
    def detect_hands(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """æ‰‹éƒ¨æª¢æ¸¬"""
        results = []
        
        # å˜—è©¦ONNXæ¨¡å‹
        if "hand_detection" in self.sessions:
            try:
                results = self._onnx_hand_detection(image)
                logger.debug("ğŸ¯ ä½¿ç”¨ONNXæ‰‹éƒ¨æª¢æ¸¬")
            except Exception as e:
                logger.warning(f"âš ï¸ ONNXæ‰‹éƒ¨æª¢æ¸¬å¤±æ•—: {e}")
        
        # å¾Œå‚™MediaPipe
        if not results and self.mediapipe_available:
            try:
                results = self._mediapipe_hand_detection(image)
                logger.debug("ğŸ¯ ä½¿ç”¨MediaPipeæ‰‹éƒ¨æª¢æ¸¬")
            except Exception as e:
                logger.warning(f"âš ï¸ MediaPipeæ‰‹éƒ¨æª¢æ¸¬å¤±æ•—: {e}")
        
        return results
    
    def _onnx_face_detection(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ONNXäººè‡‰æª¢æ¸¬å¯¦ç¾"""
        session = self.sessions["face_detection"]
        
        # é è™•ç†
        input_shape = self.platform_config.get("input_shape", [1, 3, 224, 224])
        processed_image = self._preprocess_image(image, input_shape[2:])
        
        # æ¨ç†
        input_name = session.get_inputs()[0].name
        outputs = session.run(None, {input_name: processed_image})
        
        # å¾Œè™•ç†
        return self._postprocess_face_detection(outputs, image.shape)
    
    def _onnx_pose_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """ONNXå§¿æ…‹æª¢æ¸¬å¯¦ç¾"""
        session = self.sessions["pose_estimation"]
        
        # é è™•ç†
        input_shape = self.platform_config.get("input_shape", [1, 3, 224, 224])
        processed_image = self._preprocess_image(image, input_shape[2:])
        
        # æ¨ç†
        input_name = session.get_inputs()[0].name
        outputs = session.run(None, {input_name: processed_image})
        
        # å¾Œè™•ç†
        return self._postprocess_pose_detection(outputs, image.shape)
    
    def _onnx_hand_detection(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ONNXæ‰‹éƒ¨æª¢æ¸¬å¯¦ç¾"""
        session = self.sessions["hand_detection"]
        
        # é è™•ç†
        input_shape = self.platform_config.get("input_shape", [1, 3, 224, 224])
        processed_image = self._preprocess_image(image, input_shape[2:])
        
        # æ¨ç†
        input_name = session.get_inputs()[0].name
        outputs = session.run(None, {input_name: processed_image})
        
        # å¾Œè™•ç†
        return self._postprocess_hand_detection(outputs, image.shape)
    
    def _mediapipe_face_detection(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """MediaPipeäººè‡‰æª¢æ¸¬å¯¦ç¾"""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.mp_face_detection.process(rgb_image)
        
        faces = []
        if results.detections:
            for detection in results.detections:
                bbox = detection.location_data.relative_bounding_box
                h, w = image.shape[:2]
                
                faces.append({
                    "bbox": [
                        int(bbox.xmin * w),
                        int(bbox.ymin * h),
                        int(bbox.width * w),
                        int(bbox.height * h)
                    ],
                    "confidence": detection.score[0],
                    "landmarks": []
                })
        
        return faces
    
    def _mediapipe_pose_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """MediaPipeå§¿æ…‹æª¢æ¸¬å¯¦ç¾"""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.mp_pose.process(rgb_image)
        
        if results.pose_landmarks:
            landmarks = []
            for landmark in results.pose_landmarks.landmark:
                landmarks.append({
                    "x": landmark.x,
                    "y": landmark.y,
                    "z": landmark.z,
                    "visibility": landmark.visibility
                })
            
            return {
                "landmarks": landmarks,
                "confidence": 0.8  # MediaPipeæ²’æœ‰æä¾›æ•´é«”ç½®ä¿¡åº¦
            }
        
        return {}
    
    def _mediapipe_hand_detection(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """MediaPipeæ‰‹éƒ¨æª¢æ¸¬å¯¦ç¾"""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.mp_hands.process(rgb_image)
        
        hands = []
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                landmarks = []
                for landmark in hand_landmarks.landmark:
                    landmarks.append({
                        "x": landmark.x,
                        "y": landmark.y,
                        "z": landmark.z
                    })
                
                hands.append({
                    "landmarks": landmarks,
                    "confidence": 0.8
                })
        
        return hands
    
    def _preprocess_image(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """åœ–åƒé è™•ç†"""
        # èª¿æ•´å¤§å°
        resized = cv2.resize(image, target_size)
        
        # è½‰æ›ç‚ºRGB
        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # æ­£è¦åŒ–
        normalized = rgb.astype(np.float32) / 255.0
        
        # è½‰æ›ç¶­åº¦ (H, W, C) -> (1, C, H, W)
        transposed = np.transpose(normalized, (2, 0, 1))
        batched = np.expand_dims(transposed, axis=0)
        
        return batched
    
    def _postprocess_face_detection(self, outputs: List[np.ndarray], image_shape: Tuple[int, int, int]) -> List[Dict[str, Any]]:
        """äººè‡‰æª¢æ¸¬å¾Œè™•ç†"""
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ONNXæ¨¡å‹è¼¸å‡ºæ ¼å¼ä¾†å¯¦ç¾
        # ç›®å‰è¿”å›ç©ºåˆ—è¡¨ä½œç‚ºå ä½ç¬¦
        return []
    
    def _postprocess_pose_detection(self, outputs: List[np.ndarray], image_shape: Tuple[int, int, int]) -> Dict[str, Any]:
        """å§¿æ…‹æª¢æ¸¬å¾Œè™•ç†"""
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ONNXæ¨¡å‹è¼¸å‡ºæ ¼å¼ä¾†å¯¦ç¾
        # ç›®å‰è¿”å›ç©ºå­—å…¸ä½œç‚ºå ä½ç¬¦
        return {}
    
    def _postprocess_hand_detection(self, outputs: List[np.ndarray], image_shape: Tuple[int, int, int]) -> List[Dict[str, Any]]:
        """æ‰‹éƒ¨æª¢æ¸¬å¾Œè™•ç†"""
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ONNXæ¨¡å‹è¼¸å‡ºæ ¼å¼ä¾†å¯¦ç¾
        # ç›®å‰è¿”å›ç©ºåˆ—è¡¨ä½œç‚ºå ä½ç¬¦
        return []
    
    def analyze_fall_risk(self, image: np.ndarray) -> Dict[str, Any]:
        """è·Œå€’é¢¨éšªåˆ†æ"""
        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "platform": self.platform_info["type"],
            "fall_risk_score": 0.0,
            "alerts": [],
            "detections": {}
        }
        
        try:
            # äººè‡‰æª¢æ¸¬
            faces = self.detect_faces(image)
            analysis_result["detections"]["faces"] = faces
            
            # å§¿æ…‹æª¢æ¸¬
            pose = self.detect_pose(image)
            analysis_result["detections"]["pose"] = pose
            
            # æ‰‹éƒ¨æª¢æ¸¬
            hands = self.detect_hands(image)
            analysis_result["detections"]["hands"] = hands
            
            # è·Œå€’é¢¨éšªè©•ä¼°
            risk_score = self._calculate_fall_risk(faces, pose, hands)
            analysis_result["fall_risk_score"] = risk_score
            
            # ç”Ÿæˆè­¦å ±
            if risk_score > 0.8:
                analysis_result["alerts"].append("é«˜è·Œå€’é¢¨éšª")
            elif risk_score > 0.6:
                analysis_result["alerts"].append("ä¸­ç­‰è·Œå€’é¢¨éšª")
            
            logger.debug(f"ğŸ“Š é¢¨éšªåˆ†æ•¸: {risk_score:.2f}")
            
        except Exception as e:
            logger.error(f"âŒ è·Œå€’é¢¨éšªåˆ†æå¤±æ•—: {e}")
            analysis_result["error"] = str(e)
        
        return analysis_result
    
    def _calculate_fall_risk(self, faces: List[Dict], pose: Dict, hands: List[Dict]) -> float:
        """è¨ˆç®—è·Œå€’é¢¨éšªåˆ†æ•¸"""
        risk_factors = []
        
        # äººè‡‰æª¢æ¸¬å› å­
        if not faces:
            risk_factors.append(0.3)  # æœªæª¢æ¸¬åˆ°äººè‡‰
        
        # å§¿æ…‹åˆ†æå› å­
        if pose and "landmarks" in pose:
            # é€™è£¡å¯ä»¥æ·»åŠ å…·é«”çš„å§¿æ…‹åˆ†æé‚è¼¯
            # ä¾‹å¦‚æª¢æŸ¥èº«é«”å‚¾æ–œåº¦ã€å¹³è¡¡ç‹€æ…‹ç­‰
            pass
        else:
            risk_factors.append(0.2)  # æœªæª¢æ¸¬åˆ°å§¿æ…‹
        
        # æ‰‹éƒ¨æª¢æ¸¬å› å­
        if len(hands) == 0:
            risk_factors.append(0.1)  # æœªæª¢æ¸¬åˆ°æ‰‹éƒ¨
        
        # è¨ˆç®—ç¸½é«”é¢¨éšªåˆ†æ•¸
        base_score = sum(risk_factors)
        return min(base_score, 1.0)
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            "platform": self.platform_info,
            "capabilities": {
                "onnx_runtime": self.onnx_available,
                "qai_hub": self.qai_hub_available,
                "mediapipe": self.mediapipe_available
            },
            "loaded_models": list(self.sessions.keys()),
            "providers": getattr(self, "providers", []),
            "target_device": getattr(self, "target_device", {}).name if hasattr(getattr(self, "target_device", {}), "name") else None
        }

def main():
    """ä¸»å‡½æ•¸ï¼šæ¸¬è©¦çµ±ä¸€æª¢æ¸¬å™¨"""
    print("ğŸ§  çµ±ä¸€AIæª¢æ¸¬å™¨æ¸¬è©¦")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–æª¢æ¸¬å™¨
        detector = UnifiedAIDetector()
        
        # é¡¯ç¤ºç³»çµ±ç‹€æ…‹
        status = detector.get_system_status()
        
        print("\nğŸ“Š ç³»çµ±ç‹€æ…‹:")
        print(f"   å¹³å°: {status['platform']['type']}")
        print(f"   AIåŠ é€Ÿå™¨: {status['platform']['ai_accelerator']}")
        print(f"   ONNX Runtime: {'âœ…' if status['capabilities']['onnx_runtime'] else 'âŒ'}")
        print(f"   QAI Hub: {'âœ…' if status['capabilities']['qai_hub'] else 'âŒ'}")
        print(f"   MediaPipe: {'âœ…' if status['capabilities']['mediapipe'] else 'âŒ'}")
        
        if status['providers']:
            print(f"   åŸ·è¡Œæä¾›å•†: {', '.join(status['providers'])}")
        
        if status['target_device']:
            print(f"   ç›®æ¨™è¨­å‚™: {status['target_device']}")
        
        # æ¸¬è©¦åœ–åƒæª¢æ¸¬
        test_image_path = "andy.jpg"
        if os.path.exists(test_image_path):
            print(f"\nğŸ–¼ï¸ æ¸¬è©¦åœ–åƒ: {test_image_path}")
            
            image = cv2.imread(test_image_path)
            if image is not None:
                # åŸ·è¡Œè·Œå€’é¢¨éšªåˆ†æ
                analysis = detector.analyze_fall_risk(image)
                
                print(f"   é¢¨éšªåˆ†æ•¸: {analysis['fall_risk_score']:.2f}")
                print(f"   è­¦å ±: {', '.join(analysis['alerts']) if analysis['alerts'] else 'ç„¡'}")
                print(f"   æª¢æ¸¬çµæœ: {len(analysis['detections'])} é …")
            else:
                print("   âŒ åœ–åƒè¼‰å…¥å¤±æ•—")
        else:
            print("   âš ï¸ æ¸¬è©¦åœ–åƒä¸å­˜åœ¨")
        
        print("\nâœ… çµ±ä¸€æª¢æ¸¬å™¨æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
