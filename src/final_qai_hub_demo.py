#!/usr/bin/env python3
"""
ğŸ¯ æœ€çµ‚å®˜æ–¹QAI Hubæª¢æ¸¬ç³»çµ±æ¼”ç¤º
å±•ç¤ºå®Œæ•´çš„è€äººè¡Œç‚ºé æ¸¬èˆ‡é¢¨éšªè©•ä¼°åŠŸèƒ½
"""

import cv2
import numpy as np
import os
import logging
import time
from datetime import datetime

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def final_qai_hub_demo():
    """æœ€çµ‚QAI Hubæ¼”ç¤º"""
    print("ğŸ¯ === å®˜æ–¹QAI Hubè€äººè¡Œç‚ºé æ¸¬ç³»çµ± - æœ€çµ‚æ¼”ç¤º ===")
    print()
    
    try:
        # 1. ç³»çµ±åˆå§‹åŒ–
        print("1ï¸âƒ£ ç³»çµ±åˆå§‹åŒ–...")
        from elderly_behavior_predictor import ElderlyBehaviorPredictor
        from official_qai_hub_detector import OfficialQAIHubDetector
        
        # å‰µå»ºæª¢æ¸¬å™¨å¯¦ä¾‹
        detector = OfficialQAIHubDetector()
        predictor = ElderlyBehaviorPredictor()
        
        print("   âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        print()
        
        # 2. æ¸¬è©¦åœ–åƒ
        test_images = [
            {'path': 'andy.jpg', 'description': 'å–®äººè‚–åƒ'},
            {'path': 'official_test_image.jpg', 'description': 'å¤šäººå ´æ™¯'},
            {'path': 'enhanced_test_image.jpg', 'description': 'å¢å¼·æ¸¬è©¦åœ–åƒ'}
        ]
        
        for idx, image_info in enumerate(test_images, 1):
            image_path = image_info['path']
            description = image_info['description']
            
            if not os.path.exists(image_path):
                print(f"â­ï¸ è·³é {image_path} ({description}) - æª”æ¡ˆä¸å­˜åœ¨")
                continue
                
            print(f"2ï¸âƒ£.{idx} è™•ç† {image_path} ({description})")
            
            # è¼‰å…¥åœ–åƒ
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"   âŒ ç„¡æ³•è¼‰å…¥åœ–åƒ")
                continue
                
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # 3. å®˜æ–¹QAI Hubæª¢æ¸¬
            print("   ğŸ” åŸ·è¡Œå®˜æ–¹QAI Hubæª¢æ¸¬...")
            
            # äººè‡‰æª¢æ¸¬
            face_result = detector.detect_faces(rgb_frame, raw_output=True)
            print(f"      ğŸ‘¤ äººè‡‰: {face_result.get('num_faces', 0)}å€‹")
            
            # å§¿æ…‹æª¢æ¸¬
            pose_result = detector.detect_pose(rgb_frame, raw_output=True)
            print(f"      ğŸš¶ å§¿æ…‹: {pose_result.get('num_poses', 0)}å€‹")
            
            # æ‰‹éƒ¨æª¢æ¸¬
            hand_result = detector.detect_hands(rgb_frame, raw_output=True)
            print(f"      âœ‹ æ‰‹éƒ¨: {hand_result.get('num_hands', 0)}å€‹")
            
            # çµ±ä¸€æª¢æ¸¬
            unified_result = detector.unified_detection(rgb_frame)
            if unified_result.get('success'):
                total = unified_result['total_detections']
                print(f"      ğŸ“Š çµ±ä¸€æª¢æ¸¬çµæœ: {total['faces']}è‡‰, {total['poses']}å§¿æ…‹, {total['hands']}æ‰‹")
            
            # 4. è¡Œç‚ºåˆ†æ
            print("   ğŸ§  åŸ·è¡Œè¡Œç‚ºåˆ†æ...")
            user_id = f"test_user_{idx}"
            interaction_result = predictor.process_user_interaction(user_id, frame)
            
            if interaction_result:
                face_detected = interaction_result.get('face_detected', False)
                pose_analysis = interaction_result.get('pose_analysis', {})
                risk_assessment = interaction_result.get('risk_assessment', {})
                
                print(f"      ğŸ‘ï¸ äººè‡‰æª¢æ¸¬: {'æ˜¯' if face_detected else 'å¦'}")
                
                if pose_analysis and 'error' not in pose_analysis:
                    print(f"      âš–ï¸ å¹³è¡¡è©•åˆ†: {pose_analysis.get('balance_score', 0):.2f}")
                    print(f"      ğŸ¯ ç©©å®šæ€§è©•åˆ†: {pose_analysis.get('stability_score', 0):.2f}")
                    print(f"      ğŸ“ å§¿æ…‹åå·®: {pose_analysis.get('posture_deviation', 0):.2f}")
                
                if risk_assessment:
                    risk_score = risk_assessment.get('score', 0)
                    risk_level = risk_assessment.get('level', 'unknown')
                    risk_color = 'ğŸŸ¢' if risk_level == 'low' else 'ğŸŸ¡' if risk_level == 'medium' else 'ğŸ”´'
                    print(f"      {risk_color} é¢¨éšªè©•ä¼°: {risk_score:.2f} ({risk_level})")
                
                if interaction_result.get('alert_triggered', False):
                    print("      ğŸš¨ è­¦å ±è§¸ç™¼!")
            
            # 5. ä¿å­˜çµæœ
            if face_result.get('success'):
                annotated_face = detector.detect_faces(rgb_frame, raw_output=False)
                if 'annotated_image' in annotated_face:
                    result_filename = f"final_qai_hub_face_{os.path.basename(image_path)}"
                    detector.save_annotated_result(annotated_face['annotated_image'], result_filename)
                    print(f"      ğŸ’¾ äººè‡‰æª¢æ¸¬çµæœå·²ä¿å­˜: {result_filename}")
            
            if pose_result.get('success'):
                annotated_pose = detector.detect_pose(rgb_frame, raw_output=False)
                if 'annotated_image' in annotated_pose:
                    result_filename = f"final_qai_hub_pose_{os.path.basename(image_path)}"
                    detector.save_annotated_result(annotated_pose['annotated_image'], result_filename)
                    print(f"      ğŸ’¾ å§¿æ…‹æª¢æ¸¬çµæœå·²ä¿å­˜: {result_filename}")
            
            print()
        
        # 6. ç³»çµ±ç¸½çµ
        print("3ï¸âƒ£ ç³»çµ±æ€§èƒ½ç¸½çµ")
        print("   âœ… å®˜æ–¹QAI Hubæ¨¡å‹é›†æˆ: MediaPipe Face + Pose + Hand")
        print("   âœ… å¯¦æ™‚æª¢æ¸¬èˆ‡åˆ†æåŠŸèƒ½")
        print("   âœ… è¡Œç‚ºé æ¸¬èˆ‡é¢¨éšªè©•ä¼°")
        print("   âœ… å¤šæ¨¡æ…‹æª¢æ¸¬çµ±ä¸€æ¥å£")
        print("   âœ… å®Œå…¨æŒ‰ç…§Qualcomm AI Hubå®˜æ–¹æ–‡æª”å¯¦ç¾")
        print()
        
        print("ğŸ‰ === å®˜æ–¹QAI Hubè€äººè¡Œç‚ºé æ¸¬ç³»çµ±æ¼”ç¤ºå®Œæˆ ===")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºåŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    final_qai_hub_demo()
