#!/usr/bin/env python3
"""
ğŸ¯ æœ€çµ‚QAI Hub + ONNX Runtimeç”Ÿç”¢å°±ç·’ç³»çµ±
æ­£ç¢ºè™•ç†QAI Hub Modelsä¸¦å¯¦ç¾çœŸå¯¦é€£æ¥
"""

import os
import qai_hub as hub
import numpy as np
import cv2
import onnxruntime as ort
import torch
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional
import time
import json

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class FinalQAIHubONNXSystem:
    """æœ€çµ‚QAI Hub + ONNX Runtimeç”Ÿç”¢å°±ç·’ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»çµ±"""
        self.api_token = os.getenv('QAI_HUB_API_TOKEN')
        if not self.api_token:
            raise ValueError("âŒ è«‹è¨­ç½®QAI_HUB_API_TOKENç’°å¢ƒè®Šæ•¸")
        
        # ONNX Runtimeé…ç½®
        self.onnx_providers = self._get_onnx_providers()
        self.onnx_sessions = {}
        
        # QAI Hubç›¸é—œ
        self.model_components = {}
        self.qai_hub_models = {}
        self.jobs = {}
        
        logger.info("ğŸš€ åˆå§‹åŒ–æœ€çµ‚QAI Hub + ONNXç³»çµ±...")
        self._verify_connection()
        
    def _verify_connection(self):
        """é©—è­‰QAI Hubé€£æ¥"""
        try:
            devices = hub.get_devices()
            logger.info(f"âœ… QAI Hubé€£æ¥æˆåŠŸï¼Œ{len(devices)}å€‹è¨­å‚™å¯ç”¨")
            
            # é¸æ“‡æœ€ä½³è¨­å‚™
            target_devices = [d for d in devices if any(kw in d.name for kw in 
                                                      ['Snapdragon', 'Samsung', 'Galaxy'])]
            self.target_device = target_devices[0] if target_devices else devices[0]
            logger.info(f"ğŸ¯ ç›®æ¨™è¨­å‚™: {self.target_device.name}")
            
        except Exception as e:
            logger.error(f"âŒ QAI Hubé€£æ¥å¤±æ•—: {e}")
            raise
    
    def _get_onnx_providers(self):
        """ç²å–ONNX RuntimeåŸ·è¡Œæä¾›å•†"""
        providers = []
        available = ort.get_available_providers()
        
        # æŒ‰å„ªå…ˆç´šé¸æ“‡æä¾›å•†
        priority_providers = [
            'CUDAExecutionProvider',
            'DmlExecutionProvider', 
            'CoreMLExecutionProvider',
            'CPUExecutionProvider'
        ]
        
        for provider in priority_providers:
            if provider in available:
                providers.append(provider)
                if provider != 'CPUExecutionProvider':
                    logger.info(f"âœ… å•Ÿç”¨ç¡¬é«”åŠ é€Ÿ: {provider}")
                break
        
        if not providers:
            providers = ['CPUExecutionProvider']
        
        logger.info(f"ğŸ“‹ ONNXåŸ·è¡Œæä¾›å•†: {providers}")
        return providers
    
    def load_mediapipe_components(self):
        """è¼‰å…¥MediaPipeæ¨¡å‹çµ„ä»¶"""
        logger.info("ğŸ“¥ è¼‰å…¥MediaPipeæ¨¡å‹çµ„ä»¶...")
        
        component_configs = {
            'face_detector': {
                'module_path': 'qai_hub_models.models.mediapipe_face',
                'component_name': 'face_detector',
                'description': 'MediaPipe Face Detector'
            },
            'face_landmark': {
                'module_path': 'qai_hub_models.models.mediapipe_face',
                'component_name': 'face_landmark_detector',
                'description': 'MediaPipe Face Landmark Detector'
            }
        }
        
        for comp_name, config in component_configs.items():
            try:
                logger.info(f"ğŸ“± è¼‰å…¥ {config['description']}...")
                
                # å°å…¥ä¸¦å‰µå»ºæ¨¡å‹
                module = __import__(config['module_path'], fromlist=['Model'])
                full_model = module.Model.from_pretrained()
                
                # æå–çµ„ä»¶
                component = getattr(full_model, config['component_name'])
                
                self.model_components[comp_name] = {
                    'component': component,
                    'config': config,
                    'loaded': True
                }
                
                logger.info(f"âœ… {config['description']} è¼‰å…¥æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} è¼‰å…¥å¤±æ•—: {e}")
                self.model_components[comp_name] = {
                    'component': None,
                    'config': config,
                    'loaded': False,
                    'error': str(e)
                }
    
    def convert_to_torchscript_and_upload(self):
        """è½‰æ›ç‚ºTorchScriptä¸¦ä¸Šå‚³åˆ°QAI Hub"""
        logger.info("ğŸ“¤ è½‰æ›ç‚ºTorchScriptä¸¦ä¸Šå‚³...")
        
        for comp_name, comp_info in self.model_components.items():
            if not comp_info['loaded']:
                continue
                
            try:
                component = comp_info['component']
                config = comp_info['config']
                
                logger.info(f"ğŸ”„ è™•ç† {config['description']}...")
                
                # ä½¿ç”¨çµ„ä»¶çš„convert_to_torchscriptæ–¹æ³•
                torchscript_model = component.convert_to_torchscript()
                
                # ä¿å­˜TorchScriptæ¨¡å‹
                ts_path = f"qai_hub_{comp_name}.pt"
                torchscript_model.save(ts_path)
                logger.info(f"âœ… TorchScriptå·²ä¿å­˜: {ts_path}")
                
                # ä¸Šå‚³åˆ°QAI Hub
                logger.info(f"â˜ï¸ ä¸Šå‚³ {config['description']} åˆ°QAI Hub...")
                qai_model = hub.upload_model(ts_path)
                
                self.qai_hub_models[comp_name] = {
                    'qai_model': qai_model,
                    'model_id': qai_model.model_id,
                    'torchscript_path': ts_path,
                    'config': config
                }
                
                logger.info(f"âœ… ä¸Šå‚³æˆåŠŸï¼Œæ¨¡å‹ID: {qai_model.model_id}")
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} è™•ç†å¤±æ•—: {e}")
                comp_info['process_error'] = str(e)
    
    def submit_qai_hub_jobs(self):
        """æäº¤QAI Hubç·¨è­¯å’ŒProfiling Jobs"""
        logger.info("ğŸš€ æäº¤QAI Hub Jobs...")
        
        for comp_name, model_info in self.qai_hub_models.items():
            try:
                qai_model = model_info['qai_model']
                config = model_info['config']
                
                logger.info(f"ğŸ“‹ æäº¤ {config['description']} Jobs...")
                
                # ç²å–çµ„ä»¶çš„ç¤ºä¾‹è¼¸å…¥
                component = self.model_components[comp_name]['component']
                sample_inputs = component.sample_inputs()
                
                # å¾ç¤ºä¾‹è¼¸å…¥æ¨æ–·è¼¸å…¥è¦æ ¼
                input_spec = {}
                for key, value_list in sample_inputs.items():
                    if isinstance(value_list, list) and len(value_list) > 0:
                        sample_array = value_list[0]
                        input_spec[key] = (sample_array.shape, sample_array.dtype.name)
                
                logger.info(f"   è¼¸å…¥è¦æ ¼: {input_spec}")
                
                # æäº¤ç·¨è­¯Job
                compile_job = hub.submit_compile_job(
                    model=qai_model,
                    input_specs=input_spec,
                    device=self.target_device
                )
                
                # æäº¤Profiling Job
                profile_job = hub.submit_profile_job(
                    model=qai_model,
                    input_data=sample_inputs,
                    device=self.target_device
                )
                
                self.jobs[comp_name] = {
                    'compile_job': compile_job,
                    'profile_job': profile_job,
                    'config': config
                }
                
                logger.info(f"âœ… Jobså·²æäº¤:")
                logger.info(f"   ç·¨è­¯Job: {compile_job.job_id}")
                logger.info(f"   Profiling Job: {profile_job.job_id}")
                logger.info(f"   Dashboard: https://aihub.qualcomm.com/jobs/{compile_job.job_id}")
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} Jobæäº¤å¤±æ•—: {e}")
    
    def export_to_onnx_runtime(self):
        """å°å‡ºç‚ºONNXä¸¦å‰µå»ºRuntimeæœƒè©±"""
        logger.info("ğŸ”„ å°å‡ºONNXä¸¦å‰µå»ºRuntimeæœƒè©±...")
        
        for comp_name, comp_info in self.model_components.items():
            if not comp_info['loaded']:
                continue
                
            try:
                component = comp_info['component']
                config = comp_info['config']
                
                logger.info(f"ğŸ”„ è™•ç† {config['description']}...")
                
                # ç²å–ç¤ºä¾‹è¼¸å…¥
                sample_inputs = component.sample_inputs()
                image_tensor = torch.from_numpy(sample_inputs['image'][0])
                
                # ä½¿ç”¨TorchScriptæ¨¡å‹å°å‡ºONNX
                if comp_name in self.qai_hub_models:
                    ts_path = self.qai_hub_models[comp_name]['torchscript_path']
                    ts_model = torch.jit.load(ts_path)
                    
                    # å°å‡ºONNX
                    onnx_path = f"qai_hub_{comp_name}.onnx"
                    torch.onnx.export(
                        ts_model,
                        image_tensor,
                        onnx_path,
                        export_params=True,
                        opset_version=11,
                        do_constant_folding=True,
                        input_names=['image'],
                        output_names=['output']
                    )
                    
                    logger.info(f"âœ… ONNXå·²ä¿å­˜: {onnx_path}")
                    
                    # å‰µå»ºONNX Runtimeæœƒè©±
                    self._create_onnx_session(comp_name, onnx_path, config)
                
            except Exception as e:
                logger.error(f"âŒ {config['description']} ONNXå°å‡ºå¤±æ•—: {e}")
    
    def _create_onnx_session(self, comp_name: str, onnx_path: str, config: Dict):
        """å‰µå»ºONNX Runtimeæœƒè©±"""
        try:
            logger.info(f"ğŸ”„ å‰µå»º {comp_name} ONNX Runtimeæœƒè©±...")
            
            # æœƒè©±é¸é …
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # å‰µå»ºæœƒè©±
            session = ort.InferenceSession(onnx_path, providers=self.onnx_providers)
            
            self.onnx_sessions[comp_name] = {
                'session': session,
                'onnx_path': onnx_path,
                'config': config
            }
            
            logger.info(f"âœ… {comp_name} ONNX Runtimeæœƒè©±å‰µå»ºæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ {comp_name} ONNX Runtimeæœƒè©±å‰µå»ºå¤±æ•—: {e}")
    
    def detect_with_onnx(self, image: np.ndarray, model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨ONNX RuntimeåŸ·è¡Œæª¢æ¸¬"""
        if model_name not in self.onnx_sessions:
            return {"error": f"æ¨¡å‹ {model_name} çš„ONNXæœƒè©±ä¸å­˜åœ¨"}
        
        try:
            session_info = self.onnx_sessions[model_name]
            session = session_info['session']
            config = session_info['config']
            
            # é è™•ç†åœ–åƒ
            processed_input = self._preprocess_for_onnx(image, model_name)
            
            # åŸ·è¡Œæ¨ç†
            input_name = session.get_inputs()[0].name
            start_time = time.time()
            outputs = session.run(None, {input_name: processed_input})
            inference_time = (time.time() - start_time) * 1000
            
            # åŸºæœ¬çµæœ
            results = {
                "success": True,
                "inference_time_ms": round(inference_time, 2),
                "model_name": model_name,
                "description": config['description'],
                "output_shapes": [out.shape for out in outputs],
                "num_outputs": len(outputs)
            }
            
            logger.info(f"âš¡ {model_name} æª¢æ¸¬å®Œæˆ: {inference_time:.2f}ms")
            return results
            
        except Exception as e:
            logger.error(f"âŒ {model_name} æª¢æ¸¬å¤±æ•—: {e}")
            return {"error": str(e), "success": False}
    
    def _preprocess_for_onnx(self, image: np.ndarray, model_name: str) -> np.ndarray:
        """ç‚ºONNXæ¨ç†é è™•ç†åœ–åƒ"""
        # ç²å–åŸå§‹çµ„ä»¶çš„ç¤ºä¾‹è¼¸å…¥å½¢ç‹€
        if model_name in self.model_components and self.model_components[model_name]['loaded']:
            component = self.model_components[model_name]['component']
            sample_inputs = component.sample_inputs()
            target_shape = sample_inputs['image'][0].shape
            
            # èª¿æ•´åœ–åƒå¤§å°åˆ°ç›®æ¨™å½¢ç‹€
            target_height, target_width = target_shape[2], target_shape[3]
            resized = cv2.resize(image, (target_width, target_height))
            
            # è½‰æ›é¡è‰²ç©ºé–“
            if len(resized.shape) == 3:
                resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            
            # æ­£è¦åŒ–
            normalized = resized.astype(np.float32) / 255.0
            
            # è½‰æ›ç¶­åº¦é †åºä¸¦æ·»åŠ batchç¶­åº¦
            preprocessed = np.transpose(normalized, (2, 0, 1))
            preprocessed = np.expand_dims(preprocessed, axis=0)
            
            return preprocessed
        
        # é»˜èªé è™•ç†
        resized = cv2.resize(image, (192, 192))
        resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        normalized = resized.astype(np.float32) / 255.0
        preprocessed = np.transpose(normalized, (2, 0, 1))
        preprocessed = np.expand_dims(preprocessed, axis=0)
        
        return preprocessed
    
    def run_comprehensive_detection(self, image: np.ndarray) -> Dict[str, Any]:
        """é‹è¡Œç¶œåˆæª¢æ¸¬"""
        results = {
            "timestamp": time.time(),
            "image_shape": image.shape,
            "detections": {},
            "performance": {},
            "qai_hub_info": {}
        }
        
        # åŸ·è¡ŒONNXæª¢æ¸¬
        total_time = 0
        for model_name in self.onnx_sessions.keys():
            detection_result = self.detect_with_onnx(image, model_name)
            results["detections"][model_name] = detection_result
            
            if "inference_time_ms" in detection_result:
                results["performance"][f"{model_name}_ms"] = detection_result["inference_time_ms"]
                total_time += detection_result["inference_time_ms"]
        
        results["performance"]["total_time_ms"] = round(total_time, 2)
        
        # æ·»åŠ QAI Hubä¿¡æ¯
        for model_name, job_info in self.jobs.items():
            if 'compile_job' in job_info:
                results["qai_hub_info"][model_name] = {
                    "compile_job_id": job_info['compile_job'].job_id,
                    "profile_job_id": job_info['profile_job'].job_id,
                    "dashboard_url": f"https://aihub.qualcomm.com/jobs/{job_info['compile_job'].job_id}"
                }
        
        return results
    
    def get_system_report(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±å ±å‘Š"""
        return {
            "system_info": {
                "target_device": self.target_device.name,
                "onnx_providers": self.onnx_providers
            },
            "model_components": {
                name: {
                    "loaded": info["loaded"],
                    "description": info["config"]["description"],
                    "error": info.get("error")
                }
                for name, info in self.model_components.items()
            },
            "qai_hub_models": {
                name: {
                    "model_id": info["model_id"],
                    "description": info["config"]["description"]
                }
                for name, info in self.qai_hub_models.items()
            },
            "qai_hub_jobs": {
                name: {
                    "compile_job_id": info["compile_job"].job_id,
                    "profile_job_id": info["profile_job"].job_id,
                    "description": info["config"]["description"],
                    "dashboard_url": f"https://aihub.qualcomm.com/jobs/{info['compile_job'].job_id}"
                }
                for name, info in self.jobs.items()
            },
            "onnx_sessions": list(self.onnx_sessions.keys())
        }

def main():
    """ä¸»å‡½æ•¸ï¼šæ¼”ç¤ºæœ€çµ‚QAI Hub + ONNXç³»çµ±"""
    print("ğŸ¯ æœ€çµ‚QAI Hub + ONNX Runtimeç”Ÿç”¢å°±ç·’ç³»çµ±")
    print("=" * 70)
    
    try:
        # åˆå§‹åŒ–ç³»çµ±
        system = FinalQAIHubONNXSystem()
        
        # åŸ·è¡Œå®Œæ•´æµç¨‹
        print("\nğŸ“¥ è¼‰å…¥MediaPipeçµ„ä»¶...")
        system.load_mediapipe_components()
        
        print("\nğŸ“¤ è½‰æ›TorchScriptä¸¦ä¸Šå‚³...")
        system.convert_to_torchscript_and_upload()
        
        print("\nğŸš€ æäº¤QAI Hub Jobs...")
        system.submit_qai_hub_jobs()
        
        print("\nğŸ”„ å°å‡ºONNX Runtime...")
        system.export_to_onnx_runtime()
        
        # æ¸¬è©¦æª¢æ¸¬
        if system.onnx_sessions:
            print("\nğŸ§ª æ¸¬è©¦æª¢æ¸¬ç³»çµ±...")
            
            test_images = ['andy.jpg', 'official_test_image.jpg']
            for img_path in test_images:
                if os.path.exists(img_path):
                    print(f"\nğŸ“· æ¸¬è©¦: {img_path}")
                    image = cv2.imread(img_path)
                    
                    if image is not None:
                        results = system.run_comprehensive_detection(image)
                        print(f"   æª¢æ¸¬çµæœ: {len(results['detections'])}å€‹æ¨¡å‹")
                        print(f"   ç¸½æ¨ç†æ™‚é–“: {results['performance'].get('total_time_ms', 0)}ms")
        
        # ç”Ÿæˆç³»çµ±å ±å‘Š
        print("\nğŸ“Š ç³»çµ±å ±å‘Š:")
        report = system.get_system_report()
        
        print(f"   ç›®æ¨™è¨­å‚™: {report['system_info']['target_device']}")
        print(f"   è¼‰å…¥çµ„ä»¶: {len(report['model_components'])}")
        print(f"   QAI Hubæ¨¡å‹: {len(report['qai_hub_models'])}")
        print(f"   QAI Hub Jobs: {len(report['qai_hub_jobs'])}")
        print(f"   ONNXæœƒè©±: {len(report['onnx_sessions'])}")
        
        # QAI Hub Jobsè©³æƒ…
        if report['qai_hub_jobs']:
            print("\nğŸ“‹ QAI Hub Jobs:")
            for name, job_info in report['qai_hub_jobs'].items():
                print(f"   {name}:")
                print(f"     ç·¨è­¯Job: {job_info['compile_job_id']}")
                print(f"     Profiling Job: {job_info['profile_job_id']}")
                print(f"     Dashboard: {job_info['dashboard_url']}")
        
        # ä¿å­˜å ±å‘Š
        report_file = 'final_qai_hub_onnx_system_report.json'
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nâœ… ç³»çµ±æ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")
        print(f"ğŸ¯ çœŸæ­£çš„QAI Hub + ONNX Runtimeç”Ÿç”¢ç³»çµ±é‹è¡ŒæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ ç³»çµ±æ¼”ç¤ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
