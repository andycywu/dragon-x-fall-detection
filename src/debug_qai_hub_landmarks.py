#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦èª¿è©¦ QAI Hub MediaPipe é—œéµé»è§£æ
"""

import cv2
import numpy as np
from PIL import Image
import torch

def debug_qai_hub_landmarks():
    """æ·±åº¦èª¿è©¦ QAI Hub é—œéµé»çµæ§‹"""
    try:
        from qai_hub_models.models.mediapipe_pose.app import MediaPipePoseApp
        from qai_hub_models.models.mediapipe_pose.model import MediaPipePose, MODEL_ID, MODEL_ASSET_VERSION
        from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
        
        # è¼‰å…¥æ¨¡å‹å’Œåœ–åƒ
        pose_model = MediaPipePose.from_pretrained()
        pose_app = MediaPipePoseApp.from_pretrained(pose_model)
        
        official_image_asset = CachedWebModelAsset.from_asset_store(
            MODEL_ID, MODEL_ASSET_VERSION, "pose.jpeg"
        )
        official_image = load_image(official_image_asset)
        
        if isinstance(official_image, Image.Image):
            official_image = np.array(official_image)
            official_image = cv2.cvtColor(official_image, cv2.COLOR_RGB2BGR)
        
        rgb_image = cv2.cvtColor(official_image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_image)
        
        print("ğŸ” QAI Hub MediaPipe æ·±åº¦èª¿è©¦")
        print("=" * 50)
        
        # ç²å–åŸå§‹çµæœ
        result = pose_app.predict_landmarks_from_image(pil_image, raw_output=True)
        batched_selected_boxes, batched_selected_keypoints, batched_roi_4corners, landmarks_out = result
        
        print(f"æ•´é«”çµæœ: {len(result)} å€‹å…ƒç´ ")
        print(f"Boxes: {len(batched_selected_boxes)} batches")
        print(f"Keypoints: {len(batched_selected_keypoints)} batches")  
        print(f"ROI corners: {len(batched_roi_4corners)} batches")
        print(f"Landmarks out: {len(landmarks_out)} tensors")
        
        # è©³ç´°æª¢æŸ¥ landmarks_out
        print("\nğŸ“Š è©³ç´°æª¢æŸ¥ landmarks_out:")
        for i, landmark_tensor in enumerate(landmarks_out):
            print(f"\nLandmark tensor {i}:")
            print(f"  é¡å‹: {type(landmark_tensor)}")
            if hasattr(landmark_tensor, 'shape'):
                print(f"  å½¢ç‹€: {landmark_tensor.shape}")
                print(f"  å…ƒç´ æ•¸é‡: {landmark_tensor.numel()}")
                print(f"  æ•¸æ“šé¡å‹: {landmark_tensor.dtype}")
                
                # æª¢æŸ¥æ•¸æ“šç¯„åœ
                if landmark_tensor.numel() > 0:
                    print(f"  æœ€å°å€¼: {landmark_tensor.min().item()}")
                    print(f"  æœ€å¤§å€¼: {landmark_tensor.max().item()}")
                    
                    # å˜—è©¦ä¸åŒçš„è§£ææ–¹å¼
                    if len(landmark_tensor.shape) == 3:
                        batch, num_landmarks, coords = landmark_tensor.shape
                        print(f"  è§£é‡‹ç‚º: [batch={batch}, landmarks={num_landmarks}, coords={coords}]")
                        
                        if batch > 0 and num_landmarks > 0:
                            first_batch = landmark_tensor[0]
                            print(f"  ç¬¬ä¸€å€‹batchå½¢ç‹€: {first_batch.shape}")
                            
                            # é¡¯ç¤ºå‰5å€‹é—œéµé»
                            print(f"  å‰5å€‹é—œéµé»:")
                            for j in range(min(5, num_landmarks)):
                                coords_data = first_batch[j]
                                print(f"    é»{j}: {coords_data.tolist()}")
                    
                    elif len(landmark_tensor.shape) == 2:
                        num_landmarks, coords = landmark_tensor.shape
                        print(f"  è§£é‡‹ç‚º: [landmarks={num_landmarks}, coords={coords}]")
                        
                        # é¡¯ç¤ºå‰5å€‹é—œéµé»
                        print(f"  å‰5å€‹é—œéµé»:")
                        for j in range(min(5, num_landmarks)):
                            coords_data = landmark_tensor[j]
                            print(f"    é»{j}: {coords_data.tolist()}")
        
        # å˜—è©¦è§£æé—œéµé»
        print("\nğŸ¯ å˜—è©¦è§£æé—œéµé»:")
        
        if landmarks_out and len(landmarks_out) >= 1:
            pose_landmarks_tensor = landmarks_out[0]
            height, width = official_image.shape[:2]
            
            print(f"ä½¿ç”¨ç¬¬ä¸€å€‹tensor: {pose_landmarks_tensor.shape}")
            
            pose_landmarks = []
            
            if len(pose_landmarks_tensor.shape) == 3:
                # [batch, landmarks, coords]
                landmarks_data = pose_landmarks_tensor[0]  # ç¬¬ä¸€å€‹batch
            elif len(pose_landmarks_tensor.shape) == 2:
                # [landmarks, coords]
                landmarks_data = pose_landmarks_tensor
            else:
                print(f"âŒ ç„¡æ³•è™•ç†çš„å½¢ç‹€: {pose_landmarks_tensor.shape}")
                return
            
            num_landmarks = landmarks_data.shape[0]
            num_coords = landmarks_data.shape[1]
            
            print(f"é—œéµé»æ•¸é‡: {num_landmarks}")
            print(f"åº§æ¨™ç¶­åº¦: {num_coords}")
            
            # å˜—è©¦è§£ææ¯å€‹é—œéµé»
            for i in range(min(num_landmarks, 33)):  # MediaPipe æœ€å¤š33å€‹
                coords = landmarks_data[i]
                
                if num_coords >= 2:
                    x, y = float(coords[0]), float(coords[1])
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºæ­¸ä¸€åŒ–åº§æ¨™
                    if 0 <= x <= 1 and 0 <= y <= 1:
                        img_x = x * width
                        img_y = y * height
                        
                        # æª¢æŸ¥å¯è¦‹æ€§
                        if num_coords >= 4:
                            visibility = float(coords[3])
                            if visibility > 0.1:
                                pose_landmarks.append((img_x, img_y))
                                if i < 5:  # é¡¯ç¤ºå‰5å€‹
                                    print(f"  é»{i}: ({x:.3f}, {y:.3f}) -> ({img_x:.1f}, {img_y:.1f}), vis={visibility:.3f}")
                        else:
                            pose_landmarks.append((img_x, img_y))
                            if i < 5:  # é¡¯ç¤ºå‰5å€‹
                                print(f"  é»{i}: ({x:.3f}, {y:.3f}) -> ({img_x:.1f}, {img_y:.1f})")
            
            print(f"\nâœ… æˆåŠŸè§£æ {len(pose_landmarks)} å€‹é—œéµé»")
            
            if pose_landmarks:
                # ç¹ªè£½çµæœ
                result_image = official_image.copy()
                for i, (x, y) in enumerate(pose_landmarks):
                    cv2.circle(result_image, (int(x), int(y)), 8, (0, 255, 0), -1)
                    cv2.putText(result_image, str(i), (int(x)+10, int(y)-10), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                cv2.imwrite("qai_hub_debug_result.jpg", result_image)
                print("ğŸ’¾ ä¿å­˜èª¿è©¦çµæœ: qai_hub_debug_result.jpg")
        
    except Exception as e:
        print(f"âŒ èª¿è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_qai_hub_landmarks()
